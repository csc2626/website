---
title: "Fall 2024 - CSC 2626: Imitation Learning for Robotics"
tbl-colwidths: [5,10,30,5,5,5,5,5]
---

This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.

::: column-screen-inset-right
| Week | Date | Topic | Prepare | Slides | Notebooks | Assignments | Project |
|:-------:|---------|---------|:-------:|:-------:|:-------:|:-------:|:-------:|
| [1](#week-1) | Mon, Sep 9 | Imitation learning vs supervised learning | ğŸ“– | [ğŸ–¥ï¸](/lecs/w01/lec01.html){.no-style target="_blank" aria-label="Lec 1 slides"} | ğŸ“‹ | âœï¸ |  |
|  |  | Mitigating compunding errors via dataset aggregation | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Mitigating compounding errors by choosing loss functions | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Imitation via multi-modal generative models | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Training instabilities of behavioral cloning | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Teleoperation interfaces for manipulation (optional) | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Querying experts only when necessary (optional) | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Imitation for visual navigation and autonomous driving (optional) | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [2](#week-2) | Mon, Sep 16 | Intro to optimal control | ğŸ“– | [ğŸ–¥ï¸](/lecs/w02/lec02.html){.no-style target="_blank" aria-label="Lec 2 slides"} | ğŸ“‹ |  |  |
|  |  | Intro to model-based reinforcement learning | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Intro to model-free reinforcement learning | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Monotonic improvement of the value function (optional) | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Learning dynamics well only where it matters for the value function (optional) | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ | ğŸ“‚ |  |
|  |  | Parallelizing MPC on the GPU (optional) | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [3](#week-3) | Mon, Sep 23 | Offline / batch reinforcement learning | ğŸ“– | [ğŸ–¥ï¸](/lecs/w03/lec03.html){.no-style target="_blank" aria-label="Lec 3 slides"} | ğŸ“‹ | âœï¸ |  |
|  |  | Transitioning from offline to online RL | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [4](#week-4) | Mon, Sep 30 | Imitation learning combined with RL and planning | ğŸ“– | [ğŸ–¥ï¸](/lecs/w04/lec04.html){.no-style target="_blank" aria-label="Lec 4 slides"} | ğŸ“‹ |  |  |
|  |  | Making cost-to-go queries to experts | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Expert iteration | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Imitation can improve search and exploration strategies | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Learning from experts that have privileged information | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ | ğŸ“‚ | âœï¸ |
|  |  | Dynamic movement primitives | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ | ğŸ“‚ | âœï¸ |
| [5](#week-5) | Mon, Oct 7 | Inverse reinforcement learning | ğŸ“– | [ğŸ–¥ï¸](/lecs/w05/lec05.html){.no-style target="_blank" aria-label="Lec 5 slides"} | ğŸ“‹ |  |  |
|  |  | Inferring rewards from preferences | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Task specification and human-robot dialog | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Value alignment | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [6](#week-6) | Mon, Oct 14 | Thanksgiving Monday |  |  |  |  | ğŸ“‚ |
|  |  | No in-person lecture or office hours on Monday |  |  |  |  |  |
|  |  | TA office hours are on |  |  |  |  |  |
| [7](#week-7) | Mon, Oct 21 | Imitation as program induction | ğŸ“– | [ğŸ–¥ï¸](/lecs/w07/lec07.html){.no-style target="_blank" aria-label="Lec 7 slides"} | ğŸ“‹ |  |  |
|  |  | Modular decomposition of demonstrations into skills | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | (Hierarchical) imitation of multi-goal tasks | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Inferring grammars and planning domains | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [8](#week-8) | Mon, Oct 28 | Fall Reading Week |  |  |  |  |  |
|  |  | No lecture |  |  |  |  |  |
|  |  | No office hours |  |  |  |  |  |
| [9](#week-9) | Mon, Nov 4 | Adversarial imitation learning | ğŸ“– | [ğŸ–¥ï¸](/lecs/w09/lec09.html){.no-style target="_blank" aria-label="Lec 9 slides"} | ğŸ“‹ |  |  |
| [10](#week-10) | Mon, Nov 11 | Shared autonomy | ğŸ“– | [ğŸ–¥ï¸](/lecs/w10/lec10.html){.no-style target="_blank" aria-label="Lec 10 slides"} | ğŸ“‹ |  |  |
|  |  | Imitation with a human in the loop | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
|  |  | Teleoperation | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [11](#week-11) | Mon, Nov 18 | Imitation learning from videos | ğŸ“– | [ğŸ–¥ï¸](/lecs/w11/lec11.html){.no-style target="_blank" aria-label="Lec 11 slides"} | ğŸ“‹ |  |  |
|  |  | Causal confusion in imitation learning | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [12](#week-12) | Mon, Nov 25 | Representation learning for imitation | ğŸ“– | [ğŸ–¥ï¸](/lecs/w12/lec12.html){.no-style target="_blank" aria-label="Lec 11 slides"} | ğŸ“‹ |  |  |
|  |  | Generalization and safety guarantees for imitation | ğŸ“– | ğŸ–¥ï¸ | ğŸ“‹ |  |  |
| [13](#week-13) | Mon, Dec 2 | Project presentations |  |  |  |  |  |
| [14](#week-14) | Mon, Dec 9 | Final project submission |  |  |  |  | ğŸ“‚ |
:::

### Week 1 {#week-1}

**Imitation learning vs supervised learning**\
[An invitation to imitation](https://www.ri.cmu.edu/pub_files/2015/3/InvitationToImitation_3_1415.pdf)\
[ALVINN: An autonomous land vehicle in a neural network](https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network)

**Mitigating compunding errors via dataset aggregation**\
[DAgger: A reduction of imitation learning and structured prediction to no-regret online learning](https://arxiv.org/abs/1011.0686)

**Mitigating compounding errors by choosing loss functions**\
[Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning](https://arxiv.org/abs/2407.15007)

**Imitation via multi-modal generative models**\
[Diffusion policy](https://diffusion-policy.cs.columbia.edu/)

**Training instabilities of behavioral cloning**\
[Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression](https://arxiv.org/abs/2310.11428)

<details>

<summary>Teleoperation interfaces for manipulation \[optional\]</summary>

[UMI: Universal Manipulator Interface](https://umi-gripper.github.io/)\
[ALOHA: Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/)\
[Real-Time Bimanual Dexterous Teleoperation for Imitation Learning](https://dingry.github.io/projects/bunny_visionpro.html)\
[Teleoperation with Immersive Active Visual Feedback](https://robot-tv.github.io/)\
[ACE: A Cross-platform Visual-Exoskeleton for Low-Cost Dexterous Teleoperation](https://ace-teleop.github.io/)

</details>

<details>

<summary>Querying experts only when necessary \[optional\]</summary>

[Maximum mean discrepancy imitation learning](http://www.roboticsproceedings.org/rss09/p38.pdf)\
[DropoutDAgger: A Bayesian approach to safe imitation learning](https://arxiv.org/abs/1709.06166)\
[SHIV: Reducing supervisor burden in DAgger using support vectors](https://ieeexplore.ieee.org/document/7487167/)\
[Query-efficient imitation learning for end-to-end autonomous driving](https://arxiv.org/abs/1605.06450)\
[Consistent estimators for learning to defer to an expert](https://arxiv.org/abs/2006.01862) [Selective sampling and imitation learning via online regression](https://arxiv.org/abs/2307.04998)

</details>

<details>

<summary>Imitation for visual navigation and autonomous driving \[optional\]</summary>

[Visual path following on a manifold in unstructured three-dimensional terrain](https://ieeexplore.ieee.org/document/5509140/)\
[End-to-end learning for self-driving cars](https://arxiv.org/abs/1604.07316)\
[A machine learning approach to visual perception of forest trails for mobile robots](https://ieeexplore.ieee.org/document/7358076/)\
[Learning monocular reactive UAV control in cluttered natural environments](https://arxiv.org/abs/1211.1690)

</details>

<details>

<summary>Behavioral cloning with energy-based models \[optional\]</summary>

[Implicit behavioral cloning](https://arxiv.org/abs/2109.00137)\
[Revisiting energy based models as policies: ranking noise contrastive estimation and interpolating energy models](https://arxiv.org/abs/2309.05803)

</details>

### Week 2 {#week-2}

**Intro to Optimal Control**\
[Linear Quadratic Regulator](http://people.eecs.berkeley.edu/~somil/Papers/lqrlecture.pdf) and [some examples](https://web.archive.org/web/20210613114312/https://stanford.edu/class/engr108/lectures/control_slides.pdf)\
[Iterative Linear Quadratic Regulator](https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf)\
[Model Predictive Control](https://ieeexplore.ieee.org/document/845037)\
[Ben Recht: An outsider's tour of RL](https://archives.argmin.net/2018/06/25/outsider-rl/) (watch his [ICML'18 tutorial](https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/), too)

<details>

<summary>Intro to model-based RL \[optional\]</summary>

[PILCO: Probabilistic inference for learning control](http://mlg.eng.cam.ac.uk/pilco/)\
[Deep reinforcement learning in a handful of trials using probabilistic dynamics models](https://arxiv.org/abs/1805.12114)\
[Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids](https://arxiv.org/abs/1810.01566)\
[End-to-end differentiable physics for learning and control](https://papers.nips.cc/paper/7948-end-to-end-differentiable-physics-for-learning-and-control)\
[Synthesizing neural network controllers with probabilistic model based reinforcement learning](https://arxiv.org/abs/1803.02291)\
[A survey on policy search algorithms for learning robot controllers in a handful of trials](https://arxiv.org/abs/1807.02303)\
[Reinforcement learning in robotics: a survey](https://journals.sagepub.com/doi/abs/10.1177/0278364913495721)\
[DeepMPC: Learning deep latent features for model predictive control](http://deepmpc.cs.cornell.edu/)\
[Learning latent dynamics for planning from pixels](https://planetrl.github.io/)

</details>

<details>

<summary>Monotonic improvement of the value function \[optional\]</summary>

[Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees](https://arxiv.org/abs/1807.03858)\
[When to Trust Your Model: Model-Based Policy Optimization](https://arxiv.org/abs/1906.08253)

</details>

<details>

<summary>Learning dynamics where it matters for the value function \[optional\]</summary>

[Value Gradient Weighted Model-Based Reinforcement Learning](https://arxiv.org/abs/2204.01464)

</details>

<details>

<summary>Parallelizing MPC on the GPU \[optional\]</summary>

[MPCGPU: Real-Time Nonlinear Model Predictive Control through Preconditioned Conjugate Gradient on the GPU](https://arxiv.org/abs/2309.08079)\
[STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation](https://arxiv.org/abs/2104.13542)

</details>

### Week 3 {#week-3}

**Offline / Batch Reinforcement Learning**\
[Conservative Q-Learning for offline reinforcement learning](https://arxiv.org/abs/2006.04779)\
[IQ-Learn: inverse soft-Q learning for imitation](https://arxiv.org/abs/2106.12142)\
[Scaling data-driven robotics with reward sketching and batch reinforcement learning](https://arxiv.org/abs/1909.12200)\
[Off-policy deep reinforcement learning without exploration](https://arxiv.org/abs/1812.02900)\
[D4RL: Datasets for deep data-driven reinforcement learning](https://arxiv.org/abs/2004.07219)\
[What matters in learning from offline human demonstrations for robot manipulation](https://arxiv.org/abs/2108.03298)\
[NeurIPS 2020 tutorial on offline RL](https://sites.google.com/view/offlinerltutorial-neurips2020/home)

**Transitioning from offline to online RL**\
[Cal-QL: calibrated offline RL pre-training for efficient online fine-tuning](https://arxiv.org/abs/2303.05479)

<details>

<summary>Optional reading</summary>

[Offline reinforcement learning: tutorial, review, and perspectives on open problems](https://arxiv.org/abs/2005.01643)\
[Should I run offline reinforcement learning or behavioral cloning?](https://openreview.net/forum?id=AP1MKT37rJ)\
[Why should I trust you, Bellman? The Bellman error is a poor replacement for value error](https://arxiv.org/abs/2201.12417)\
[A minimalist approach to offline reinforcement learning](https://arxiv.org/abs/2106.06860)\
[Benchmarking batch deep reinforcement learning algorithms](https://arxiv.org/abs/1910.01708)\
[Stabilizing off-policy Q-Learning via bootstrapping error reduction](https://arxiv.org/abs/1906.00949)\
[An optimistic perspective on offline reinforcement learning](https://arxiv.org/abs/1907.04543)\
[COG: Connecting new skills to past experience with offline reinforcement learning](https://arxiv.org/abs/2010.14500)\
[IRIS: Implicit reinforcement without interaction at scale for learning control from offline robot manipulation data](https://arxiv.org/abs/1911.05321)\
[(Batch) reinforcement learning for robot soccer](https://link.springer.com/article/10.1007/s10514-009-9120-4)\
[Instabilities of offline RL with pre-trained neural representation](https://arxiv.org/abs/2103.04947)\
[Targeted environment design from offline data](https://openreview.net/forum?id=Is5Hpwg2R-h)

</details>

### Week 4 {#week-4}

**Imitation learning combined with RL and planning**\
[Learning neural network policies with guided policy search under unknown dynamics](https://papers.nips.cc/paper_files/paper/2014/hash/6766aa2750c19aad2fa1b32f36ed4aee-Abstract.html)\
[Planning with diffusion for flexible behavior synthesis](https://diffusion-planning.github.io/)

**Expert iteration**\
[Thinking fast and slow with deep learning and tree search](https://arxiv.org/abs/1705.08439)\
[Dual policy iteration](https://arxiv.org/abs/1805.10755)\
[Learning to search via retrospective imitation](https://arxiv.org/abs/1804.00846)

**Learning from experts that have privileged information**\
[PLATO: Policy learning using adaptive trajectory optimization](https://arxiv.org/abs/1603.00622)

**Dynamic movement primitives**\
[Dynamic Movement Primitives in robotics: a tutorial survey](https://arxiv.org/abs/2102.03861)\
[Using probabilistic movement primitives in robotics](https://link.springer.com/article/10.1007/s10514-017-9648-7)

<details>

<summary>Making cost-to-go queries to experts (optional)</summary>

[AggreVaTe: Reinforcement and imitation learning via interactive no-regret learning](https://arxiv.org/abs/1406.5979)\
[Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction](https://arxiv.org/abs/1703.01030)\
[Truncated Horizon Policy Search: Combining RL & Imitation Learning](https://arxiv.org/abs/1805.11240)

</details>

<details>

<summary>Imitation can improve search and exploration strategies (optional)</summary>

[Learning to gather information via imitation](https://arxiv.org/abs/1611.04180)\
[Overcoming exploration in reinforcement learning with demonstrations](https://arxiv.org/abs/1709.10089)\
[Data-driven planning via imitation learning](https://arxiv.org/abs/1711.06391)

</details>

### Week 5 {#week-5}

**Inverse reinforcement learning**\
[Maximum entropy inverse reinforcement learning](https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)\
[Guided Cost Learning: Deep inverse optimal control via policy optimization](https://arxiv.org/abs/1603.00448)\
[Bayesian inverse reinforcement learning](https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf)

**Inferring rewards from preferences**\
[Active preference-based learning of reward functions](http://rss2017.lids.mit.edu/program/papers/04/)\
[Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations](https://arxiv.org/abs/1904.06387)

**Inferring constraints from demonstrations**\
[Inverse KKT: Learning cost functions of manipulation tasks from demonstrations](http://journals.sagepub.com/doi/abs/10.1177/0278364917745980)

**Task specification and human-robot dialog**\
[Robots that ask for help: uncertainty alignment for LLM planners](https://arxiv.org/abs/2307.01928)

**Value alignment**\
[Inverse reward design](https://arxiv.org/abs/1711.02827)

<details>

<summary>Optional reading</summary>

[Nonlinear inverse reinforcement learning with gaussian processes](https://papers.nips.cc/paper/4420-nonlinear-inverse-reinforcement-learning-with-gaussian-processes)\
[Maximum margin planning](https://dl.acm.org/citation.cfm?id=1143936)\
[Compatible reward inverse reinforcement learning](https://papers.nips.cc/paper/6800-compatible-reward-inverse-reinforcement-learning.pdf)\
[Learning the preferences of ignorant, inconsistent agents](https://arxiv.org/abs/1512.05832)\
[Imputing a convex objective function](https://ieeexplore.ieee.org/abstract/document/6045410)\
[Better-than-demonstrator imitation learning via automatically-ranked demonstrations](https://arxiv.org/abs/1907.03976)

</details>

<details>

<summary>Applications of Inverse RL (optional)</summary>

[Socially compliant mobile robot navigation via inverse reinforcement learning](http://journals.sagepub.com/doi/10.1177/0278364915619772)\
[Model-based probabilistic pursuit via inverse reinforcement learning](http://www.cim.mcgill.ca/~florian/pdfs/pursuit-icra2018.pdf)\
[First-person activity forecasting with online inverse reinforcement learning](https://arxiv.org/abs/1612.07796)\
[Learning strategies in table tennis using inverse reinforcement learning](https://www.ias.informatik.tu-darmstadt.de/uploads/Site/EditPublication/Muelling_BICY_2014.pdf)\
[Planning-based prediction for pedestrians](https://www.ri.cmu.edu/pub_files/2009/10/planning-based-prediction-pedestrians.pdf)\
[Activity forecasting](http://www.cs.cmu.edu/~kkitani/pdf/KZBH-ECCV12.pdf)\
[Large-scale cost function learning for path planning using deep inverse reinforcement learning](https://journals.sagepub.com/doi/abs/10.1177/0278364917722396)

</details>

### Week 6 {#week-6}

Thanksgiving week. No lecture on Monday.

### Week 7 {#week-7}

**Imitation as program induction**\
[Neural programmer-interpreters](https://arxiv.org/abs/1511.06279)\
[Neural Task Programming: Learning to generalize across hierarchical tasks](https://arxiv.org/abs/1710.01813)

**Modular decomposition of demonstrations into skills**\
[TACO: Learning task decomposition via temporal alignment for control](https://arxiv.org/abs/1803.01840)

**(Hierarchical) imitation of multi-goal tasks**\
[Learning to generalize across long-horizon tasks from human demonstrations](https://arxiv.org/abs/2003.06085)

**Inferring goals, grammars, and planning domains**\
[The motion grammar: analysis of a linguistic method for robot control](https://ieeexplore.ieee.org/document/6457507/)\
[Action understanding as inverse planning](https://www.sciencedirect.com/science/article/pii/S0010027709001607?via%3Dihub)

<details>

<summary>Optional reading</summary>

[Incremental learning of subtasks from unsegmented demonstration](https://ieeexplore.ieee.org/document/5650500/)\
[Inducing probabilistic context-free grammars for the sequencing of movement primitives](https://www.ias.informatik.tu-darmstadt.de/uploads/Team/RudolfLioutikov/lioutikov_movement_pcfg_icra2018.pdf)\
[Neural Task Graphs: Generalizing to unseen tasks from a single video demonstration](https://arxiv.org/abs/1807.03480)\
[Neural program synthesis from diverse demonstration videos](https://shaohua0116.github.io/demo2program/)\
[Automata guided reinforcement learning with demonstrations](https://arxiv.org/abs/1809.06305)\
[A syntactic approach to robot imitation learning using probabilistic activity grammars](https://www.sciencedirect.com/science/article/pii/S0921889013001449)\
[Robot learning from demonstration by constructing skill trees](http://journals.sagepub.com/doi/abs/10.1177/0278364911428653)\
[Learning to sequence movement primitives from demonstrations](https://ieeexplore.ieee.org/document/6943187)\
[Imitation-projected programmatic reinforcement learning](https://arxiv.org/abs/1907.05431)\
[Reinforcement and imitation learning for diverse visuomotor skills](https://arxiv.org/abs/1802.09564)\
[Inferring task goals and constraints using Bayesian nonparametric inverse reinforcement learning](https://arxiv.org/abs/1802.09564)\
[You only demonstrate once: category-level manipulation from single visual demonstration](https://arxiv.org/abs/2201.12716)\
[Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation](https://ieeexplore.ieee.org/abstract/document/9695333)

</details>

### Week 8 {#week-8}

Fall reading week. No lecture.

### Week 9 {#week-9}

**Adversarial imitation learning**\
[GAIL: Generative adversarial imitation learning](https://arxiv.org/abs/1606.03476)\
[Learning robust rewards with adversarial inverse reinforcement learning](https://arxiv.org/abs/1710.11248)\
[InfoGAIL: interpretable imitation learning from visual demonstrations](https://arxiv.org/abs/1703.08840)\
[What matters for adversarial imitation learning?](https://arxiv.org/abs/2106.00672)\
[A divergence minimization perspective on imitation learning methods](https://proceedings.mlr.press/v100/ghasemipour20a.html)\
[Multi-agent generative adversarial imitation learning](https://arxiv.org/abs/1807.09936)

<details>

<summary>Optional reading</summary>

[Model-free imitation learning with policy optimization](https://arxiv.org/abs/1605.08478)\
[Imitation learning via off-policy distribution matching](https://openreview.net/forum?id=Hyg-JC4FDr)\
[Domain adaptive imitation learning](https://arxiv.org/abs/1910.00105)

</details>

### Week 10 {#week-10}

**Teleoperation**\
[RelaxedIK: Real-time synthesis of accurate and feasible robot arm motion](http://www.roboticsproceedings.org/rss14/p43.html)\
[Error-aware imitation learning from teleoperation data for mobile manipulation](https://proceedings.mlr.press/v164/wong22a.html)\
[Controlling assistive robots with learned latent actions](https://ieeexplore.ieee.org/abstract/document/9197197)

**Shared autonomy**\
[Shared autonomy via deep reinforcement learning](http://bair.berkeley.edu/blog/2018/04/18/shared-autonomy/)\
[Shared autonomy via hindsight optimization](https://www.ri.cmu.edu/pub_files/2015/7/Javdani15Hindsight.pdf)

**Imitation with a human in the loop**\
[Learning models for shared control of human-machine systems with unknown dynamics](https://arxiv.org/abs/1808.08268)\
[Human-in-the-loop imitation learning using remote teleoperation](https://arxiv.org/abs/2012.06733)

<details>

<summary>Optional reading</summary>

<b>Optional Reading</b><br/> [Designing robot learners that ask good questions](https://www.cc.gatech.edu/social-machines/papers/cakmak12_hri_active.pdf)\
[Blending human and robot inputs for sliding scale autonomy](https://ieeexplore.ieee.org/document/1513835/)\
[Inferring and assisting with constraints in shared autonomy](https://ieeexplore.ieee.org/document/7799299/)\
[Collaborative control for a robotic wheelchair: evaluation of performance, attention, and workload](https://ieeexplore.ieee.org/document/6135817/)\
[Director: A user interface designed for robot operation with shared autonomy](https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21681)\
[Learning multi-arm manipulation through collaborative teleoperation](https://ieeexplore.ieee.org/abstract/document/9561491)\
[Interactive autonomous driving through adaptation from participation](http://www.cim.mcgill.ca/~mrl/pubs/anqixu/icius2014_apexcommander.pdf)

</details>

### Week 11 {#week-11}

**Imitation learning from videos**\
[K-VIL: Keypoints-based visual imitation learning](https://arxiv.org/abs/2209.03277)\
[Track2Act: Predicting point tracks from internet videos enables generalizable robot manipulation](https://arxiv.org/abs/2405.01527)\
[VideoDex: Learning dexterity from internet videos](https://arxiv.org/abs/2212.04498)

**Motion Retargeting**\
[Robotic Telekinesis: Learning a robotic hand imitator by watching humans on YouTube](https://robotic-telekinesis.github.io/)

**Causal confusion in imitation learning**\
[Causal confusion in imitation learning](https://arxiv.org/abs/1905.11979)

<details>

<summary>Optional reading</summary>

[Towards generalist robot learning from internet video: a survey](https://arxiv.org/abs/2404.19664)\
[AVID: Learning multi-stage tasks via pixel-level translation of human videos](https://arxiv.org/abs/1912.04443)\
[Dreamitate: Real-world visuomotor policy learning via video generation](https://arxiv.org/abs/2406.16862)\
[Understanding Human Hands in Contact at Internet Scale](https://arxiv.org/abs/2006.06669)\
[Zero-shot robot manipulation from passive human videos](https://arxiv.org/abs/2302.02011)\
[SFV: Reinforcement Learning of Physical Skills from Videos](https://arxiv.org/abs/1810.03599)\
[DexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video](https://arxiv.org/abs/2202.00164)\
[Diffusion Reward: Learning Rewards via Conditional Video Diffusion](https://arxiv.org/abs/2312.14134)\
[Task Success is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors](https://arxiv.org/abs/2402.04210)\
[Video Prediction Models as Rewards for Reinforcement Learning](https://arxiv.org/abs/2305.14343)\
[Giving Robots a Hand: Learning generalizable manipulation with eye-in-hand human video demonstrations](https://giving-robots-a-hand.github.io/)\
[Estimating Q(s,s') with Deep Deterministic Dynamics Gradients](https://arxiv.org/abs/2002.09505)\
[RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818)\
[Human-to-robot imitation in the wild](https://human2robot.github.io/)\
[Vision-Language Models as Success Detectors](https://arxiv.org/abs/2303.07280)\
[Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos](https://arxiv.org/abs/2103.16817)\
[Semantic Visual Navigation by Watching YouTube Videos](https://arxiv.org/abs/2006.10034)\
[Robotic Offline RL from Internet Videos via Value-Function Pre-Training](https://arxiv.org/abs/2309.13041)\
[HumanPlus: Humanoid Shadowing and Imitation from Humans](https://arxiv.org/abs/2406.10454)\
[ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos](https://arxiv.org/abs/2404.15709)\
[Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation](https://arxiv.org/abs/2409.16283)\
[OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics](https://arxiv.org/abs/2401.12202)\
[Unifying 3D Representation and Control of Diverse Robots with a Single Camera](https://arxiv.org/abs/2407.08722)

</details>

### Week 12 {#week-12}

**Representation learning for imitation**\
[Generalization guarantees for imitation learning](https://arxiv.org/abs/2008.01913)\
[Provable representation learning for imitation with contrastive Fourier features](https://arxiv.org/abs/2105.12272)\
[TRAIL: near-optimal imitation learning with suboptimal data](https://arxiv.org/abs/2110.14770)\
[Representation matters: offline pretraining for sequential decision making](http://proceedings.mlr.press/v139/yang21h.html)\
[Self-supervised correspondence in visuomotor policy learning](https://arxiv.org/abs/1909.06933)\
[The surprising effectiveness of representation learning for visual imitation](https://arxiv.org/abs/2112.01511)

**Generalization and safety guarantees for imitation**\
[Provable guarantees for generative behavior cloning: bridging low-level stability and high-level behavior](https://arxiv.org/abs/2307.14619)\
[Imitation learning with stability and safety guarantees](https://arxiv.org/abs/2012.09293)

### Week 13 {#week-13}

Project presentations

### Week 14 {#week-14}

Final project submission