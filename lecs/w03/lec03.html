<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Florian Shkurti">
  <title>CSC2626 - Fall 2024 – CSC2626 Imitation Learning for Robotics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="../style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC2626 Imitation Learning for Robotics – CSC2626 - Fall 2024">
<meta property="og:description" content="Week 3: Offline/Batch Reinforcement Learning">
<meta property="og:site_name" content="CSC2626 - Fall 2024">
<meta name="twitter:title" content="CSC2626 Imitation Learning for Robotics – CSC2626 - Fall 2024">
<meta name="twitter:description" content="Week 3: Offline/Batch Reinforcement Learning">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC2626 Imitation Learning for Robotics</h1>
  <p class="subtitle">Week 3: Offline/Batch Reinforcement Learning</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Florian Shkurti 
</div>
</div>
</div>

</section>
<section id="todays-agenda" class="slide level2">
<h2>Today’s agenda</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>• Reinforcement Learning Terminology</p>
<p>• Distribution Shift in Offline RL</p>
<p>• Offline RL with Policy Constraints</p>
<p>• Offline RL with Conservative Q-Estimates</p>
</div><div class="column" style="width:40%;">
<p><img data-src="images/robot.png" style="width:80.0%"></p>
</div></div>

<aside><div>
<p>Acknowledgments<br>
Today’s slides borrow very heavily from: Sergey Levine &amp; Aviral Kumar (CSC285 and NeurIPS Offline RL workshop), Joelle Pineau (DLSS’16)</p>
</div></aside></section>
<section id="rl-terminology" class="slide level2">
<h2>RL Terminology</h2>
<p>• Episodic vs Non-episodic</p>
<p>• Tabular vs Function Approximation</p>
<p>• Exploration vs Exploitation</p>
<p>• Model-based vs Model-free</p>
<p>• Policy Optimization vs Value Function Estimation</p>
<p>• On-policy vs Off-Policy</p>
<p>• Batch (Offline) vs Online</p>
</section>
<section id="episodic-vs-non-episodic-rl-methods" class="slide level2">
<h2>Episodic vs Non-episodic RL methods</h2>
<p><u>Episodic</u>: optimize expected reward-to-go for finite time horizon</p>
<p><span class="math display">\[
V_T^{\pi}(s_0) = \mathbb{E}_{a_t \sim \pi_t(a|s_t), s_{t+1} \sim p(s_{t+1}|s_t, a_t)} \left[ \sum_{t=0}^{T} r(s_t, a_t) \right]
\]</span></p>
<p>Often need to have a reset mechanism to bring back the system to state <span class="math inline">\(s_0\)</span></p>
<p><br></p>
<div class="fragment">
<p><u>Non-episodic</u>: optimize expected discounted reward-to-go for infinite time horizon, i.e.&nbsp;a task may go on forever, no resets</p>
<p><span class="math display">\[
V^{\pi}(s_0) = \mathbb{E}_{a_t \sim \pi(a|s_t), s_{t+1} \sim p(s_{t+1}|s_t, a_t)} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right] \qquad \gamma \in (0, 1)
\]</span></p>
</div>
</section>
<section id="tabular-vs-function-approximation-methods" class="slide level2">
<h2>Tabular vs Function Approximation Methods</h2>
<p><u>Tabular</u>: discretize states and actions, represent them as a grid, and compute a policy or value function directly on states and actions (typically can enumerate small number of states and actions)</p>
<p><br></p>
<div class="fragment">
<p><u>Function approximation</u>: discrete or continuous states and actions, associate a feature representation <span class="math inline">\(\phi(s, a)\)</span> to each state action pair and compute a policy or value function in terms of features (typically useful for continuous or very large, but discrete, state-action spaces)</p>
</div>
</section>
<section id="exploration-vs-exploitation-in-rl" class="slide level2">
<h2>Exploration vs Exploitation in RL</h2>
<p><u>Exploitation</u>: act by using current estimates of dynamics and value function to improve task performance in the short term</p>
<div class="fragment">
<p><br></p>
<p><u>Exploration</u>: act to improve current estimates of dynamics and value function to improve task performance in the long-term, even if it hurts short-term performance</p>
</div>
</section>
<section id="model-based-vs-model-free-rl-methods" class="slide level2">
<h2>Model-based vs Model-free RL Methods</h2>
<p><u>Model-based</u>: estimate predictive models of the instantaneous reward as well as the dynamics model, and act by making value function predictions based on these models and optimizing the estimated value function. I.e. plan with “imagined” state transition and reward data.</p>
<p><u>Model-free</u>: do not try to estimate models of reward or dynamics, but interact with the environment to optimize policy. Plan with real state transition and reward data.</p>
<p><u>Mixed</u>: only trust your dynamics and reward model (imagined data) for a few steps in the near future, and then use real data.</p>
</section>
<section id="policy-optimization-vs-value-function-estimation" class="slide level2">
<h2>Policy Optimization vs Value Function Estimation</h2>

<img data-src="images/policy-optimization.png" class="r-stretch"><p>Credit: John Schulman</p>
</section>
<section id="on-policy-vs-off-policy-methods" class="slide level2">
<h2>On-policy vs Off-policy Methods</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="images/on-policy.png"></p>
</div><div class="column" style="width:70%;">
<p><br></p>
<p><u>On-policy RL methods</u>: improve the policy that acts on the environment using data collected from that same policy</p>
</div></div>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="images/off-policy.png"></p>
</div><div class="column" style="width:70%;">
<p><br></p>
<p><u>Off-policy RL methods</u>: improve the policy that acts on the environment using data collected from <strong><em>any policy</em></strong></p>
</div></div>
</section>
<section id="batch-offline-vs-online-methods" class="slide level2">
<h2>Batch (Offline) vs Online Methods</h2>
<div class="columns">
<div class="column quarto-layout-panel" data-layout="[[50,50], [100]]" style="width:50%;">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="images/on-policy.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="images/off-policy.png"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img data-src="images/offline-reinforcement.png"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><u>Online RL methods</u>: Can collect data over multiple rounds. Data distribution changes over time.</p>
<p><br><br><br></p>
<p><u>Batch/Offline RL methods</u>: Can collect data only once from any policy. Data distribution is stationary.</p>
</div></div>
</section>
<section id="batch-offline-vs-online-methods-1" class="slide level2">
<h2>Batch (Offline) vs Online Methods</h2>
<div class="quarto-layout-panel" data-layout="[1,1]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="images/offline-reinforcement.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="images/online-methods.png"></p>
</div>
</div>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>

<!-- Formalism & Notation -->
<img data-src="images/supervised-learning.png" class="r-stretch"></section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="images/formalism.png"></p>
</div><div class="column small-math" style="width:40%;">
<p><span class="math inline">\(\mathbf{s} \in \mathcal{S}\)</span> – discrete or continuous state</p>
<p><span class="math inline">\(\mathbf{a} \in \mathcal{A}\)</span> – discrete or continuous action</p>
<p><span class="math inline">\(\tau = \{s_0, a_0, s_1, a_1, \ldots, s_T, a_T\}\)</span> - <em>trajectory</em></p>
<p><span class="math inline">\(\underbrace{\pi(s_0, a_0, \ldots, s_T, a_T)}_{\pi(\tau)} = p(s_1) \prod_{t=0}^{T} \pi(a_t | s_t) p(s_{t+1} | s_t, a_t)\)</span></p>
<p><span class="math inline">\(d_t^{\pi}(s_t)\)</span> – state marginal of <span class="math inline">\(\pi(\tau)\)</span> at <span class="math inline">\(t\)</span></p>
<p><span class="math inline">\(d^{\pi}(s) = \frac{1}{1-\gamma} \sum_{t=0}^{T} \gamma^t d_t^{\pi}(s_t) \quad \text{– "visitation frequency"}\)</span></p>
</div></div>
<p><span class="math inline">\(Q^{\pi}(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p(s_{t+1} | s_t, a_t), a_{t+1} \sim \pi(a_{t+1} | s_{t+1})} \left[Q^{\pi}(s_{t+1}, a_{t+1})\right]\)</span></p>
<p><span class="math inline">\(V^{\pi}(s_t) = \mathbb{E}_{a_t \sim \pi(a_t | s_t)} \left[Q^{\pi}(s_t, a_t)\right]\)</span></p>
</section>
<section id="on-policy-actor-critic-with-function-approximation" class="slide level2">
<h2>On-policy actor-critic with function approximation</h2>
<p><img data-src="images/actor-policy.png" height="200"></p>
<div class="fragment">
<div class="columns">
<div class="column" style="width:10%;">
<p><img data-src="images/arrow.png" height="130"></p>
</div><div class="column small-math" style="width:90%;">
<ol type="1">
<li>update <span class="math inline">\(Q_\phi\)</span> to decrease <span class="math inline">\(E_{s \sim d^{\pi_\theta}(s), a \sim \pi_\theta(a|s)} \left[\left(Q_\phi(s,a) - (r(s,a) + \gamma E_{\pi_\theta}[Q_\phi(s',a')])\right)^2\right]\)</span></li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li>update <span class="math inline">\(\pi_\theta\)</span> to increase <span class="math inline">\(E_{s \sim d^{\pi_\theta}(s), a \sim \pi_\theta(a|s)} [Q_\phi(s,a)]\)</span></li>
</ol>
</div></div>
</div>
</section>
<section id="off-policy-actor-critic-with-function-approximation" class="slide level2">
<h2>Off-policy actor-critic with function approximation</h2>
<p><img data-src="images/actor-policy.png" height="200"></p>
<div class="columns">
<div class="column" style="width:10%;">
<p><img data-src="images/arrow.png" height="130"></p>
</div><div class="column small-math" style="width:90%;">
<ol type="1">
<li>update <span class="math inline">\(Q_\phi\)</span> to decrease <span class="math inline">\(E_{s \sim d^{\pi_\beta}(s), a \sim \pi_\beta(a|s)} \left[\left(Q_\phi(s,a) - (r(s,a) + \gamma E_{\pi_\theta}[Q_\phi(s',a')])\right)^2\right]\)</span></li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li>update <span class="math inline">\(\pi_\theta\)</span> to increase <span class="math inline">\(E_{s \sim d^{\pi_\beta}(s), a \sim \pi_\theta(a|s)} [Q_\phi(s,a)]\)</span></li>
</ol>
</div></div>
</section>
<section id="q-learningfitted-q-iteration-with-function-approximation-off-policy" class="slide level2">
<h2>Q-Learning/Fitted Q-Iteration with function approximation (off-policy)</h2>
<div class="quarto-figure quarto-figure-right">
<figure>
<p><img data-src="images/critic.png" class="quarto-figure quarto-figure-right" height="200"></p>
</figure>
</div>
<div class="columns">
<div class="column" style="width:10%;">
<p><img data-src="images/arrow.png" height="130"></p>
</div><div class="column small-math" style="width:90%;">
<ol type="1">
<li>update <span class="math inline">\(Q_\phi\)</span> to decrease <span class="math inline">\(E_{s \sim d^{\pi_\beta}(s), a \sim \pi_\beta(a|s)} \left[\left(Q_\phi(s,a) - (r(s,a) + \gamma E_{\pi_\theta}[Q_\phi(s',a')])\right)^2\right]\)</span></li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li><del>update</del> <span class="math inline">\(\pi_\theta\)</span> to increase <span class="math inline">\(E_{s \sim d^{\pi_\beta}(s), a \sim \pi_\theta(a|s)} [Q_\phi(s,a)]\)</span></li>
</ol>
<p>choose <span class="math inline">\(\pi\)</span> according to: <span class="math inline">\(\pi(a_t | s_t) = \begin{cases} 1 &amp; \text{if } a_t = \arg\max_{a_t} Q_\phi(s_t, a_t) \\ 0 &amp; \text{otherwise} \end{cases}\)</span></p>
</div></div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>

<img data-src="images/bellman.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="section-3" class="slide level2">
<h2></h2>

<img data-src="images/conf-paper.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="section-4" class="slide level2">
<h2></h2>

<img data-src="images/logistic-learning.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="policy-gradients-on-policy" class="slide level2">
<h2>Policy gradients (on-policy)</h2>
<p>RL objective: <span class="math inline">\(\max_{\pi} \sum_{t=0}^{T} E_{s_t \sim d^{\pi}(s), a_t \sim \pi(a|s)} [\gamma^t r(s_t, \mathbf{a}_t)]\)</span></p>
<p><span class="math inline">\(\qquad \qquad \qquad \qquad \qquad \qquad \qquad \uparrow\)</span> <span class="small-math">exactly the same thing!</span></p>
<p><span class="math display">\[J(\theta) = E_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t=0}^{T} \gamma^t r(s_t, a_t)\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \gamma^t r(s_{t,i}, a_{t,i})\]</span></p>
<p><span class="math display">\[\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta(\tau)} \left[\nabla_\theta \log \pi_\theta(\tau) \sum_{t=0}^{T} \gamma^t r(s_t, \mathbf{a}_t)\right] \text{ simple algebraic derivation}\]</span></p>
<p><span class="math display">\[= E_{\tau \sim \pi_\theta(\tau)} \left[\left(\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t)\right) \left(\sum_{t=0}^{T} \gamma^t r(s_t, \mathbf{a}_t)\right)\right] \text{ from definition of } \tau\]</span></p>
</section>
<section id="policy-gradients-on-policy-1" class="slide level2">
<h2>Policy gradients (on-policy)</h2>
<div class="columns">
<div class="column small-math" style="width:70%;">
<p><br></p>
<p>RL objective: <span class="math inline">\(\max_{\pi} \sum_{t=0}^{T} E_{s_t \sim d^{\pi}(s), a_t \sim \pi(a|s)} [\gamma^t r(s_t, \mathbf{a}_t)]\)</span></p>
<p><span class="math inline">\(\qquad \qquad \qquad \qquad \qquad \nearrow\)</span> <span class="small-math">exactly the same thing!</span></p>
<p><span class="math inline">\(J(\theta) = E_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t=0}^{T} \gamma^t r(s_t, a_t)\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \gamma^t r(s_{t,i}, a_{t,i})\)</span></p>
<p><span class="math inline">\(\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta(\tau)} \left[\nabla_\theta \log \pi_\theta(\tau) \sum_{t=0}^{T} \gamma^t r(s_t, \mathbf{a}_t)\right] \text{ simple algebraic derivation}\)</span></p>
<p>(REINFORCE gradient estimator)</p>
</div><div class="column" style="width:30%;">
<p><img data-src="images/policy-gradients.png"></p>
</div></div>
</section>
<section id="todays-agenda-1" class="slide level2">
<h2>Today’s agenda</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>• Reinforcement Learning Terminology</p>
<p>• Distribution Shift in Offline RL</p>
<p>• Offline RL with Policy Constraints</p>
<p>• Offline RL with Conservative Q-Estimates</p>
</div><div class="column" style="width:40%;">
<p><img data-src="images/robot.png" style="width:80.0%"></p>
</div></div>

<aside><div>
<p>Acknowledgments<br>
Today’s slides borrow very heavily from: Sergey Levine &amp; Aviral Kumar (CSC285 and NeurIPS Offline RL workshop), Joelle Pineau (DLSS’16)</p>
</div></aside></section>
<section id="qt-opt-roughly-continuous-action-q-learning" class="slide level2">
<h2>QT-Opt (roughly: continuous-action Q-Learning)</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/W4joe3zzglU?rel=0" width="700" height="450" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<!-- <iframe data-external="1" src="https://www.youtube.com/embed/W4joe3zzglU?rel=0&amp;enablejsapi=1" width="700" height="450" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> -->
</section>
<section id="qt-opt-roughly-continuous-action-q-learning-1" class="slide level2">
<h2>QT-Opt (roughly: continuous-action Q-Learning)</h2>

<img data-src="images/qt-opt.png" class="r-stretch"></section>
<section id="section-5" class="slide level2">
<h2></h2>

<img data-src="images/does-it-work.png" class="r-stretch"></section>
<section id="section-6" class="slide level2">
<h2></h2>

<img data-src="images/modern-offline.png" class="r-stretch"></section>
<section id="section-7" class="slide level2">
<h2></h2>

<img data-src="images/offline-reinforcement-learning.png" class="r-stretch"></section>
<section id="section-8" class="slide level2">
<h2></h2>

<img data-src="images/qfunction-update.png" class="r-stretch"></section>
<section id="section-9" class="slide level2">
<h2></h2>

<img data-src="images/distribution-shift.png" class="r-stretch"></section>
<section id="section-10" class="slide level2">
<h2></h2>

<img data-src="images/erroneous-backup.png" class="r-stretch"></section>
<section id="section-11" class="slide level2">
<h2></h2>

<img data-src="images/distribution-shift-graph.png" class="r-stretch"></section>
<section id="section-12" class="slide level2">
<h2></h2>

<img data-src="images/distribution-shift-offline.png" class="r-stretch"></section>
<section id="todays-agenda-2" class="slide level2">
<h2>Today’s agenda</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>• Reinforcement Learning Terminology</p>
<p>• Distribution Shift in Offline RL</p>
<p>• Offline RL with Policy Constraints</p>
<p>• Offline RL with Conservative Q-Estimates</p>
</div><div class="column" style="width:40%;">
<p><img data-src="images/robot.png" style="width:80.0%"></p>
</div></div>

<aside><div>
<p>Acknowledgments<br>
Today’s slides borrow very heavily from: Sergey Levine &amp; Aviral Kumar (CSC285 and NeurIPS Offline RL workshop), Joelle Pineau (DLSS’16)</p>
</div></aside></section>
<section id="addressing-distribution-shift-via-pessimism" class="slide level2">
<h2>Addressing Distribution Shift via Pessimism</h2>

<img data-src="images/distribution-shift-pessimism.png" class="r-stretch"></section>
<section id="different-types-of-policy-constraint-methods" class="slide level2">
<h2>Different Types of Policy Constraint Methods</h2>

<img data-src="images/policy-constraint.png" class="r-stretch"></section>
<section id="note-kl-divergence-is-not-symmetric" class="slide level2">
<h2>Note: KL divergence is not symmetric</h2>

<img data-src="images/kl-divergence.png" class="r-stretch"></section>
<section id="section-13" class="slide level2">
<h2></h2>

<img data-src="images/policy-constraint-methods.png" class="r-stretch"></section>
<section id="how-should-we-evaluate-offline-rl-methods" class="slide level2">
<h2>How should we evaluate offline RL methods</h2>

<img data-src="images/offline-rl-methods.png" class="r-stretch"></section>
<section id="evaluating-offline-rl-d4rl" class="slide level2">
<h2>Evaluating Offline RL – D4RL</h2>

<img data-src="images/d4rl.png" class="r-stretch"></section>
<section id="evaluating-offline-rl-d4rl-1" class="slide level2">
<h2>Evaluating Offline RL – D4RL</h2>

<img data-src="images/d4rl-pics.png" class="r-stretch"></section>
<section id="standardized-benchmark-for-offline-rl" class="slide level2">
<h2>Standardized Benchmark for Offline RL</h2>

<img data-src="images/standardized-benchmark.png" class="r-stretch"></section>
<section id="section-14" class="slide level2">
<h2></h2>

<img data-src="images/policy-constraint-dataset.png" class="r-stretch"></section>
<section id="value-function-regularization-for-offline-rl" class="slide level2">
<h2>Value Function Regularization for Offline RL</h2>

<img data-src="images/value-function.png" class="r-stretch"></section>
<section id="learning-lower-bounded-q-values" class="slide level2">
<h2>Learning Lower-Bounded Q-values</h2>

<img data-src="images/qlearning-alg.png" class="r-stretch"></section>
<section id="a-tighter-lower-bound" class="slide level2">
<h2>A Tighter Lower Bound</h2>

<img data-src="images/lower-bound.png" class="r-stretch"></section>
<section id="practical-cql-algorithm" class="slide level2">
<h2>Practical CQL Algorithm</h2>

<img data-src="images/cql-algo.png" class="r-stretch"></section>
<section id="section-15" class="slide level2">
<h2></h2>

<img data-src="images/cql-empirically.png" class="r-stretch"></section>
<section id="the-need-for-safe-exploration-in-rl-for-robotics" class="slide level2">
<h2><span class="medium">The need for safe exploration in RL for robotics</span></h2>
<div class="columns">
<div class="column" style="width:25%;">
<p><img data-src="images/robot1.png" style="width:100.0%"></p>
</div><div class="column" style="width:25%;">
<p><img data-src="images/robot2.png" style="width:100.0%"></p>
</div><div class="column" style="width:25%;">
<iframe data-external="1" src="https://www.youtube.com/embed/eRwTbRtnT1I?rel=0" width="300" height="200" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div><div class="column" style="width:25%;">
<iframe data-external="1" src="https://www.youtube.com/embed/VfDYUeyzHqc?rel=0" width="200" height="200" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div></div>
<div class="fragment">
<p>When applying RL to robotics we need to guarantee that the algorithm will not visit <strong>unsafe states</strong> very often during learning.</p>
</div>
<div class="fragment">
<p><img data-src="images/2-theories.png" height="140"></p>
</div>
</section>
<section id="the-need-for-safe-exploration-in-rl-for-robotics-1" class="slide level2">
<h2><span class="medium">The need for safe exploration in RL for robotics</span></h2>
<div class="columns">
<div class="column" style="width:25%;">
<p><img data-src="images/robot1.png" style="width:100.0%"></p>
</div><div class="column" style="width:25%;">
<p><img data-src="images/robot2.png" style="width:100.0%"></p>
</div><div class="column" style="width:25%;">
<iframe data-external="1" src="https://www.youtube.com/embed/eRwTbRtnT1I?rel=0" width="300" height="200" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div><div class="column" style="width:25%;">
<iframe data-external="1" src="https://www.youtube.com/embed/VfDYUeyzHqc?rel=0" width="200" height="200" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div></div>
<p>When applying RL to robotics we need to guarantee that the algorithm will not visit <strong>unsafe states</strong> very often during learning.</p>
<p><img data-src="images/2theories-method.png"></p>
</section>
<section id="the-need-for-safe-exploration-in-rl-for-robotics-2" class="slide level2">
<h2>The need for safe exploration in RL for robotics</h2>
<p><img data-src="images/robot1.png" width="250"> <img data-src="images/robot2.png" width="200"> <iframe data-external="1" src="https://www.youtube.com/embed/eRwTbRtnT1I?rel=0" width="300" height="200" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> <iframe data-external="1" src="https://www.youtube.com/embed/VfDYUeyzHqc?rel=0" width="200" height="200" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>When applying RL to robotics we need to guarantee that the algorithm will not visit <strong>unsafe states</strong> very often during learning.</p>
<div style="color:red;">
<p>Our proposed method:</p>
<p>The learned policy should be safe at each iteration, not just when optimization has converged</p>
</div>
</section>
<section id="our-solution-constrained-safety-critics-csc" class="slide level2">
<h2>Our solution: Constrained Safety Critics (CSC)</h2>
<div class="columns">
<div class="column small-math" style="width:40%;">
<p><span class="math inline">\(\text{maximize}_{\theta} \mathcal{V}_{\text{task}}^{\pi_\theta}(s_0)\)</span></p>
<p>subject to <span class="math inline">\(\boxed{\mathcal{V}_{\text{accident}}^{\pi_\theta}(s_0) &lt; \epsilon} \qquad\)</span></p>
<p><span class="math inline">\(\text{KL}(\pi_\theta || \pi_{\text{prev}}) &lt; \delta \qquad\)</span></p>
</div><div class="column small-math" style="width:60%;">
<p><br></p>
<p><span style="color:green;"><strong>Probability of an accident should be upper bounded</strong></span></p>
<div class="fragment">
<p><span style="color:red;"><strong>Problem: estimation errors for value function</strong> <span class="math inline">\(\mathcal{V}_{\text{accident}}^{\pi_\theta}(s_0)\)</span> <strong>can make the constraint falsely confident</strong></span></p>
</div>
</div></div>
<p><br></p>
<div class="fragment">
<p><span class="small-math" style="color: green;"><strong>Fix: use the CQL update rule to guarantee that the probability of an accident is overestimated</strong></span></p>
<div class="small-math">
<p><span class="math inline">\(\min_Q \alpha \mathbb{E}_{s \sim \mathcal{D}} \left[\log \sum_a \exp(Q(s,a)) - \mathbb{E}_{a \sim \hat{\pi}_\beta(a|s)} [Q(s,a)]\right] + \frac{1}{2} \mathbb{E}_{s,a,s' \sim \mathcal{D}} \left[\left(Q - \hat{B}^{\pi} \hat{Q}^k\right)^2\right] \Rightarrow \mathcal{V}_{\text{accident}}^{\pi_\theta}(s_0) \leq \hat{\mathcal{V}}_{\text{accident}}^{\pi_\theta}(s_0) &lt; \epsilon\)</span></p>
</div>
</div>
<p><br></p>
<p><span class="small-math">Conservative Safety Critics for Exploration, <a href="https://arxiv.org/abs/2010.14497" class="uri">https://arxiv.org/abs/2010.14497</a>, Bharadhwaj, Kumar, Rhinehart, Levine, Shkurti, Garg</span></p>
</section>
<section id="results-fewer-accidents" class="slide level2">
<h2>Results: fewer accidents</h2>

<img data-src="images/results-graph.png" class="r-stretch"><p><span class="small-math">Constrained Policy Optimization, <a href="https://arxiv.org/abs/1705.10528" class="uri">https://arxiv.org/abs/1705.10528</a>, Achiam, Held, Tamam, Abbeel</span></p>
</section>
<section id="results-task-value-vs-safety" class="slide level2">
<h2>Results: task value vs safety</h2>



<img data-src="images/taskvalue-safety.png" class="r-stretch"></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://csc2626.github.io/website/" target="_blank" style="font-size:0.8em; bottom: -5px;">↩︎ Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"src":"chalkboard.json","boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/csc2626\.github\.io\/website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script type="text/javascript" src="../custom_curtain.js"></script>
    

</body></html>