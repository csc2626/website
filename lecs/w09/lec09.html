<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Florian Shkurti">
  <title>CSC2626 - Fall 2024 ‚Äì CSC2626 Imitation Learning for Robotics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="../style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC2626 Imitation Learning for Robotics ‚Äì CSC2626 - Fall 2024">
<meta property="og:description" content="Week 9: Adversarial Imitation Learning">
<meta property="og:site_name" content="CSC2626 - Fall 2024">
<meta name="twitter:title" content="CSC2626 Imitation Learning for Robotics ‚Äì CSC2626 - Fall 2024">
<meta name="twitter:description" content="Week 9: Adversarial Imitation Learning">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC2626 Imitation Learning for Robotics</h1>
  <p class="subtitle">Week 9: Adversarial Imitation Learning</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Florian Shkurti 
</div>
</div>
</div>

</section>
<section id="todays-agenda" class="slide level2">
<h2>Today‚Äôs agenda</h2>
<p>‚Ä¢ Adversarial optimization for imitation learning (GAIL)</p>
<p>‚Ä¢ Adversarial optimization with multiple inferred behaviors (InfoGAIL)</p>
<p>‚Ä¢ Model based adversarial optimization (MGAIL)</p>
<p>‚Ä¢ Multi-agent imitation learning</p>

<aside><div>
<p>Acknowledgments<br>
Today‚Äôs slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong</p>
</div></aside></section>
<section id="detour-solving-mdps-via-linear-programming" class="slide level2">
<h2>Detour: solving MDPs via linear programming</h2>
<p><span class="math inline">\(\underset{v}{\arg\min} \quad \underset{s \in S}{\sum} d_s v_s\)</span></p>
<p><span class="math inline">\(\text{subject to:} \quad v_s \geq r_{s,a} + \gamma \underset{s' \in S}{\sum} T_{s,a}^{s'} v_{s'} \quad \forall s \in S, a \in A\)</span></p>
<p><span class="math inline">\(\qquad \qquad \quad d\)</span> is the initial state distribution.</p>
<p><br><br></p>
<div class="right-align">
<p>Optimal policy</p>
<p><span class="math inline">\(\pi^*(s) = \underset{a \in A}{\text{argmax}} Q^*(s, a)\)</span></p>
</div>
</section>
<section id="detour-solving-mdps-via-linear-programming-1" class="slide level2">
<h2>Detour: solving MDPs via linear programming</h2>
<p><span class="math inline">\(\underset{\mu}{\text{argmax}} \underset{s \in S, a \in A}{\sum} \mu_{s,a} r_{s,a}\)</span></p>
<p><span class="math inline">\(\text{subject to} \quad \underset{a \in A}{\sum} \mu_{s',a} = d_{s'} + \gamma \underset{s \in S, a \in A}{\sum} T_{s,a}^{s'} \mu_{s,a} \quad \forall s' \in S\)</span></p>
<p><span class="math inline">\(\qquad \qquad \quad \mu_{s,a} \geq 0\)</span></p>
<p><br></p>
<div class="right-align">
<p>Discounted state action counts / occupancy measure</p>
<p><span class="math inline">\(\mu(s,a) = \underset{t=0}{\sum}^{\infty} \gamma^t p(s_t = s, a_t = a)\)</span></p>
<p>Optimal policy</p>
<p><span class="math inline">\(\pi^*(s) = \underset{a \in A}{\text{argmax}} \, \mu(s,a)\)</span></p>
</div>
</section>
<section id="detour-solving-mdps-via-linear-programming-2" class="slide level2">
<h2>Detour: solving MDPs via linear programming</h2>
<div class="columns">
<div class="column" style="width:85%;">
<p><span class="math inline">\(\underset{v}{\text{argmin}} \underset{s \in S}{\sum} d_s v_s\)</span></p>
<p><span class="math inline">\(\text{subject to} \quad v_s \geq r_{s,a} + \gamma \underset{s' \in S}{\sum} T_{s,a}^{s'} v_{s'} \quad \forall s \in S, a \in A\)</span></p>
<p><span class="math inline">\(\qquad \qquad \quad d\)</span> is the initial state distribution</p>
<p><br><br></p>
<p><span class="math inline">\(\underset{\mu}{\text{argmax}} \underset{s \in S, a \in A}{\sum} \mu_{s,a} r_{s,a}\)</span></p>
<p><span class="math inline">\(\text{subject to} \quad \underset{a \in A}{\sum} \mu_{s',a} = d_{s'} + \underset{s \in S, a \in A}{\sum} T_{s,a}^{s'} \mu_{s,a} \quad \forall s' \in S\)</span></p>
<p><span class="math inline">\(\qquad \qquad \quad \mu_{s,a} \geq 0\)</span></p>
</div><div class="column" style="width:15%;">
<p>Primal LP</p>
<p><br><br><br><br></p>
<p><br></p>
<p>Dual LP</p>
</div></div>
</section>
<section id="background" class="slide level2">
<h2>Background</h2>
<p>‚ñ∂ <strong>Definitions:</strong> Action space <span class="math inline">\(\mathcal{A}\)</span> and sample space <span class="math inline">\(\mathcal{S}\)</span>. <span class="math inline">\(\Pi\)</span> is the set of all policies. Also assume <span class="math inline">\(P(s'|s,a)\)</span> is the dynamics model. In this paper, <span class="math inline">\(\pi_E\)</span> denotes the expert policy.</p>
<p>‚ñ∂ <strong>Imitation Learning:</strong> Learning to perform a task from expert demonstrations without querying the expert while training.</p>
<p>‚ñ∂ <strong>Behavioral cloning:</strong> Its success depends on large amounts of data.</p>
<p>‚ñ∂ <strong>Inverse RL:</strong> The paper adopts the maximum causal entropy IRL which fits a cost function <span class="math inline">\(c\)</span> with the following problem.</p>
<p><span class="math display">\[\pi^* = \arg \min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_\pi[c(s,a)]\]</span></p>
<p><span class="math display">\[\tilde{c} = \arg \max_{c \in \mathcal{C}} \mathbb{E}_{\pi^*}[c(s,a)] - \mathbb{E}_{\pi_E}[c(s,a)]\]</span></p>
<p>where <span class="math inline">\(H(\pi) = \mathbb{E}_\pi[-\log \pi(a|s)]\)</span> is the entropy of the policy.</p>
</section>
<section id="formulation" class="slide level2">
<h2>Formulation</h2>
<p>‚ñ∂ We first study the policies found by RL on costs learned by IRL on the largest possible set of cost functions <span class="math inline">\(\mathcal{C} = \{c : S \times A \to \mathbb{R}\}\)</span>.</p>
<p>‚ñ∂ Also need to define a convex cost function regularizer <span class="math inline">\(\psi : \mathbb{R}_{S \times A} \to \mathbb{R}\)</span>, which turns out to be important in this paper.</p>
<p>‚ñ∂ Re-write the Eq. 1 as the following:</p>
<p><span class="math display">\[IRL_\psi(\pi_E) = \arg \max_{c \in \mathcal{C}} -\psi(c) + (\min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_\pi[c(s,a)])\]</span></p>
<p><span class="math display">\[- \mathbb{E}_{\pi_E}[c(s,a)]\]</span></p>
<p>‚ñ∂ Define <span class="math inline">\(RL(c) = \arg \min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_\pi[c(s,a)]\)</span>.<br>
Let <span class="math inline">\(\tilde{c} \in IRL_\psi(\pi_E)\)</span>. We are interested in characterizing the induced policy <span class="math inline">\(RL(\tilde{c})\)</span>.</p>
</section>
<section id="derivations" class="slide level2">
<h2>Derivations</h2>
<p>‚ñ∂ It is easier to characterize <span class="math inline">\(RL(\tilde{c})\)</span> if we transform optimization problems over policies into convex problems.</p>
<p>‚ñ∂ So the paper introduces an occupancy measure <span class="math inline">\(\rho_\pi : S \times A \to \mathbb{R}\)</span>:</p>
<p><span class="math display">\[\rho_\pi(s,a) = \pi(a|s) \sum_{t=0}^{\infty} \gamma^t P(s_t = s|\pi) \qquad (1)\]</span></p>
<p>It can be interpreted as the distribution of state-action pairs when roll-out with policy <span class="math inline">\(\pi\)</span>.</p>
<p>‚ñ∂ There is an <strong>one-to-one</strong> correspondence between policy and occupancy measure. It also allows us to re-write the expected cost as</p>
<p><span class="math display">\[\mathbb{E}_\pi[c(s,a)] = \sum_{s,a} \rho_\pi(s,a)c(s,a) \qquad (2)\]</span></p>
</section>
<section id="derivations-1" class="slide level2">
<h2>Derivations</h2>
<p>‚ñ∂ <strong>Lemma 1:</strong> If we define</p>
<p><span class="math display">\[\hat{H}(\rho) = -\sum_{s,a} \rho(s,a) \log(\rho(s,a)/\sum_{a'} \rho(s,a')) \qquad (3)\]</span></p>
<p>then we have <span class="math inline">\(\hat{H}(\rho) = H(\pi_\rho)\)</span> and <span class="math inline">\(H(\pi) = \hat{H}(\rho_\pi)\)</span>. So we can represent the entropy of a policy <span class="math inline">\(\pi\)</span> with the occupancy measure <span class="math inline">\(\rho_\pi\)</span>.</p>
<p>‚ñ∂ <strong>Lemma 2:</strong> If we define,</p>
<p><span class="math display">\[L(\pi, c) = -H(\pi) + \mathbb{E}_\pi[c(s,a)]\]</span></p>
<p><span class="math display">\[\hat{L}(\rho, c) = -\hat{H}(\rho) + \sum_{s,a} \rho(s,a)c(s,a)\]</span></p>
<p>then we have <span class="math inline">\(L(\pi, c) = \hat{L}(\rho_\pi, c)\)</span> and <span class="math inline">\(\hat{L}(\rho, c) = L(\pi_\rho, c)\)</span>. The Lemma allows us to transform the problem from optimizing <span class="math inline">\(\pi\)</span> to <span class="math inline">\(\rho\)</span>.</p>
</section>
<section id="convex-conjugate" class="slide level2">
<h2>Convex Conjugate</h2>
<p>‚ñ∂ Given a function <span class="math inline">\(f\)</span>, it can be represented by the supremum of all affine functions that are majorized by <span class="math inline">\(f\)</span>.</p>
<p>‚ñ∂ For any given slope <span class="math inline">\(m\)</span>, there may be many different constants <span class="math inline">\(b\)</span> such that the affine function <span class="math inline">\(\langle m, x \rangle - b\)</span> is majorized by <span class="math inline">\(f\)</span>. We only need the best such constant.</p>
<p>‚ñ∂ That‚Äôs what the convex conjugate <span class="math inline">\(f^*\)</span> does. Given a slope <span class="math inline">\(m\)</span>, <span class="math inline">\(f^*\)</span> returns the best constant <span class="math inline">\(b\)</span> such that <span class="math inline">\(\langle m, x \rangle - b\)</span> is majorized by <span class="math inline">\(f\)</span>. Thus,</p>
<p><span class="math display">\[f^*(m) = \sup_x \langle m, x \rangle - f(x)\]</span></p>
<p>‚ñ∂ Note that <span class="math inline">\(f^{**} = f\)</span>.</p>
<div class="small-math">
<p>There is a nice visualization of convex conjugate at <a href="https://remilepriol.github.io/dualityviz/" class="uri">https://remilepriol.github.io/dualityviz/</a></p>
</div>
</section>
<section id="derivations-2" class="slide level2">
<h2>Derivations</h2>
<p>‚ñ∂ By Lemma 2, if <span class="math inline">\(\psi\)</span> is a constant regularizer and <span class="math inline">\(\tilde{c} \in IRL_\psi(\pi_E)\)</span> and <span class="math inline">\(\hat{\pi} \in RL(\hat{c})\)</span>, then <span class="math inline">\(\rho_{\hat{\pi}} = \rho_{\pi_E}\)</span>.</p>
<p>‚ñ∂ Furthermore, we can also get the main result of the paper</p>
<p><span class="math display">\[RL \circ IRL_\psi(\pi_E) = \arg \min_{\pi \in \Pi} -H(\pi) + \psi^*(\rho_\pi - \rho_{\pi_E}) \qquad (4)\]</span></p>
<p>where <span class="math inline">\(\psi^*\)</span> is the convex conjugate of <span class="math inline">\(\psi\)</span>, which is defined as</p>
<p><span class="math display">\[\psi^*(m) = \sup_{x \in \mathbb{R}^{S \times A}} m^T x - \psi(x)\]</span></p>
<p>‚ñ∂ It tells us that the <span class="math inline">\(\psi\)</span>-regularized inverse RL seeks a policy whose occupancy measure is close to the expert‚Äôs as measured by the convex function <span class="math inline">\(\psi^*\)</span>.</p>
<p>‚ñ∂ A good imitation learning algorithm boils down to a good choice of the regularizer <span class="math inline">\(\psi\)</span>.</p>
</section>
<section id="occupancy-measure-matching" class="slide level2">
<h2>Occupancy Measure Matching</h2>
<p>‚ñ∂ As we showed previously, if <span class="math inline">\(\psi\)</span> is a constant, then the resulting policy would have the same occupancy measures with expert at all states and actions.</p>
<p>‚ñ∂ It is not practically useful because most of the occupancy measure of the expert values are exactly zero, due to the limited expert samples.</p>
<p>‚ñ∂ Thus, exact occupancy measure matching will force the learned policy to never visit the unseen state-action pairs.</p>
<p>‚ñ∂ If we restrict the class of cost function <span class="math inline">\(\mathcal{C}\)</span> to be convex and set the regularizer <span class="math inline">\(\psi\)</span> to be the indicator function of the set <span class="math inline">\(\mathcal{C}\)</span>. Then optimization problem in (6) can be written as</p>
<p><span class="math display">\[\min_\pi -H(\pi) + \max_{c \in \mathcal{C}} \mathbb{E}_\pi[c(s,a)] - \mathbb{E}_{\pi_E}[c(s,a)] \qquad (5)\]</span></p>
<p>which is a entropy-regularized apprenticeship learning problem.</p>
</section>
<section id="apprenticeship-learning" class="slide level2">
<h2>Apprenticeship Learning</h2>
<p>‚ñ∂ Policy gradient method can be used to update the parameterized policy <span class="math inline">\(\pi_\theta\)</span> to optimize the apprenticeship objective, Eq. 7.</p>
<p><span class="math display">\[\nabla_\theta \max_{c \in \mathcal{C}} \mathbb{E}_{\pi_\theta}[c(s,a)] - \mathbb{E}_{\pi_E}[c(s,a)] = \nabla_\theta \mathbb{E}_{\pi_\theta}[c^*(s,a)]\]</span></p>
<p><span class="math display">\[= \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q_{c^*}(s,a)]\]</span></p>
<p>where</p>
<p><span class="math display">\[c^* = \arg \max_{c \in \mathcal{C}} \mathbb{E}_{\pi_\theta}[c(s,a)] - \mathbb{E}_{\pi_E}[c(s,a)] \qquad (6)\]</span></p>
<p><span class="math display">\[Q_{c^*}(\bar{s}, \bar{a}) = \mathbb{E}_{\pi_\theta}[c^*(\bar{s}, \bar{a})|s_0 = \bar{s}, a_0 = \bar{a}] \qquad (7)\]</span></p>
<p>‚ñ∂ Fit <span class="math inline">\(c_i^*\)</span> as defined above. Analytical solution is feasible if <span class="math inline">\(\mathcal{C}\)</span> is restricted to Convex or Linear cost classes.</p>
<p>‚ñ∂ Given the <span class="math inline">\(c_i^*\)</span>, compute the policy gradient and take a TRPO step to produce <span class="math inline">\(\pi_{\theta_{i+1}}\)</span>.</p>
</section>
<section id="gail" class="slide level2">
<h2>GAIL</h2>
<p>‚ñ∂ Apprenticeship learning via TRPO is tractable in large environments but is incapable of exactly matching occupancy measures without careful tuning due to the restrictive cost classes <span class="math inline">\(\mathcal{C}\)</span>.</p>
<p>‚ñ∂ Constant regularizer <span class="math inline">\(\psi\)</span> leads to exact matching but is intractable in large environments. Thus, GAIL is proposed to combine the best of both methods.</p>
<p><span class="math display">\[
\psi_{GA}(c) \triangleq \begin{cases}
\mathbb{E}_{\pi_E}[g(c(s,a))] &amp; \text{if } c &lt; 0 \\
+\infty &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
g(x) = \begin{cases}
-x - \log(1 - e^x) &amp; \text{if } x &lt; 0 \\
+\infty &amp; \text{otherwise}
\end{cases}
\]</span></p>
</section>
<section id="gail-1" class="slide level2">
<h2>GAIL</h2>
<p>‚ñ∂ The GAIL regularizer <span class="math inline">\(\psi_{GA}\)</span> places low penalty on cost functions <span class="math inline">\(c\)</span> that assign an amount of negative cost to expert state-action pairs; It havily penalizes <span class="math inline">\(c\)</span> if it assigns large cost to the expert.</p>
<p>‚ñ∂ <span class="math inline">\(\psi_{GA}\)</span> is an average over expert data so it can adjust to arbitrary expert datasets.</p>
<p>‚ñ∂ In comparison, if <span class="math inline">\(\psi\)</span> is an indicator function (Apprenticeship Learning), then it‚Äôs always fixed.</p>
<p>‚ñ∂ Another property of <span class="math inline">\(\psi_{GA}\)</span> is its convex conjugate <span class="math inline">\(\psi_{GA}^*(\rho_\pi - \rho_{\pi_E})\)</span> can be derived in the following form:</p>
<p><span class="math display">\[\max_{D \in (0,1)^{S \times A}} \mathbb{E}_\pi[\log(D(s,a))] + \mathbb{E}_{\pi_E}[\log(1 - D(s,a))] \qquad (8)\]</span></p>
<p>‚ñ∂ It can be interpreted to find a discriminator that distinguishes trajectory between learned policy and expert policy. t</p>
</section>
<section id="gail-2" class="slide level2">
<h2>GAIL</h2>
<p>‚ñ∂ Combining with the main result Eq. (6) in the paper,</p>
<p><span class="math display">\[RL \circ IRL_\psi(\pi_E) = \arg \min_{\pi \in \Pi} -H(\pi) + \psi^*(\rho_\pi - \rho_{\pi_E})\]</span></p>
<p>The imitation learning problem is equivalent to find a saddle point <span class="math inline">\((\pi, D)\)</span> of the expression</p>
<p><span class="math display">\[\mathbb{E}_\pi[\log(D(s,a))] + \mathbb{E}_{\pi_E}[\log(1 - D(s,a))] - \lambda H(\pi) \qquad (9)\]</span></p>
<p>‚ñ∂ In terms of implementation, we just need to fit a parameterized policy <span class="math inline">\(\pi_\theta\)</span> with weights <span class="math inline">\(\theta\)</span> and a discriminator network <span class="math inline">\(D_w : S \times A \to (0,1)\)</span> with weights <span class="math inline">\(w\)</span>.</p>
<p>‚ñ∂ Update <span class="math inline">\(D_w\)</span> with Adam and update <span class="math inline">\(\pi_\theta\)</span> with TRPO iteratively.</p>
</section>
<section id="algorithm" class="slide level2">
<h2>Algorithm</h2>

<img data-src="images/algo.jpg" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="results" class="slide level2">
<h2>Results</h2>

<img data-src="images/results.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="todays-agenda-1" class="slide level2">
<h2>Today‚Äôs agenda</h2>
<div class="grey-text">
<p>‚Ä¢ Adversarial optimization for imitation learning (GAIL)</p>
</div>
<p>‚Ä¢ Adversarial optimization with multiple inferred behaviors (InfoGAIL)</p>
<p>‚Ä¢ Model based adversarial optimization (MGAIL)</p>
<p>‚Ä¢ Multi-agent imitation learning</p>

<aside><div>
<p>Acknowledgments<br>
Today‚Äôs slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong</p>
</div></aside></section>
<section id="infogail-interpretable-imitation-learning-from-visual-demonstrations" class="slide level2 center">
<h2>InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</h2>
<p>Presenter: Yin-Hung Chen</p>
</section>
<section>
<section id="motivation" class="title-slide slide level1 center">
<h1>Motivation</h1>

</section>
<section id="gail-3" class="slide level2">
<h2>GAIL</h2>
<p>A generator producing a policy ùúã competes with a discriminator distinguishing ùúã and the expert.</p>

<img data-src="images/gail-diagram.png" class="r-stretch"></section>
<section id="drawbacks-of-gail" class="slide level2">
<h2>Drawbacks of GAIL</h2>
<ul>
<li><p>Expert demonstrations can show significant <strong>variability</strong>.</p></li>
<li><p>The observations might have been sampled from <strong>different experts with different skills and habits</strong>.</p></li>
<li><p><strong>External latent factors</strong> of variation are not explicitly captured by GAIL, but they can significantly affect the observed behaviors.</p></li>
</ul>
</section></section>
<section>
<section id="infogail" class="title-slide slide level1 center">
<h1>InfoGAIL</h1>

</section>
<section id="modified-gail" class="slide level2">
<h2>Modified GAIL</h2>

<img data-src="images/modified-gail.png" class="r-stretch"></section>
<section id="objective-function" class="slide level2">
<h2>Objective Function</h2>
<p>GAIL: <span class="math display">\[
\underset{\pi}{\min} \underset{ùê∑\in(0,1)^{S√óA}}{\max} + ùîº_{ùúã_ùê∏}  [ ùëôùëúùëî(1 ‚àí ùê∑(ùë†, a))] ‚àí ùúÜùêª(ùúã)
\]</span></p>
<p>where ùúã is learner policy, and <span class="math inline">\(ùúã_ùê∏\)</span> is expert policy.</p>
<p>InfoGAIL:</p>
<ul>
<li>Discriminator: same with GAIL</li>
<li>Generator: simply introducing latent factor c into ùúã ‚Üí ùúã ùëé ùë†, ùëê<br>
<span class="orange">However, applying GAIL to ùúã(ùëé | ùë†, ùëê) could simply ignore c and fail to separate different expert behaviors ‚Üí <strong>adding more constraints over c</strong></span></li>
</ul>
</section>
<section id="constraints-over-latent-features" class="slide level2">
<h2>Constraints over Latent Features</h2>
<p>There should be high mutual information between the latent factor c and learner trajectory œÑ.</p>
<p><span class="math display">\[
I(c; \tau) = \sum_{\tau} p(\tau) \sum_c p(c|\tau) \log_2 \frac{p(c|\tau)}{p(c)}
\]</span></p>
<p>Independence between c and trajectory œÑ:</p>
<p><span class="math display">\[
p(c|\tau) = \frac{p(c)p(\tau)}{p(\tau)}, \quad \frac{p(c|\tau)}{p(c)} = 1, \quad \log_2 \frac{p(c|\tau)}{p(c)} = 0
\]</span></p>
<p>Maximizing mutual information <span class="math inline">\(I(c; \tau)\)</span><br>
‚Üí hard to maximize directly as it requires the posterior <span class="math inline">\(P(c|\tau)\)</span><br>
‚Üí using <span class="math inline">\(Q(c|\tau)\)</span> to estimate <span class="math inline">\(P(c|\tau)\)</span> There should be high mutual information between the latent factor c and learner trajectory ùúè.</p>
</section>
<section id="constraints-over-latent-features-1" class="slide level2">
<h2>Constraints over Latent Features</h2>
<p>Introducing the lower bound <span class="math inline">\(L_I(\pi, Q)\)</span> of <span class="math inline">\(I(c; \tau)\)</span></p>
<p><span class="math display">\[
\begin{align}
&amp; I(c; \tau) \\
&amp;= H(c) - H(c|\tau) \\
&amp;= \mathbb{E}_{a \sim \pi(\cdot|s,c)} \left[ \mathbb{E}_{c' \sim P(c|\tau)} [log P(c'|\tau)] \right] + H(c) \\
&amp;= \mathbb{E}_{a \sim \pi(\cdot|s,c)} \left[ D_{KL}(P(\cdot|\tau) \| Q(\cdot|\tau)) + \mathbb{E}_{c' \sim P(c|\tau)} [log Q(c'|\tau)] \right] + H(c) \\
&amp;\geq \mathbb{E}_{a \sim \pi(\cdot|s,c)} \left[ \mathbb{E}_{c' \sim P(c|\tau)} [log Q(c'|\tau)] \right] + H(c) \\
&amp;= \mathbb{E}_{c \sim P(c), a \sim \pi(\cdot|s,c)} [log Q(c|\tau)] + H(c) \\
&amp;= L_I(\pi, Q)
\end{align}
\]</span></p>
</section>
<section id="constraints-over-latent-features-2" class="slide level2">
<h2>Constraints over Latent Features</h2>
<p>There should be high mutual information between the latent factor c and learner trajectory ùúè.</p>
<p><br></p>
<p>Maximizing mutual information ùêº(ùëê; ùúè)<br>
‚Üí hard to maximize directly as it requires the posterior ùëÉ(ùëê|ùúè)<br>
‚Üí using ùëÑ(ùëê|ùúè) to estimate ùëÉ(ùëê|ùúè)</p>
<p><br></p>
<p>Maximizing ùêº(ùëê; ùúè) through maximize the lower bound ùêøùêº(ùúã, ùëÑ)</p>
</section>
<section id="objective-function-1" class="slide level2">
<h2>Objective Function</h2>
<p>GAIL:</p>
<p><span class="math display">\[
\min_\pi \max_{D \in (0,1)^{S \times A}} \mathbb{E}_\pi [log D(s, a)] + \mathbb{E}_{\pi_E} [log(1 - D(s, a))] - \lambda H(\pi)
\]</span></p>
<p>where <span class="math inline">\(\pi\)</span> is learner policy, and <span class="math inline">\(\pi_E\)</span> is expert policy.</p>
<p><br></p>
<p>InfoGAIL:</p>
<p><span class="math display">\[
\min_{\pi,Q} \max_D \mathbb{E}_\pi [log D(s, a)] + \mathbb{E}_{\pi_E} [log(1 - D(s, a))] - \lambda_1 L_I(\pi, Q) - \lambda_2 H(\pi)
\]</span></p>
<p><span class="math inline">\(\qquad \qquad \qquad \qquad \qquad \qquad\)</span> where <span class="math inline">\(\lambda_1 &gt; 0\)</span> and <span class="math inline">\(\lambda_2 &gt; 0\)</span>.</p>
</section>
<section id="infogail-1" class="slide level2">
<h2>InfoGAIL</h2>

<img data-src="images/infogail.png" class="r-stretch"></section>
<section id="section" class="slide level2">
<h2></h2>

<img data-src="images/algo-infogail.png" class="r-stretch"></section></section>
<section>
<section id="additional-optimization" class="title-slide slide level1 center">
<h1>Additional Optimization</h1>

</section>
<section id="improved-optimization" class="slide level2">
<h2>Improved Optimization</h2>
<p>The traditional GAN objective suffers from vanishing gradient and mode collapse problems.<br>
Vanishing gradient</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><span class="math display">\[
\frac{\partial C}{\partial b_1} = \frac{\partial C}{\partial y_3} \frac{\partial y_3}{\partial z_3} \frac{\partial z_3}{\partial x_3} \frac{\partial x_3}{\partial z_2} \frac{\partial z_2}{\partial x_2} \frac{\partial x_2}{\partial z_1} \frac{\partial z_1}{\partial b_1}
\]</span></p>
<p><span class="math display">\[
= \frac{\partial C}{\partial y_{3}} \sigma'(z_{3}) w_{3} \sigma'(z_{2}) w_{2} \sigma'(z_{1})
\]</span></p>
<p><img data-src="images/improved-opt.png"></p>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-right">
<figure>
<p><img data-src="images/sigmold-func.png" class="quarto-figure quarto-figure-right"></p>
</figure>
</div>
</div></div>
</section>
<section id="improved-optimization-1" class="slide level2">
<h2>Improved Optimization</h2>
<p>The traditional GAN objective suffers from vanishing gradient and mode collapse problems.</p>
<p><br></p>
<p>Mode collapse: generator tends to produce the same type of data ‚Üí generator yields the same G(z) for different z</p>
</section>
<section id="improved-optimization-2" class="slide level2">
<h2>Improved Optimization</h2>
<p>The traditional GAN objective suffers from vanishing gradient and mode collapse problems.</p>
<p>‚Üí using the Wasserstein GAN (WGAN)</p>
<p><span class="math display">\[
\min_{\theta, \psi} \max_{\omega} \mathbb{E}_{\pi_\theta}[D_\omega(s, a)] - \mathbb{E}_{\pi_E}[D_\omega(s, a)] - \lambda_0 \eta(\pi_\theta) - \lambda_1 L_I(\pi_\theta, Q_\psi) - \lambda_2 H(\pi_\theta)
\]</span></p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>

<img data-src="images/infogail-algo2.png" class="r-stretch"></section></section>
<section>
<section id="experiments" class="title-slide slide level1 center">
<h1>Experiments</h1>

</section>
<section id="learning-to-distinguish-trajectories" class="slide level2">
<h2>Learning to Distinguish Trajectories</h2>
<ul>
<li>The observations at time t are positions from t ‚àí 4 to t.</li>
<li>The latent code is a one-hot encoded vector with 3 dimensions and a uniform prior.</li>
</ul>

<img data-src="images/trajectory-observation.png" class="r-stretch"></section>
<section id="self-driving-car-in-the-torcs-environment" class="slide level2">
<h2>Self-driving car in the TORCS Environment</h2>
<p>‚Ä¢ The demonstrations collected by manually driving</p>
<p>‚Ä¢ Three-dimensional continuous action composed of steering, acceleration,and <em>braking</em></p>
<p>‚Ä¢ Raw visual inputs as the only external inputs for the state</p>
<p>‚Ä¢ Auxiliary information as internal input, including velocity at time t, actions at time t ‚àí 1 and t ‚àí 2, and damage of the car</p>
<p>‚Ä¢ Pre-trained ResNet on ImageNet</p>

<img data-src="images/self-drivingcar.png" class="quarto-figure quarto-figure-right r-stretch"></section>
<section id="section-2" class="slide level2">
<h2></h2>

<img data-src="images/torcs-env.png" class="r-stretch"></section>
<section id="performance" class="slide level2">
<h2>Performance</h2>
<p><strong>Turn</strong></p>
<p>[0, 1] corresponds to using the inside lane (blue lines), while [1, 0] corresponds to the outside lane (red lines).</p>

<img data-src="images/perf1.png" class="r-stretch"></section>
<section id="performance-1" class="slide level2">
<h2>Performance</h2>
<p><strong>Pass</strong></p>
<p>[0, 1] corresponds to passing from right (red lines), while [1, 0] corresponds to passing from left (blue lines).</p>

<img data-src="images/perf2.png" class="r-stretch"><div class="vertical-align">
<p>infoGAIL</p>
<p>GAIL</p>
</div>
</section>
<section id="performance-2" class="slide level2">
<h2>Performance</h2>
<p>‚Ä¢ Classification accuracies of ùëÑ(ùëê|ùúè)</p>
<p>‚Ä¢ Reward augmentation encouraging the car to drive faster</p>

<img data-src="images/perf-table.png" class="r-stretch"></section>
<section id="section-3" class="slide level2">
<h2></h2>
<iframe data-external="1" src="https://www.youtube.com/embed/YtNPBAW6h5k?rel=0" width="100%" height="80%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="small-math"><a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" class="uri">https://www.youtube.com/watch?v=YtNPBAW6h5k</a></span></p>
</section>
<section id="todays-agenda-2" class="slide level2">
<h2>Today‚Äôs agenda</h2>
<div class="grey-text">
<p>‚Ä¢ Adversarial optimization for imitation learning (GAIL)</p>
<p>‚Ä¢ Adversarial optimization with multiple inferred behaviors (InfoGAIL)</p>
</div>
<p>‚Ä¢ Model based adversarial optimization (MGAIL)</p>
<p>‚Ä¢ Multi-agent imitation learning</p>

<aside><div>
<p>Acknowledgments<br>
Today‚Äôs slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong</p>
</div></aside></section>
<section id="model-based-adversarial-imitation-learning" class="slide level2 center">
<h2>Model-based Adversarial Imitation Learning</h2>
<p>Nir Baram, Oron Anschel, Shie Mannor</p>
<p>Presented by Yuwen Xiong, Mar 1 st</p>
</section>
<section id="recap-gail-algorithm" class="slide level2">
<h2>Recap: GAIL algorithm</h2>
<p><span class="math display">\[
\underset{\pi}{\text{argmin}} \quad \underset{D \in (0,1)}{\text{argmax}} \mathbb{E}_{\pi}[\log D(s, a)] + \mathbb{E}_{\pi_E}[\log(1 - D(s, a))] - \lambda H(\pi)
\]</span></p>
<ul>
<li>We use Adam to optimize the discriminator and use TRPO to optimize the policy</li>
<li>The optimization of the discriminator can be done by using backpropagation, but this is not the case for the optimization of the policy</li>
<li><span class="math inline">\(\pi\)</span> affects the data distribution but do not appear in the objective itself</li>
<li>We use these two equations to get gradient estimation for <span class="math inline">\(\pi_{\theta}\)</span></li>
</ul>
<p><span class="math display">\[
\nabla_{\theta} \mathbb{E}_{\pi}[\log D(s, a)] \cong \mathbb{\hat{E}}_{\tau_i}[\nabla_{\theta} \log \pi_{\theta}(a|s) Q(s, a)]
\]</span></p>
<p><span class="math display">\[Q(\hat{s}, \hat{a}) = \mathbb{\hat{E}}_{\tau_i}[\log D(s, a) \mid s_0 = \hat{s}, a_0 = \hat{a}]\]</span></p>
</section>
<section id="motivation-1" class="slide level2">
<h2>Motivation</h2>
<ul>
<li>A model-free approach like GAIL has its limitations
<ul>
<li>The generative model can no longer be trained by simply backpropagating the gradient from the loss function defined over the discriminator</li>
<li>Has to resort to high-variance gradient estimations</li>
</ul></li>
</ul>
<div class="fragment">
<p><br></p>
<ul>
<li>If we have a model-based version of adversarial imitation learning
<ul>
<li>The system can be easily trained end-to-end using regular backpropagation</li>
<li>The policy gradient can be derived directly from the gradient of the discriminator</li>
<li>Policies can be more robust and training requires fewer interactions with the environment</li>
</ul></li>
</ul>
</div>
</section>
<section id="algorithm---overview" class="slide level2">
<h2>Algorithm - overview</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p>The model-free approach treats the state s as fixed and only tries to optimize the behavior.</p>
<div class="fragment">
<p>Instead, we treat s as a function of the policy: <span class="math display">\[s' = f(s, a)\]</span></p>
<p>So that, by using the law of total derivative we can get:</p>
<p><span class="math display">\[
\begin{align}
\left.\nabla_{\theta} D(s_t, a_t)\right|_{s=s_t, a=a_t} &amp;= \left.\frac{\partial D}{\partial a} \frac{\partial a}{\partial \theta}\right|_{a=a_t} + \left.\frac{\partial D}{\partial s} \frac{\partial s}{\partial \theta}\right|_{s=s_t} \\
&amp;= \left.\frac{\partial D}{\partial a} \frac{\partial a}{\partial \theta}\right|_{a=a_t} + \frac{\partial D}{\partial s} \left(\left.\frac{\partial f}{\partial s} \frac{\partial s}{\partial \theta}\right|_{s=s_{t-1}} + \left.\frac{\partial f}{\partial a} \frac{\partial a}{\partial \theta}\right|_{a=a_{t-1}}\right)
\end{align}
\]</span></p>
</div>
</div><div class="column" style="width:30%;">
<p><img data-src="images/algo-diagram.png" height="300"></p>
</div></div>
</section>
<section id="algorithm---preparation" class="slide level2">
<h2>Algorithm - preparation</h2>
<p>First, we know that <span class="math inline">\(D(s, a) = p(y|s, a)\)</span>, where <span class="math inline">\(y = \{\pi_E, \pi\}\)</span></p>
<div class="fragment">
<p>By using Bayes rule and the law of total probability we can get:</p>
<p><span class="math display">\[D(s, a) = p(\pi|s, a) = \frac{p(s, a|\pi)p(\pi)}{p(s, a)}\]</span></p>
<p><span class="math display">\[\qquad \qquad \qquad = \frac{p(s, a|\pi)p(\pi)}{p(s, a|\pi)p(\pi) + p(s, a|\pi_E)p(\pi_E)}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\qquad = \frac{p(s, a|\pi)}{p(s, a|\pi) + p(s, a|\pi_E)}\]</span></p>
</div>
</section>
<section id="algorithm---preparation-1" class="slide level2">
<h2>Algorithm - preparation</h2>
<p>Re-writing it as following:</p>
<p><span class="math display">\[
D(s, a) = \frac{1}{\frac{p(s,a|\pi)+p(s,a|\pi_E)}{p(s,a|\pi)}} = \frac{1}{1 + \frac{p(s,a|\pi_E)}{p(s,a|\pi)}} = \frac{1}{1 + \frac{p(a|s,\pi_E)}{p(a|s,\pi)} \cdot \frac{p(s|\pi_E)}{p(s|\pi)}}\]</span></p>
<div class="fragment">
<p>Let <span class="math inline">\(\varphi(s, a) = \frac{p(a|s,\pi_E)}{p(a|s,\pi)}\)</span> and <span class="math inline">\(\psi(s) = \frac{p(s|\pi_E)}{p(s|\pi)}\)</span>, we can get:</p>
<p><span class="math display">\[
D(s, a) = \frac{1}{1 + \varphi(s, a) \cdot \psi(s)}
\]</span></p>
</div>
</section>
<section id="algorithm---preparation-2" class="slide level2">
<h2>Algorithm - preparation</h2>
<p>Here <span class="math inline">\(\varphi(s, a) = \frac{p(a|s,\pi_E)}{p(a|s,\pi)}\)</span> stands for policy likelihood ratio</p>
<p>And <span class="math inline">\(\psi(s) = \frac{p(s|\pi_E)}{p(s|\pi)}\)</span> stands for state distribution likelihood ratio</p>
<div class="fragment">
<p>By using differentiation rule we can easily get:</p>
<p><span class="math display">\[
\nabla_a D = -\frac{\varphi_a(s, a)\psi(s)}{(1 + \varphi(s, a)\psi(s))^2}
\]</span></p>
<p><span class="math display">\[
\nabla_s D = -\frac{\varphi_s(s, a)\psi(s) + \varphi(s, a)\psi_s(s)}{(1 + \varphi(s, a)\psi(s))^2}
\]</span></p>
</div>
<div class="fragment">
<p>Recall what we need: <span class="math inline">\(\left.\nabla_\theta D(s_t, a_t)\right|_{s=s_t,a=a_t} = \left.\frac{\partial D}{\partial a} \frac{\partial a}{\partial \theta}\right|_{a=a_t} + \left.\frac{\partial D}{\partial s} \frac{\partial s}{\partial \theta}\right|_{s=s_t}\)</span></p>
</div>
</section>
<section id="algorithm---re-parameterization-of-distribution" class="slide level2">
<h2>Algorithm - re-parameterization of distribution</h2>
<p>Assuming the policy is given by</p>
<p><span class="math display">\[
\pi_\theta(a|s) = \mathcal{N}(a|\mu_\theta(s), \sigma_\theta^2(s))
\]</span></p>
<div class="fragment">
<p>We can rewrite it to</p>
<p><span class="math display">\[
\pi_\theta(a|s) = \mu_\theta(s) + \xi\sigma_\theta(s), \text{ where } \xi \sim \mathcal{N}(0, 1)
\]</span></p>
</div>
<div class="fragment">
<p>So that we can get a Monte-Carlo estimator of the derivative</p>
<p><span class="math display">\[
\begin{align}
\nabla_\theta \mathbb{E}_\pi(a|s) D(s, a) &amp;= \mathbb{E}_{\rho(\xi)} \nabla_a D(a, s) \nabla_\theta \pi_\theta(a|s) \\
&amp; \cong \frac{1}{M} \sum_{i=1}^M \left.\nabla_a D(s, a) \nabla_\theta \pi_\theta(a|s)\right|_{\xi=\xi_i}
\end{align}
\]</span></p>
</div>
</section>
<section id="algorithm-1" class="slide level2">
<h2>Algorithm</h2>

<img data-src="images/algo-model-diagram.png" class="r-stretch"></section>
<section id="algorithm-2" class="slide level2">
<h2>Algorithm</h2>
<p>To maximize the reward function, we can view reward as <span class="math inline">\(r(s, a) = -D(s, a)\)</span>, and then maximizing the total reward is equivalent to minimizing the total discriminator beliefs along a trajectory.</p>
<div class="fragment">
<p>So that we can define: <span class="math display">\[
J(\theta) = \mathbb{E} \left[ \sum_{t=0} \gamma^t D(s_t, a_t) \mid \theta \right]
\]</span></p>
<p>And write down the derivatives: (this follows SVG paper [Heess et al.&nbsp;2015]) <span class="math display">\[
J_s = \mathbb{E}_{p(a\mid s)}\mathbb{E}_{p(s'\mid s,a)}\mathbb{E}_{p(\xi\mid s,a,s')}
\left[ D_s + D_a\pi_s + \gamma J'_{s'}(f_s + f_a\pi_s)
\right]
\]</span></p>
<p><span class="math display">\[
J_\theta =  \mathbb{E}_{p(a\mid s)}\mathbb{E}_{p(s'\mid s,a)}\mathbb{E}_{p(\xi\mid s,a,s')}
[D_a\pi_\theta +  \gamma(J'_{s'}f_a\pi_\theta + J'_\theta)]
\]</span></p>
</div>
</section>
<section id="algorithm-3" class="slide level2">
<h2>Algorithm</h2>

<img data-src="images/algo1.png" class="r-stretch"></section>
<section id="experiments-1" class="slide level2">
<h2>Experiments</h2>

<img data-src="images/experiments.png" class="r-stretch"></section>
<section id="todays-agenda-3" class="slide level2">
<h2>Today‚Äôs agenda</h2>
<div class="grey-text">
<p>‚Ä¢ Adversarial optimization for imitation learning (GAIL)</p>
<p>‚Ä¢ Adversarial optimization with multiple inferred behaviors (InfoGAIL)</p>
<p>‚Ä¢ Model based adversarial optimization (MGAIL)</p>
</div>
<p>‚Ä¢ Multi-agent imitation learning</p>



<aside><div>
<p>Acknowledgments<br>
Today‚Äôs slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong</p>
</div></aside></section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://csc2626.github.io/website/" target="_blank" style="font-size:0.8em; bottom: -5px;">‚Ü©Ô∏é Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"src":"chalkboard.json","boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/csc2626\.github\.io\/website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>