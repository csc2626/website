<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Florian Shkurti">
  <title>CSC2626 - Fall 2024 – CSC2626 Imitation Learning for Robotics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-f563837468303362081e247dddd440d0.css">
  <link rel="stylesheet" href="../style.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="CSC2626 Imitation Learning for Robotics – CSC2626 - Fall 2024">
<meta property="og:description" content="Week 4: Imitation Learners Guided by Optimal Control Experts and Physics">
<meta property="og:site_name" content="CSC2626 - Fall 2024">
<meta name="twitter:title" content="CSC2626 Imitation Learning for Robotics – CSC2626 - Fall 2024">
<meta name="twitter:description" content="Week 4: Imitation Learners Guided by Optimal Control Experts and Physics">
<meta name="twitter:card" content="summary">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">CSC2626 Imitation Learning for Robotics</h1>
  <p class="subtitle">Week 4: Imitation Learners Guided by Optimal Control Experts and Physics</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Florian Shkurti 
</div>
</div>
</div>

</section>
<section id="todays-agenda" class="slide level2">
<h2>Today’s agenda</h2>
<p>• Guided policy search (GPS)</p>
<p>• Policy learning from adaptive MPC with privileged information (PLATO)</p>
<p>• Combining behavioral cloning and RL</p>
<p>• Dynamic movement primitives (DMPs)</p>
<p>• Expert iteration &amp; learning to search</p>

<aside><div>
<p>Acknowledgments<br>
Today’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:<br>
<a href="https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory" class="uri">https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory</a></p>
</div></aside></section>
<section id="section" class="slide level2">
<h2></h2>

<img data-src="images/overview-1.png" class="r-stretch"></section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="images/overview-2.png"></p>
</div><div class="column small-math" style="width:30%;">
<p><br></p>
<p><span class="math inline">\(\boxed{\begin{aligned} p(x_{t+1} | x_t, u_t) &amp;= \mathcal{N}(f(x_t, u_t), \Sigma) \\
f(x_t, u_t) &amp;\approx A_t x_t + B_t u_t \\
A_t &amp;= \frac{df}{dx_t} \quad B_t = \frac{df}{du_t}
\end{aligned}}\)</span></p>
</div></div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="images/overview-3.png"></p>
</div><div class="column small-math" style="width:40%;">
<p><br></p>
<p><span class="math inline">\(\boxed{\begin{aligned} \text{iLQR/iLQG control} \\
\mathbf{u}_t = \mathbf{K}_t (\mathbf{x}_t - \hat{\mathbf{x}}_t) + \mathbf{k}_t + \hat{\mathbf{u}}_t \\
\text{is deterministic. We need it stochastic.}
\end{aligned}}\)</span></p>
<p><br><br></p>
<div class="fragment">
<p><span class="math inline">\(p(u_t \mid x_t) = \mathcal{N}(K_t (x_t - \hat{x}_t) + k_t + \hat{u}_t, \Sigma_t)\)</span></p>
<p><br><br><br><br></p>
<p><span class="math inline">\(p(\tau) = p(x_1) \prod_{t=1}^{T} p(u_t \mid x_t) p(x_{t+1} \mid x_t, u_t)\)</span></p>
</div>
</div></div>
</section>
<section id="learning-a-globally-valid-nn-policy-an-example" class="slide level2">
<h2>Learning a globally-valid NN policy: an example</h2>
<div style="color:red;">
<p>We want to solve a stochastic version of this problem</p>
</div>
<p><span class="math display">\[
\begin{align}
\min_{\mathbf{u}_1, \ldots, \mathbf{u}_T, \mathbf{x}_1, \ldots, \mathbf{x}_T, \theta} \sum_{t=1}^{T} c(\mathbf{x}_t, \mathbf{u}_t) \text{s.t.} \mathbb{x}_t = f(x_{t-1}, u_{t-1}) \\
\text{s.t.} \mathbf{u}_t = \pi_\theta (x_t)
\end{align}
\]</span></p>
<div style="color:red;">
<p>where both the policy and the dynamics are stochastic. I.e. we want to learn a globally-valid policy that imitates the actions of locally-valid iLQG policies.</p>
</div>
</section>
<section id="kl-divergence-constraints" class="slide level2">
<h2>KL-Divergence Constraints</h2>
<p><br></p>
<div class="columns">
<div class="column small-math" style="width:60%;">
<p>▶ Modified cost function</p>
<p><span class="math display">\[
\min_{p(\tau) \in \mathcal{N}(\tau)} E_p[l(\tau)] \text{ s.t. } D_{\text{KL}}(p(\tau)||\hat{p}(\tau)) \leq \epsilon
\]</span></p>
</div><div class="column" style="width:40%;">

</div></div>
<div class="small-math absolute" style="top: 100px; right: 0px; ">
<p><span class="math display">\[
\begin{align}
\color{red} \leftarrow
\boxed{
    \begin{align}
    \color{red}\text{Want to solve a stochastic version of} \\
    \color{black} \min_{u_1, \ldots, u_T, x_1, \ldots, x_T} \sum_{t=1}^{T} c(x_t, u_t) \quad \text{s.t.} \quad x_t = f(x_{t-1}, u_{t-1}) \\
    \color{red}\text{with an additional trust-region inequality constraint}
    \end{align}
    } \\
\end{align}
\]</span></p>
</div>
<div class="absolute red-annotation" style="top: 300px; right: 0px; ">
<p><span class="math inline">\(\color{black} \hat{p}(\tau)\)</span> is the previous trajectory<br>
distribution. Same dynamics,<br>
different policy.</p>
</div>
</section>
<section id="kl-divergence-constraints-1" class="slide level2">
<h2>KL-Divergence Constraints</h2>
<div class="small-math">
<p>▶ Modified cost function</p>
<p><span class="math display">\[
\min_{p(\tau) \in \mathcal{N}(\tau)} E_p[l(\tau)] \text{ s.t. } D_{\text{KL}}(p(\tau)||\hat{p}(\tau)) \leq \epsilon
\]</span></p>
<p>▶ Lagrangian of this problem (<span class="math inline">\(\eta\)</span> dual variable)</p>
<p><span class="math display">\[\mathcal{L}_{\text{traj}}(p(\tau), \eta) = E_p[l(\tau)] + \eta[D_{\text{KL}}(p(\tau)||\hat{p}(\tau)) - \epsilon]\]</span></p>
<p>▶ Assuming <span class="math inline">\(p(\mathbf{x}_{t+1}|\mathbf{x}_t, \mathbf{u}_t) = \hat{p}(\mathbf{x}_{t+1}|\mathbf{x}_t, \mathbf{u}_t) = \mathcal{N}(f_{\mathbf{x}t}\mathbf{x}_t + f_{\mathbf{u}t}\mathbf{u}_t, \mathbf{F}_t)\)</span></p>
<p><span class="math display">\[
\mathcal{L}_{\text{traj}}(p(\tau), \eta) = \left[\sum_t E_{p(\mathbf{x}_t, \mathbf{u}_t)}[l(\mathbf{x}_t, \mathbf{u}_t) - \eta \log p(\mathbf{u}_t|\mathbf{x}_t)]\right] - \eta H(p(\tau)) - \eta \epsilon
\]</span></p>
<div class="fragment">
<p>▶ Augmented cost function</p>
<p><span class="math display">\[\tilde{l}(\mathbf{x}_t, \mathbf{u}_t) = \frac{1}{\eta}l(\mathbf{x}_t, \mathbf{u}_t) - \log \hat{p}(\mathbf{u}_t|\mathbf{x}_t)\]</span></p>
</div>
<div class="fragment">
<p>▶ Solved by dual gradient descent</p>
</div>
</div>
</section>
<section id="dual-gradient-descent" class="slide level2">
<h2>Dual Gradient Descent</h2>
<div class="columns small-math">
<div class="column" style="width:30%;">
<p><span class="math inline">\(\min_x f(x) \quad \text{s.t.} \quad C(x) = 0\)</span></p>
</div><div class="column" style="width:30%;">
<p><span class="math inline">\(L(x, \lambda) = f(x) + \lambda C(x)\)</span></p>
</div><div class="column" style="width:30%;">
<p><span class="math inline">\(g(\lambda) = L(x^*(\lambda), \lambda)\)</span></p>
</div></div>
<div class="fragment">
<p><img data-src="images/dual-gradient-steps.png" height="200"></p>
<p><span class="tiny-font"><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf" class="uri">http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf</a></span></p>
</div>
</section>
<section id="general-parameterized-policies" class="slide level2">
<h2>General Parameterized Policies</h2>
<div class="small-math">
<p>▶ Objective</p>
<p><span class="math display">\[
\min_{\theta,\, p(\tau)} \mathbb{E}_{p(\tau)}[\ell(\tau)] \quad \text{s.t.} \quad D_{KL}(p(x_t)\pi_\theta(u_t|x_t)\,\|\,p(x_t, u_t)) = 0,\ \forall t
\]</span></p>
<p>▶ Lagrangian of the problem <span class="math display">\[
\begin{align}
\mathcal{L}_{GPS}(\theta, p, \lambda) &amp;= \mathbb{E}_{p(\tau)}[\ell(\tau)] + \sum_{t=1}^{T} \lambda_t D_{KL}(p(x_t)\pi_\theta(u_t|x_t)\,\|\,p(x_t, u_t)) \\
\end{align}
\]</span></p>
<div class="center-align fragment">
<p><span class="red">OR</span> <span class="math display">\[
\qquad \qquad \qquad \qquad \qquad \qquad = \mathbb{E}_{p(\tau)}[\ell(\tau) - \eta\log\hat{p}(\tau)] - \eta\mathcal{H}(p) + \sum_{t=1}^{T} \lambda_t D_{KL}(p(x_t)\pi_\theta(u_t|x_t)\,\|\,p(x_t, u_t))
\]</span></p>
<p><span class="red">if we include the trust-region constraint</span></p>
</div>
</div>
</section>
<section id="general-parameterized-policies-1" class="slide level2">
<h2>General Parameterized Policies</h2>

<img data-src="images/algo1.png" class="r-stretch"></section>
<section id="experiments-conducted" class="slide level2">
<h2>Experiments Conducted</h2>
<p>▶ 2D, 3D peg insertion (discontinuous dynamics)</p>
<p>▶ Octopus arm control (high-dimensional state and action space)</p>
<p>▶ Planar swimming (three-link snake)</p>
<p>▶ Walking (seven-link biped to maintain a target velocity)</p>
</section>
<section id="trajectory-optimization" class="slide level2">
<h2>Trajectory Optimization</h2>

<img data-src="images/trajectory.png" class="r-stretch"></section>
<section id="trajectory-optimization-1" class="slide level2">
<h2>Trajectory Optimization</h2>

<img data-src="images/trajectory-graph.png" class="r-stretch"></section>
<section id="neural-network-policy-learning-with-gps" class="slide level2">
<h2>Neural Network Policy Learning with GPS</h2>

<img data-src="images/neural-network-policy.png" class="r-stretch"></section>
<section id="training-with-privileged-information" class="slide level2">
<h2>Training with privileged information</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">\[
\boxed{
    \begin{align}
\min_{p,\, \theta} \mathbb{E}_{\tau \sim p(\tau)}[c(\tau)] \quad \\
\text{s.t.} \quad p(u_t \mid x_t) = \pi_{\theta}(u_t \mid x_t)
\end{align}
}
\]</span></p>
</div><div class="column" style="width:50%;">
<p><span class="math display">\[
\boxed{
    \begin{align}
\min_{p,\, \theta} \mathbb{E}_{\tau \sim p(\tau)}[c(\tau)] \quad \\
\text{s.t.} \quad p(u_t \mid x_t) = \pi_{\theta}(u_t \mid o_t)
\end{align}
}
\]</span></p>
</div></div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<iframe data-external="1" src="https://www.youtube.com/embed/Q4bMcUk6pcw" width="800" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="tiny-font"><a href="https://www.youtube.com/watch?v=Q4bMcUk6pcw&amp;t=40s&amp;ab_channel=RAIL" class="uri">https://www.youtube.com/watch?v=Q4bMcUk6pcw&amp;t=40s&amp;ab_channel=RAIL</a></span></p>
</section>
<section id="todays-agenda-1" class="slide level2">
<h2>Today’s agenda</h2>
<p><span class="grey-text">• Guided policy search (GPS)</span></p>
<p>• Policy learning from adaptive MPC with privileged information (PLATO)</p>
<p>• Combining behavioral cloning and RL</p>
<p>• Dynamic movement primitives (DMPs)</p>
<p>• Expert iteration &amp; learning to search</p>

<aside><div>
<p>Acknowledgments<br>
Today’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:<br>
<a href="https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory" class="uri">https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory</a></p>
</div></aside></section>
<section id="section-4" class="slide level2">
<h2></h2>
<p><iframe data-external="1" src="https://www.youtube.com/embed/clHp6QgVyAU" width="800" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>
<span class="tiny-font"><a href="https://www.youtube.com/watch?v=clHp6QgVyAU" class="uri">https://www.youtube.com/watch?v=clHp6QgVyAU</a></span></p>
</section>
<section id="problem-set-up" class="slide level2">
<h2>Problem Set-up</h2>
<div class="small-math">
<ul>
<li>States <span class="math inline">\(x\)</span>, actions <span class="math inline">\(u\)</span>.<br>
</li>
<li>The policy could only control the system from observations <span class="math inline">\(o\)</span>.<br>
</li>
<li>The policy <span class="math inline">\(\pi_\theta(u \mid o_t)\)</span>, parametrized by <span class="math inline">\(\theta\)</span>.<br>
</li>
<li>At test time, the agent chooses actions according to <span class="math inline">\(\pi_\theta(u \mid o_t)\)</span> at each time step <span class="math inline">\(t\)</span>, and experiences a loss <span class="math inline">\(c(x_t \mid o_t) \in [0, 1]\)</span>.<br>
</li>
<li>The next state is distributed by dynamics <span class="math inline">\(p(x_{t+1} \mid x_t, u_t)\)</span>.</li>
</ul>
<p>The objective is to learn policy <span class="math inline">\(\pi_\theta(u \mid o_t)\)</span> such that</p>
<p><span class="math display">\[
\arg\min_\pi J(\pi) = \mathbb{E}_\pi \left[ \sum_{t=1}^{T} c(x_t, u_t) \right].
\]</span></p>
<p>At the same time, let’s define expected cost from state <span class="math inline">\(x_t\)</span> at time <span class="math inline">\(t\)</span> as</p>
<p><span class="math display">\[
J(\pi \mid x_t) = \mathbb{E}_\pi \left[ \sum_{t=1}^{T} c(x_t, u_t) \mid x_t \right].
\]</span></p>
</div>
</section>
<section id="adaptive-mpc-teacher" class="slide level2">
<h2>Adaptive MPC Teacher</h2>
<p>One naive way is to train the policy with supervised learning from data generated from an MPC teacher. However, because the state distribution for the teacher and learner are different, the learned policy might fail.<br>
In order to overcome this challenge, an adaptive MPC teacher is used which generates actions from a controller obtained by:</p>
<p><span class="math display">\[
\pi^t_\lambda(u \mid x_t, \theta) \leftarrow \arg\min_\pi J_t(\pi \mid x_t) + \lambda D_{KL}(\pi(u \mid x_t) \,\|\, \pi_\theta(u \mid o_t)) \qquad \tag{1}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> determines the relative importance of matching the learner policy versus optimizing the expected return. Note that the particular MPC algorithm is based on iLQG.</p>
</section>
<section id="algorithm" class="slide level2">
<h2>Algorithm</h2>
<p><strong>Algorithm 1: PLATO algorithm</strong></p>
<p>Initialize data <span class="math inline">\(D \leftarrow \emptyset\)</span><br>
<strong>for</strong> <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(N\)</span> <strong>do</strong><br>
 <strong>for</strong> <span class="math inline">\(t = 1\)</span> to <span class="math inline">\(T\)</span> <strong>do</strong><br>
  <span class="math inline">\(\pi_\lambda^{t}(u_t \mid x_t, \theta) \leftarrow \arg\min_{\pi} J_t(\pi \mid x_t) + \lambda D_{KL}(\pi(u \mid x_t) \,\|\, \pi_{\theta}(u \mid o_t))\)</span><br>
  Sample <span class="math inline">\(u_t \sim \pi_\lambda^{t}(u \mid x_t, \theta)\)</span><br>
  <span class="math inline">\(\pi^{*}(u_t \mid x_t) \leftarrow \arg\min_{\pi} J(\pi)\)</span><br>
  Sample <span class="math inline">\(u_t^{*} \sim \pi^{*}(u \mid x_t)\)</span><br>
  Append <span class="math inline">\((o_t, u_t^{*})\)</span> to dataset <span class="math inline">\(D\)</span><br>
  State evolves <span class="math inline">\(x_{t+1} \sim p(x_{t+1} \mid x_t, u_t)\)</span><br>
 <strong>end for</strong><br>
Train <span class="math inline">\(\pi_\theta\)</span> on <span class="math inline">\(D\)</span><br>
<strong>end for</strong></p>
</section>
<section id="training-the-learners-policy" class="slide level2">
<h2>Training the learner’s policy</h2>
<p>During the supervised learning phase, we minimize the KL-divergence between the learner policy <span class="math inline">\(\pi_\theta\)</span> and precomputed near-optimal policies <span class="math inline">\(\pi^*\)</span>, which is estimated by iLQG:</p>
<p><span class="math display">\[
\theta \leftarrow \arg\min_\theta \sum_{(x_t, o_t) \in D} D_{KL}(\pi_\theta(u \mid o_t) \,\|\, \pi^*(u \mid x_t)).
\]</span></p>
<p>Since both <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi^*\)</span> are conditionally Gaussian, the KL divergence can be expressed in closed form if we ignore the terms not involving the learner policy means <span class="math inline">\(\mu_\theta(o_t)\)</span>:</p>
<p><span class="math display">\[
\min_\theta \sum_{(x_t, o_t) \in D} \left\| \mu^*(x_t) - \mu_\theta(o_t) \right\|^2_{\Sigma_{\pi^*}^{-1/2}}.
\]</span></p>
<p>In this paper, <span class="math inline">\(\mu_\theta\)</span> is represented by a NN, and solved by SGD.</p>
</section>
<section id="theoretical-analysis" class="slide level2">
<h2>Theoretical Analysis</h2>
<p>Let <span class="math inline">\(Q_t(x, \pi, \tilde{\pi})\)</span> denote the cost of executing <span class="math inline">\(\pi\)</span> for one time step starting from an initial state, and then executing <span class="math inline">\(\tilde{\pi}\)</span> for the remaining <span class="math inline">\(t - 1\)</span> time steps. We assume the cost-to-go difference between the learned policy and the optimal policy is bounded: <span class="math inline">\(Q_t(x, \pi, \pi^*) - Q_t(x, \pi^*, \pi^*) \leq \delta\)</span></p>
<div class="callout callout-none no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p><strong>Theorem</strong></p>
<p>Let the cost-to-go <span class="math inline">\(Q_t(x,\pi,\pi^*) - Q_t(x,\pi^*,\pi^*) \leq \delta\)</span> for all <span class="math inline">\(t \in \{1,\ldots,T\}\)</span>.<br>
Then for PLATO, <span class="math inline">\(J(\pi_\theta) \leq J(\pi^*) + \delta \sqrt{\epsilon_\theta *} \cdot O(T) + O(1)\)</span></p>
</div>
</div>
</div>
<p>Therefore, the policy learned by PLATO converges to a policy with bounded cost.</p>
</section>
<section id="section-5" class="slide level2">
<h2></h2>

<img data-src="images/canvon.png" class="r-stretch"></section>
<section id="comparison-to-dagger" class="slide level2">
<h2>Comparison to DAgger</h2>
<p>PLATO could be viewed as a generalization of DAgger, which samples from mixture policy</p>
<p><span class="math display">\[
\pi_{\text{mix},i} = \beta_i \pi^* + (1 - \beta_i) \pi_{\theta_i}
\]</span></p>
<p><strong>Differences with the DAgger:</strong></p>
<ol type="1">
<li>The training data is labeled with actions from <span class="math inline">\(\pi^*\)</span>.<br>
</li>
<li>PLATO uses adaptive MPC policy to select actions at each time step, rather than the mixture policy <span class="math inline">\(\pi_{\text{mix},i}\)</span> used.</li>
</ol>
</section>
<section id="todays-agenda-2" class="slide level2">
<h2>Today’s agenda</h2>
<div class="grey-text">
<p>• Guided policy search (GPS)</p>
<p>• Policy learning from adaptive MPC with privileged information (PLATO)</p>
</div>
<p>• Combining behavioral cloning and RL</p>
<p>• Dynamic movement primitives (DMPs)</p>
<p>• Expert iteration &amp; learning to search</p>

<aside><div>
<p>Acknowledgments<br>
Today’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:<br>
<a href="https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory" class="uri">https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory</a></p>
</div></aside></section>
<section id="section-6" class="slide level2">
<h2></h2>
<iframe data-external="1" src="https://www.youtube.com/embed/vppFvq2quQ0" width="800" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="tiny-font"><a href="https://www.youtube.com/watch?v=vppFvq2quQ0" class="uri">https://www.youtube.com/watch?v=vppFvq2quQ0</a></span></p>
</section>
<section id="learning-complex-dexterous-manipulation-with-deep-reinforcement-learning-and-demonstrations" class="slide level2">
<h2>Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations</h2>
<div class="center-align">
<p>Aravind Rajeswaran , Vikash Kumar , Abhishek Gupta, Giulia Vezzani , John Schulman , Emanuel Todorov , Sergey Levine</p>
<p><br></p>
<p>Jason Rebello<br>
UTIAS</p>
</div>
</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p>• Dexterous multi-fingered hands are <strong><em>extremely versatile</em></strong></p>
<p>• <strong><em>Control</em></strong> is challenging due to high dimensionality, complex contact patterns</p>
<p>• Previous methods require <strong><em>reward shaping</em></strong></p>
<p>• DRL limited to <strong><em>simpler manipulators and simple tasks</em></strong></p>
<p>• Lack of physical systems due to <strong><em>sample inefficiency</em></strong></p>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/object-relocation.png"></p>
<figcaption>Object Relocation task</figcaption>
</figure>
</div>
</div></div>
</section>
<section id="contributions" class="slide level2">
<h2>Contributions</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p>• Manipulation with <strong><em>24-DOF</em></strong> hand</p>
<p>• <strong><em>Model Free</em></strong> DRL</p>
<p>• Used in <strong><em>complex tasks with variety of tools</em></strong></p>
<p>• Small number of <strong><em>human demonstrations</em></strong> reduces sample complexity</p>
<p>• <strong><em>Reduces learning time</em></strong></p>
<p>• <strong><em>Robust</em></strong> and natural movements</p>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/grab-object.png"></p>
<figcaption>Tool use task</figcaption>
</figure>
</div>
</div></div>
</section>
<section id="manipulation-task-1" class="slide level2">
<h2>Manipulation Task 1</h2>
<p><u><strong><em>Object Relocation</em></strong></u></p>

<img data-src="images/object-relocation-4.png" class="r-stretch"><p>• Move Blue ball to green position</p>
<p>• Task complete when ball is epsilon ball away from target</p>
<p>• Positions of ball and target are randomized</p>
<p>• Main challenge is exploration (reach object, grab and move to target location)</p>
</section>
<section id="manipulation-task-2" class="slide level2">
<h2>Manipulation Task 2</h2>
<p><u><strong><em>In-hand Manipulation</em></strong></u></p>

<img data-src="images/inhand-manipulation.png" class="r-stretch"><p>• Reposition blue pen to match orientation of green target</p>
<p>• Task complete when orientation is achieved</p>
<p>• Base of hand is fixed</p>
<p>• Large number of contacts with complex solutions</p>
<p>• Used a well shaped reward for training an expert</p>
</section>
<section id="manipulation-task-3" class="slide level2">
<h2>Manipulation Task 3</h2>
<p><u><strong><em>Door Opening</em></strong></u></p>

<img data-src="images/door-open.png" class="r-stretch"><p>• Undo latch and swing door open</p>
<p>• Task complete when door touches door stopper</p>
<p>• No information of latch explicitly provided</p>
<p>• A lot of hidden sub-tasks</p>
<p>• Position of door is randomized</p>
</section>
<section id="manipulation-task-4" class="slide level2">
<h2>Manipulation Task 4</h2>
<p><u><strong><em>Tool Use</em></strong></u></p>

<img data-src="images/tool-use.png" class="r-stretch"><p>• Pickup and hammer nail</p>
<p>• Task complete when entire nail is inside the board</p>
<p>• Use tool instead of just relocation</p>
<p>• Multiple steps in task</p>
</section>
<section id="experimental-setup" class="slide level2">
<h2>Experimental Setup</h2>
<div class="columns">
<div class="column small-math" style="width:55%;">
<div class="quarto-layout-panel" data-layout="[1,1]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><u><strong><em>ADROIT hand</em></strong></u> <img data-src="images/adroit-hand.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><u><strong><em>HTC headset</em></strong></u> <img data-src="images/ntc-headset.png"></p>
</div>
</div>
</div>
<ul>
<li>24-DOF hand</li>
<li>First, middle, ring – 4 DOF each</li>
<li>Little finger, thumb – 5 DOF each</li>
<li>Wrist – 2 DOF</li>
<li>Actuated with position control and has joint angle sensor</li>
<li>MuJoCo physics simulation with friction</li>
<li>25 demonstrations for each task</li>
</ul>
</div><div class="column small-math" style="width:45%;">
<p><u><strong><em>HAPTIX Simulator</em></strong></u></p>
<video id="video_shortcode_videojs_video1" width="360" height="250" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://roboti.us/book/file/bakeoff.mp4"></video>
<p><img data-src="images/cyberglove.png"><br>
<u><strong><em>CyberGlove 3</em></strong></u></p>
</div></div>
</section>
<section id="methodology-preliminaries" class="slide level2">
<h2>Methodology (Preliminaries)</h2>
<p><br></p>
<div class="orange tiny-font">
<p><span class="absolute" style="top: 120px; left: 250px; ">States <span class="math inline">\(\downarrow\)</span></span></p>
<p><span class="absolute" style="top: 100px; left: 360px; ">Rewards<br>
<span class="math inline">\(\downarrow\)</span></span></p>
<p><span class="absolute" style="top: 100px; left: 450px; ">Initial Probability distribution<br>
<span class="math inline">\(\downarrow\)</span></span></p>
</div>
<p><strong>MDP definition:</strong> <span class="math inline">\(\mathcal{M} = \{S, A, R, \mathcal{T}, \rho_0, \gamma\}\)</span></p>
<div class="orange tiny-font">
<p><span class="absolute" style="top: 170px; left: 280px; ">Actions <span class="math inline">\(\uparrow\)</span></span></p>
<p><span class="absolute" style="top: 170px; left: 400px; "><span class="math inline">\(\uparrow\)</span><br>
Transition<br>
dynamics</span></p>
<p><span class="absolute" style="top: 180px; left: 485px; "><span class="math inline">\(\uparrow\)</span><br>
Discount Factor</span></p>
</div>
<p><br></p>
<p><strong>Value function:</strong><br>
<span class="math display">\[
V^{\pi}(s) = \mathbb{E}_{\pi,\mathcal{M}} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \,\big|\, s_0 = s \right]
\]</span></p>
<p><strong>Q function:</strong> <span class="math inline">\(Q^{\pi}(s, a) = \underline{\mathbb{E}_{\mathcal{M}} [R(s,a)]} + \underline{\mathbb{E}_{s' \sim \mathcal{T}(s,a)} [V^{\pi}(s')]}\)</span></p>
<p><br><br></p>
<p><strong>Advantage function:</strong> <span class="math inline">\(A^{\pi}(s, a) = Q^{\pi}(s,a) - V^{\pi}(s)\)</span></p>
<div class="orange tiny-font">
<p><span class="absolute" style="left: 300px; bottom: 190px; ">Reward for taking<br>
action a in state s</span></p>
<p><span class="absolute" style="left: 500px; bottom: 200px; ">Expected reward in state s’</span></p>
</div>
</section>
<section id="methodology-npg" class="slide level2">
<h2>Methodology (NPG)</h2>
<p>• Directly optimize parameters of policy to maximize objective</p>
<p><br></p>
<p><span class="orange tiny-font absolute" style="top: 170px; left: 200px; ">Sub-optimal <span class="math inline">\(\searrow\)</span></span></p>
<p><strong>Vanilla Policy Gradient:</strong> <span class="math inline">\(g = \frac{1}{NT} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t^i \mid s_t^i) \hat{A}^{\pi}(s_t^i, a_t^i, t)\)</span></p>
<p><strong>Fisher Information Matrix:</strong> <span class="math inline">\(F_{\theta} = \frac{1}{NT} \sum_{i=1}^{N} \nabla_{\theta} \log \pi_{\theta}(a_t^i \mid s_t^i) \nabla_{\theta} \log \pi_{\theta}(a_t^i \mid s_t^i)^{T}\)</span></p>
<p><br></p>
<p>• Fisher information matrix measures the curvature (sensitivity) of policy relative to model parameters</p>
<p>• Fisher information matrix is related to the Hessian matrix</p>
</section>
<section id="methodology-npg-1" class="slide level2">
<h2>Methodology (NPG)</h2>
<p>• Limit policy change based on parameter change</p>
<p>• Fisher information matrix maps between parameter space and policy space</p>
<p>• Generally use learning rate in optimization</p>
<p>• Poor step size leads to poor initialization</p>
<p>• Use Fisher information matrix to perform update</p>
<p><br></p>
<p><strong>Gradient ascent update:</strong> <span class="math inline">\(\theta_{k+1} = \theta_k + \underline{\sqrt{\frac{\delta}{g^T F_{\theta_k}^{-1} g}}} \, \overline{F_{\theta_k}^{-1} g}\)</span></p>
<div class="orange small-math">
<p><span class="absolute" style="bottom: 210px; right: 400px; ">Normalized step-size</span></p>
<p><span class="absolute" style="bottom: 320px; right: 280px; ">Steepest Ascent direction</span></p>
</div>
</section>
<section id="methodology-problems-with-rl" class="slide level2">
<h2>Methodology (Problems with RL)</h2>
<p>• Challenges with using NPG</p>
<div class="line-block">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• RL requires careful reward shaping<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Impractical number of samples to learn (approx. 100 hours)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Unnatural movement<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Not as robust to environmental variations</div>
<p><br></p>
<p>• Solution</p>
<div class="line-block">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Combine RL with demonstrations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Guide exploration and decrease sample complexity<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Robust and natural looking behaviour<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Demonstration Augmented Policy Gradient (DAPG)</div>
</section>
<section id="methodology-pretraining-with-bc" class="slide level2">
<h2>Methodology (Pretraining with BC)</h2>
<p>• Exploration in PG achieved with stochastic action distribution<br>
• Poor initialization leads to slow exploration<br>
• Behavioral Cloning (BC) guides exploration<br>
• Reduces sample complexity</p>
<p><span class="math display">\[
\text{maximize}_{\theta} \sum_{(s, a) \in \rho_D} \ln \pi_{\theta}(a \mid s)
\]</span></p>
<p>• Mimic actions taken in demonstrations<br>
• Does not guarantee effectiveness of policy due to distributional shift</p>
</section>
<section id="methodology-fine-tuning-with-augmented-loss" class="slide level2">
<h2>Methodology (Fine-tuning with augmented loss)</h2>
<p>• BC does not make optimal use of demonstrations<br>
• Cannot learn subtasks (reaching, grasping, hammering)<br>
• BC policy (only grasping)<br>
• Capturing all data</p>
<p><span class="math display">\[
g_{\text{aug}} = \sum_{(s,a) \in \rho_{\pi}} \overline{\nabla_{\theta} \ln \pi_{\theta}(a \mid s)} \, A^{\pi}(s, a) + \sum_{(s,a) \in \rho_{D}} \overline{\nabla_{\theta} \ln \pi_{\theta}(a \mid s)} \, w(s, a)
\]</span></p>
<p><br></p>
<p><span class="math display">\[
w(s, a) = \lambda_0 \lambda_1^k \max_{(s', a') \in \rho_\pi} A^\pi(s', a') \quad \forall (s, a) \in \rho_D
\]</span></p>
<div class="orange small-math">
<p><span class="absolute" style="left: 300px; bottom: 360px; ">Policy gradient</span></p>
<p><span class="absolute" style="bottom: 360px; right: 200px; ">Behavioral cloning</span></p>
<p><span class="absolute" style="left: 235px; bottom: 230px; "><span class="math inline">\(\uparrow\)</span><br>
Dataset from policy</span></p>
<p><span class="absolute" style="bottom: 210px; right: 260px; "><span class="math inline">\(\uparrow\)</span><br>
Dataset from<br>
demonstrations</span></p>
<p><span class="absolute" style="bottom: 240px; right: 70px; "><span class="math inline">\(\uparrow\)</span><br>
Weighting<br>
function</span></p>
<p><span class="absolute" style="left: 400px; bottom: 190px; "><span class="math inline">\(\downarrow\)</span> iteration</span></p>
<p><span class="absolute" style="left: 350px; bottom: 90px; "><span class="math inline">\(\uparrow \quad \uparrow\)</span><br>
hyperparameters</span></p>
</div>
</section>
<section id="results-1" class="slide level2">
<h2>Results 1</h2>
<p><u>Reinforcement learning from scratch</u></p>
<p>• Can RL cope with high dimensional manipulation tasks ?<br>
• Is it robust to variations in environment ?<br>
• Are movements safe and can they be used on real hardware ?</p>
<p><br></p>
<p>• Compare NPG vs DDPG (Deep Deterministic Policy Gradient)<br>
• DDPG is a policy gradient actor-critic algorithm that is off-policy<br>
• Stochastic policy for exploration, estimates deterministic policy<br>
• Score based on percentage of successful trajectories (100 samples)<br>
• Sparse Reward vs Reward shaping</p>
</section>
<section id="results-1-1" class="slide level2">
<h2>Results 1</h2>
<p><u>Reinforcement learning from scratch</u></p>

<img data-src="images/reinforcement-learning.png" class="r-stretch"><p>• NPG learns with reward shaping, DDPG fails to learn<br>
• DDPG is sample efficient but sensitive to hyper-parameters<br>
• Resulting policies have unnatural behaviors<br>
• Poor sample efficiency, cant use on hardware<br>
• Cannot generalize to unseen environment (weight and ball size change)</p>
</section>
<section id="results-2" class="slide level2">
<h2>Results 2</h2>
<p><u>Reinforcement learning with demonstrations</u></p>
<p>• Does incorporating demonstrations reduce learning time?<br>
• Comparison of DAPG vs DDPGfD (.. from Demonstrations)?<br>
• Does it result in human like behaviour ?</p>
<p><br></p>
<p>• DDPGfD better version of DDPG (demonstrations in replay buffer, prioritized experience replay, n-step returns, regularization)<br>
• Only use sparse rewards</p>
</section>
<section id="results-2-1" class="slide level2">
<h2>Results 2</h2>
<p><u>Reinforcement learning with demonstrations</u></p>
<p><img data-src="images/reinforcement-learning2.png"></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>• DAPG outperforms DDPGfD<br>
• DAPG requires few robot hours<br>
• Can be used on real hardware<br>
• Robust and human behavior<br>
• Generalizes to unseen environment</p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/dapg-table.png"></p>
</div></div>
</section>
<section id="future-work" class="slide level2">
<h2>Future Work</h2>
<p>• Tests on real hardware</p>
<p>• Reduce sample complexity using novelty based exploration methods</p>
<p>• Learn policies from raw visual inputs and tactile sensing</p>
</section>
<section id="results" class="slide level2">
<h2>Results</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/jJtBll8l_OM" width="800" height="500" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="tiny-font"><a href="https://www.youtube.com/watch?v=jJtBll8l_OM" class="uri">https://www.youtube.com/watch?v=jJtBll8l_OM</a></span></p>
</section>
<section id="todays-agenda-3" class="slide level2">
<h2>Today’s agenda</h2>
<div class="grey-text">
<p>• Guided policy search (GPS)</p>
<p>• Policy learning from adaptive MPC with privileged information (PLATO)</p>
<p>• Combining behavioral cloning and RL</p>
</div>
<p>• Dynamic movement primitives (DMPs)</p>
<p>• Expert iteration &amp; learning to search</p>

<aside><div>
<p>Acknowledgments<br>
Today’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:<br>
<a href="https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory" class="uri">https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory</a></p>
</div></aside></section>
<section id="section-7" class="slide level2">
<h2></h2>
<iframe data-external="1" src="https://www.youtube.com/embed/IV8Eze9Hxrc" width="900" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p><span class="tiny-font"><a href="https://www.youtube.com/watch?v=IV8Eze9Hxrc" class="uri">https://www.youtube.com/watch?v=IV8Eze9Hxrc</a></span></p>
</section>
<section id="todays-agenda-4" class="slide level2">
<h2>Today’s agenda</h2>
<div class="grey-text">
<p>• Guided policy search (GPS)</p>
<p>• Policy learning from adaptive MPC with privileged information (PLATO)</p>
<p>• Combining behavioral cloning and RL</p>
<p>• Dynamic movement primitives (DMPs)</p>
</div>
<p>• Expert iteration &amp; learning to search</p>

<aside><div>
<p>Acknowledgments<br>
Today’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:<br>
<a href="https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory" class="uri">https://www.youtube.com/watch?v=IV8Eze9Hxrc&amp;ab_channel=Dynamicfieldtheory</a></p>
</div></aside></section>
<section id="from-search-to-learning-and-back" class="slide level2">
<h2>From Search to Learning and Back</h2>

<img data-src="images/search-learning-back.png" class="r-stretch"><div class="columns" style="font-size:0.6em;">
<div class="column" style="width:30%;">
<p>Logic Geometric Programming<br>
Marc Toussaint et al.&nbsp;IJCAI’15, RSS’17</p>
<p>PDDLStream Planners<br>
Caelan Garret et al.&nbsp;ICAPS’20</p>
<p>TAMP = SMT + Motion Planning<br>
Neil Dantam et al.&nbsp;IJRR’18</p>
<p>. . .</p>
<div class="red">
<p><span class="math inline">\(-\)</span> Need to specify symbols / logic</p>
<p><span class="math inline">\(-\)</span> Slow to plan, not very reactive</p>
</div>
<div class="green">
<p><span class="math inline">\(+\)</span> Generalize</p>
<p><span class="math inline">\(+\)</span> No training data</p>
</div>
</div><div class="column" style="width:5%;">

</div><div class="column fragment" data-fragment-index="2" style="width:30%;">
<p>Learning to Guide TAMP<br>
Beomjoom Kim et al., AAAI’18</p>
<p>PLOI<br>
Tom Silver, Rohan Chitnis et al.&nbsp;AAAI’21</p>
<p>Deep Visual Heuristics<br>
Danny Driess et al.&nbsp;ICRA’20</p>
<p>Learning to Search for TAMP<br>
Mohamed Khodeir et al.&nbsp;RAL’22, ICRA’23</p>
<p>Text2Motion<br>
Chris Agia et al.&nbsp;ICRA’23</p>
<p><span class="red"><span class="math inline">\(-\)</span> Need to specify symbols / logic</span></p>
<p><span class="light-orange"><span class="math inline">\(+\)</span> Can be made fast, reactive</span></p>
<div class="green">
<p><span class="math inline">\(+\)</span> Generalize</p>
<p><span class="math inline">\(+\)</span> Few training data needed</p>
</div>
</div><div class="column" style="width:4%;">

</div><div class="column right-align fragment" data-fragment-index="1" style="width:30%;">
<p>PaLM-E<br>
Danny Driess et al.&nbsp;’23</p>
<p>SayCan<br>
Michael Ahn et al.&nbsp;CoRL‘23</p>
<p>RT-1<br>
Anthony Brohan et al.&nbsp;’22</p>
<p>Deep Affordance Foresight<br>
Danfei Xu et al.&nbsp;ICRA’21</p>
<p>PlaNet, Dreamer, Hierarchical RL<br>
Danijar Hafner et al.</p>
<div class="green">
<p><span class="math inline">\(+\)</span> Symbols not needed</p>
<p><span class="math inline">\(+\)</span> Fast, reactive</p>
</div>
<div class="red">
<p><span class="math inline">\(-\)</span> Do not generalize</p>
<p><span class="math inline">\(-\)</span> Large data regime</p>
</div>
</div></div>
</section>
<section id="learning-to-plan-via-expert-iteration" class="slide level2">
<h2>Learning to Plan via Expert Iteration</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/expert-iteration.png"></p>
</div><div class="column" style="width:50%;">
<p>If we do multiple rounds of heuristic learning and tree search, we could potentially get:</p>
<p>• monotonic improvement guarantees for the policy / planning heuristic</p>
<p>• convergence to a point where tree search and the policy are equally good</p>
</div></div>
<div class="tiny-font">
<p>Dual Policy Iteration. Sun, Gordon, Boots, Bagnell. NeurIPS’18.<br>
Thinking Fast and Slow with Deep Learning and Tree Search. Anthony, Tian, Barber. NeurIPS’17.<br>
AlphaGo Zero: Mastering the Game of Go Without Human Knowledge. Silver, Schrittwieser, Simonyan, Antonoglou. Nature’17.</p>
</div>
</section>
<section id="alphago-zero" class="slide level2">
<h2>AlphaGo Zero</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/alphago-zero.png"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/alphagozero-descrip.png"></p>
</div></div>
<p><span class="tiny-font">AlphaGo Zero: Mastering the Game of Go Without Human Knowledge. Silver, Schrittwieser, Simonyan, Antonoglou. Nature’17.</span></p>
</section>
<section id="mcts-in-alphago-zero" class="slide level2">
<h2>MCTS in AlphaGo Zero</h2>

<img data-src="images/mcts-alphago.png" class="r-stretch"><p><span class="tiny-font">AlphaGo Zero: Mastering the Game of Go Without Human Knowledge. Silver, Schrittwieser, Simonyan, Antonoglou. Nature’17.</span></p>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/dual-policy.png"></p>
</div><div class="column red small-math" style="width:50%;">
<p><br><br></p>
<p>A policy optimization framework that includes</p>
<ul>
<li>Guided Policy Search</li>
<li>Expert Iteration</li>
<li>AlphaGo Zero</li>
<li>“Thinking fast and slow”</li>
<li>AggreVaTeD (a varianbt of DAgger)</li>
</ul>
<p>as special cases and provides conditions under which we expect monotonic improvement of the fast, reactive policy.</p>
</div></div>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<p><img data-src="images/dual-policy-iteration.png" height="300"></p>
<p><img data-src="images/dual-policy-steps.png"></p>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<p><img data-src="images/dual-policy-iteration.png" height="300"></p>
<div class="small-math">
<p><span class="math display">\[
\begin{array}{ll}
\text{1.} &amp; \text{Fit MLE } \hat{P} \text{ on states and actions from } d_{\pi_n} \pi_n \text{ (Eq. 2).} \\
\text{2.} &amp; \eta_n \leftarrow \text{MBOC}(\hat{P}), \text{ subject to trust region } \mathbb{E}_{s \sim d_{\pi_n}} D_{TV}(\pi, \pi_n) \leq \alpha \text{ (Eq. 4)} \\
\text{3.} &amp; \boxed{\text{Update to } \pi_{n+1} \text{ by imitating } \eta_n, \text{ subject to trust region } \mathbb{E}_{s \sim d_{\pi_{\eta}}} D_{TV}(\pi, \pi_n) \leq \beta \text{ (Eq. 5)}}
\end{array}
\]</span></p>
</div>
<div class="center-align">
<p><span class="red-annotation">Main difference with respect to Guided Policy Search</span></p>
</div>
<div class="tiny-font">
<p>GPS, including the mirror descent version, phrases the update procedure of the reactive policy as<br>
a <em>behavior cloning procedure</em>, i.e., given an expert policy <span class="math inline">\(\eta\)</span>, we perform <span class="math inline">\(\min_{\pi} D_{KL}(d_{\mu \mu} \,\|\, d_{\pi \pi})^3\)</span><br>
Note that our approach to updating <span class="math inline">\(\pi\)</span> is fundamentally on-policy, i.e., we generate samples from <span class="math inline">\(\pi\)</span>.</p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://csc2626.github.io/website/" target="_blank">↩︎ Back to Course Website</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"src":"chalkboard.json","boardmarkerWidth":2,"chalkWidth":2,"chalkEffect":1},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/csc2626\.github\.io\/website\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    

</body></html>