[
  {
    "objectID": "course-grading.html",
    "href": "course-grading.html",
    "title": "Grading Scheme",
    "section": "",
    "text": "Assignment 1\n15%\n\n\nAssignment 2\n15%\n\n\nPanel Discussion\n10%\n\n\nProject Proposal\n10%\n\n\nProject Presentation\n25%\n\n\nFinal Project Report\n25%\n\n\n\n\nMarking rubric for panel discussion\nEvery week we will reserve 20-30 mins for a panel discussion based on the assigned reading for that day (4-5 papers). This discussion will include three types of roles: panel members, audience members, and a moderator. Each panel discussion will include 4 panel members, 1 moderator, and audience members. Panel members are responsible for answering questions, the audience is responsible for asking questions, and the moderator is responsible for steering the discussion and having backup questions if the audience is not asking any.\n\nPanel member evaluation\n\nAnswering questions from the moderator and the audience correctly / well (6 pts)\nEngaging with points of other panelists (1 pts)\nKeeping answers brief / allowing other people time to speak (2 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\nAudience member evaluation\n\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (10 pts)\n\n\n\nModerator evaluation\n\nSteering the discussion in terms of groups / themes of questions (2 pts)\nEnsuring there is time for every panel member to speak (4 pts)\nEngaging the audience / ensuring the audience has enough time to ask questions (3 pts)\nPre-submitting 1-2 questions on Quercus about the assigned papers of the day, the Thursday before lecture (1 pts)\n\n\n\n\nMarking rubric for the project proposal\n\nIntroduction (1 pts), which states the proposed problem being solved and any applications / implications.\nFigure or diagram (1 pts), showing an overview of your proposed solution, i.e. shows the overall idea in a way that is easily understandable without even reading the rest of the report.\nRelated work (1 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (2 pts). Describe your proposed methodology as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly.\nEvaluation (2 pts). What experiments are you planning to do and why? What are the questions you want to answer through these experiments?\nTimeline (1 pts). What are the milestones required to complete your project and by when do you plan to complete them?\nAnticipated risks and mitigation plan (2 pts). What issues might arise with your proposed project and timeline and how will you address these issues if they occur?\n\n\n\nMarking rubric for the project presentation\n\nQuality of presentation\n\nSlide design (2 pts)\nDelivery of presentation (3 pts)\nRespecting time constraints (2 pts)\nResponse to questions (3 pts)\n\n\n\nTechnical content\n\nMotivation and definition of the problem (2 pts)\nPutting prior work into context (3 pts)\nMethodology explanation (3 pts)\nDiscussion of experiments (5 pts)\nDiscussion of limitations (2 pts)\n\n\n\n\nMarking rubric for the final project report\n\nAbstract (2 pts) that summarizes the main idea of the project and your contributions.\nIntroduction (3 pts) that states the problem being solved and any applications / implications.\nFigure or diagram (2 pts) that shows the overall idea in a way that is easily understandable.\nRelated work (2 pts) and bibliography. Highlight how your method is different from other approaches. Present other approaches in the proper light without diminishing their contributions.\nMethodology (7 pts). Describe your method in detail as well as any assumptions it relies on. Explain prerequisite concepts clearly and succinctly. Include algorithm descriptions, figures, and equations as you wish.\nEvaluation (8 pts). Include any figures or tables that illustrate your experimental results. Do not forget to include error bars if applicable. Analyze your findings, and comment on their statistical significance. In your evaluation please take into account Joelle Pineau’s ML reproducibility checklist.\nLimitations (2 pts). Describe some settings in which your approach performs poorly, and list a few ideas for how to adddress them. Describe opportunities for future work, as well as open problems.\nConclusions (1 pts). A summary of your contributions and results.",
    "crumbs": [
      "Grading"
    ]
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda",
    "href": "lecs/w01/lec01.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n• Administrivia\n• Topics covered by the course\n• Behavioral cloning\n• Imitation learning\n• Teleoperation interfaces for manipulation\n• Imitation via multi-modal generative models (diffusion policy)\n• (Time permitting) Query the expert only when policy is uncertain"
  },
  {
    "objectID": "lecs/w01/lec01.html#administrivia-1",
    "href": "lecs/w01/lec01.html#administrivia-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Administrivia",
    "text": "Administrivia\nThis is a graduate level course\nCourse website: https://csc2626.github.io/website/\nDiscussion forum + announcements: https://q.utoronto.ca (Quercus)\nRequest improvements anonymously: https://www.surveymonkey.com/r/LJJV5LY\nCourse-related emails should have CSC2626 in the subject"
  },
  {
    "objectID": "lecs/w01/lec01.html#prerequisites",
    "href": "lecs/w01/lec01.html#prerequisites",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\nMandatory:\n\n• Introductory machine learning (e.g. CSC411/ECE521 or equivalent)\n• Basic linear algebra + multivariable calculus\n• Intro to probability\n• Programming skills in Python or C++ (enough to validate your ideas)\n\n\n\nIf you’re missing any of these this is not the course for you.\nYou’re welcome to audit.\n\n\n\nRecommended:\n\n• Experience training neural networks or other function approximators\n• Introductory concepts from reinforcement learning or control (e.g. value function/cost-to-go)\n\n\n\nIf you’re missing this we can organize tutorials to help you."
  },
  {
    "objectID": "lecs/w01/lec01.html#grading",
    "href": "lecs/w01/lec01.html#grading",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Grading",
    "text": "Grading\nTwo assignments: 30%\nCourse project: 60%\n\n• Project proposal: 10%\n• Project presentation: 25%\n• Final project report (6-8 pages) + code: 25%\nProject guidelines https://csc2626.github.io/website/project-description.html\n\nIn-class panel discussions: 10%\n\n\\(\\leftarrow\\) Individual submissions\n\\(\\leftarrow\\) Individual submissions"
  },
  {
    "objectID": "lecs/w01/lec01.html#evaluation-environments-simulators-real-robots",
    "href": "lecs/w01/lec01.html#evaluation-environments-simulators-real-robots",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Evaluation environments: simulators & real robots",
    "text": "Evaluation environments: simulators & real robots"
  },
  {
    "objectID": "lecs/w01/lec01.html#guiding-principles-for-this-course",
    "href": "lecs/w01/lec01.html#guiding-principles-for-this-course",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Guiding principles for this course",
    "text": "Guiding principles for this course\nRobots do not operate in a vacuum. They do not need to learn everything from scratch.\n\n\nHumans need to easily interact with robots and share our expertise with them.\n\n\n\nRobots need to learn from the behavior and experience of others, not just their own."
  },
  {
    "objectID": "lecs/w01/lec01.html#main-questions",
    "href": "lecs/w01/lec01.html#main-questions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Main questions",
    "text": "Main questions\n\n\nHow can robots incorporate others’ decisions into their own?\n\nHow can robots easily understand our objectives from demonstrations?\n\nHow do we balance autonomous control and human control in the same system?\n\n\n\n\\(\\quad \\color{blue}\\Rightarrow\\)\n\nLearning from demonstrations Apprenticeship learning Imitation learning\n\nReward/cost learning Task specification Inverse reinforcement learning Inverse optimal control Inverse optimization\n\nShared or sliding autonomy"
  },
  {
    "objectID": "lecs/w01/lec01.html#applications",
    "href": "lecs/w01/lec01.html#applications",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Applications",
    "text": "Applications\n\n\nAny control problem where:\n\nwriting down a dense cost function is difficult\nthere is a hierarchy of decision-making processes\nour engineered solutions might not cover all cases\nunrestricted exploration during learning is slow or dangerous\n\n\n\n\n\nhttps://www.youtube.com/watch?v=M8r0gmQXm1Y"
  },
  {
    "objectID": "lecs/w01/lec01.html#applications-1",
    "href": "lecs/w01/lec01.html#applications-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Applications",
    "text": "Applications\n\n\nAny control problem where:\n\nwriting down a dense cost function is difficult\nthere is a hierarchy of interacting decision-making processes\nour engineered solutions might not cover all cases\nunrestricted exploration during learning is slow or dangerous\n\n\n\n\n\nhttps://www.youtube.com/watch?v=Q3LXJGha7Ws"
  },
  {
    "objectID": "lecs/w01/lec01.html#applications-2",
    "href": "lecs/w01/lec01.html#applications-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Applications",
    "text": "Applications\n\n\nAny control problem where:\n\nwriting down a dense cost function is difficult\nthere is a hierarchy of interacting decision-making processes\nour engineered solutions might not cover all cases\nunrestricted exploration during learning is slow or dangerous\n\n\n\n\n\nhttps://www.youtube.com/watch?v=RjGe0GiiFzw"
  },
  {
    "objectID": "lecs/w01/lec01.html#applications-3",
    "href": "lecs/w01/lec01.html#applications-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Applications",
    "text": "Applications\n\n\nAny control problem where:\n\nwriting down a dense cost function is difficult\nthere is a hierarchy of interacting decision-making processes\nour engineered solutions might not cover all cases\nunrestricted exploration during learning is slow or dangerous\n\n\n\n\n\nRobot Explorer"
  },
  {
    "objectID": "lecs/w01/lec01.html#applications-4",
    "href": "lecs/w01/lec01.html#applications-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Applications",
    "text": "Applications\n\n\nAny control problem where:\n\nwriting down a dense cost function is difficult\nthere is a hierarchy of interacting decision-making processes\nour engineered solutions might not cover all cases\nunrestricted exploration during learning is slow or dangerous\n\n\n\n\n\nhttps://www.youtube.com/watch?v=0XdC1HUp-rU"
  },
  {
    "objectID": "lecs/w01/lec01.html#back-to-the-future",
    "href": "lecs/w01/lec01.html#back-to-the-future",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Back to the future",
    "text": "Back to the future\n\n\n\n\nhttps://www.youtube.com/watch?v=I39sxwYKlEE\nErnst Dickmans + Mercedes (1986-2003)\n\n\nhttps://www.youtube.com/watch?v=2KMAAmkz9go\nNavlab 1 (1986-1989)\n\n\nhttps://www.youtube.com/watch?v=ilP4aPDTBPE\nNavlab 2 + ALVINN, Dean Pomerleau’s PhD thesis (1989-1993)\n30 x 32 pixels, 3-layer network, outputs steering command, ~5 minutes of training per road type"
  },
  {
    "objectID": "lecs/w01/lec01.html#alvinn-architecture",
    "href": "lecs/w01/lec01.html#alvinn-architecture",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "ALVINN: architecture",
    "text": "ALVINN: architecture\n\n\n\n\n\n\nhttps://drive.google.com/file/d/0Bz9namoRlUKMa0pJYzRGSFVwbm8/view Dean Pomerleau’s PhD thesis"
  },
  {
    "objectID": "lecs/w01/lec01.html#alvinn-training-set",
    "href": "lecs/w01/lec01.html#alvinn-training-set",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "ALVINN: training set",
    "text": "ALVINN: training set\n\nOnline updates via backpropagation"
  },
  {
    "objectID": "lecs/w01/lec01.html#problems-identified-by-pomerleau",
    "href": "lecs/w01/lec01.html#problems-identified-by-pomerleau",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Problems Identified by Pomerleau",
    "text": "Problems Identified by Pomerleau\n\n\n\n\n\n\nTest distribution is different from training distribution (covariate shift)\n\n\n\n\nCatastrophic forgetting"
  },
  {
    "objectID": "lecs/w01/lec01.html#partially-addressing-covariate-shift",
    "href": "lecs/w01/lec01.html#partially-addressing-covariate-shift",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "(Partially) Addressing Covariate Shift",
    "text": "(Partially) Addressing Covariate Shift"
  },
  {
    "objectID": "lecs/w01/lec01.html#partially-addressing-catastrophic-forgetting",
    "href": "lecs/w01/lec01.html#partially-addressing-catastrophic-forgetting",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "(Partially) Addressing Catastrophic Forgetting",
    "text": "(Partially) Addressing Catastrophic Forgetting\n\n\nMaintains a buffer of old (image, action) pairs\nExperiments with different techniques to ensure diversity and avoid outliers"
  },
  {
    "objectID": "lecs/w01/lec01.html#behavioral-cloning-supervised-learning",
    "href": "lecs/w01/lec01.html#behavioral-cloning-supervised-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Behavioral Cloning = Supervised Learning",
    "text": "Behavioral Cloning = Supervised Learning"
  },
  {
    "objectID": "lecs/w01/lec01.html#years-later-what-has-changed",
    "href": "lecs/w01/lec01.html#years-later-what-has-changed",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "25 years later: what has changed?",
    "text": "25 years later: what has changed?\n\nhttps://www.youtube.com/watch?v=qhUvQiKec2U"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-has-changed",
    "href": "lecs/w01/lec01.html#what-has-changed",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What has changed?",
    "text": "What has changed?\n\n\n\n\n\n\n\nEnd to End Learning for Self-Driving Cars, Bojarski et al, 2016"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-has-changed-1",
    "href": "lecs/w01/lec01.html#what-has-changed-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What has changed?",
    "text": "What has changed?\n\n\n\n“Our collected data is labeled with road type, weather condition, and the driver’s activity (staying in a lane, switching lanes, turning, and so forth).”\n\n\n\n\nEnd to End Learning for Self-Driving Cars, Bojarski et al, 2016"
  },
  {
    "objectID": "lecs/w01/lec01.html#what-has-changed-2",
    "href": "lecs/w01/lec01.html#what-has-changed-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What has changed?",
    "text": "What has changed?"
  },
  {
    "objectID": "lecs/w01/lec01.html#how-much-has-changed",
    "href": "lecs/w01/lec01.html#how-much-has-changed",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "How much has changed?",
    "text": "How much has changed?\n\n\nA Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots, Giusti et al., 2016\nhttps://www.youtube.com/watch?v=umRdt3zGgpU"
  },
  {
    "objectID": "lecs/w01/lec01.html#how-much-has-changed-1",
    "href": "lecs/w01/lec01.html#how-much-has-changed-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "How much has changed?",
    "text": "How much has changed?\n\nNot a lot for learning lane following with neural networks.\n\nBut, there are a few other beautiful ideas that do not involve end-to-end learning."
  },
  {
    "objectID": "lecs/w01/lec01.html#visual-teach-repeat",
    "href": "lecs/w01/lec01.html#visual-teach-repeat",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Visual Teach & Repeat",
    "text": "Visual Teach & Repeat\nHuman Operator or Planning Algorithm\n\nVisual Path Following on a Manifold in Unstructured Three-Dimensional Terrain, Furgale & Barfoot, 2010"
  },
  {
    "objectID": "lecs/w01/lec01.html#visual-teach-repeat-1",
    "href": "lecs/w01/lec01.html#visual-teach-repeat-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Visual Teach & Repeat",
    "text": "Visual Teach & Repeat\n\n\nKey Idea #1: Manifold Map\nBuild local maps relative to the\npath. No global coordinate frame.\n\n\n\n\nKey Idea #2: Visual Odometry\nGiven two consecutive images,\nhow much has the camera\nmoved? Relative motion.\n\n\nVisual Path Following on a Manifold in Unstructured Three-Dimensional Terrain, Furgale & Barfoot, 2010"
  },
  {
    "objectID": "lecs/w01/lec01.html#visual-teach-repeat-2",
    "href": "lecs/w01/lec01.html#visual-teach-repeat-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Visual Teach & Repeat",
    "text": "Visual Teach & Repeat\n\n\n https://www.youtube.com/watch?v=_ZdBfU4xJnQ\n\n https://www.youtube.com/watch?v=9dN0wwXDuqo\n\nCentimeter-level precision in tracking the demonstrated path over kilometers-long trails."
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda-1",
    "href": "lecs/w01/lec01.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Administrivia\n• Topics covered by the course\n• Behavioral cloning\n\n• Imitation learning\n• Teleoperation interfaces for manipulation\n• Imitation via multi-modal generative models (diffusion policy)\n• (Time permitting) Query the expert only when policy is uncertain"
  },
  {
    "objectID": "lecs/w01/lec01.html#nomenclature",
    "href": "lecs/w01/lec01.html#nomenclature",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Nomenclature",
    "text": "Nomenclature\n• Offline imitation learning:\n     Learn from a fixed dataset (eg behavioral cloning).\n     Cannot interact with the environment.\n\n• Online imitation learning:\n     Learn from a dataset that is not fixed.\n     Gather new data by interacting with the environment."
  },
  {
    "objectID": "lecs/w01/lec01.html#back-to-pomerleau",
    "href": "lecs/w01/lec01.html#back-to-pomerleau",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Back to Pomerleau",
    "text": "Back to Pomerleau\n\n\n\nTest distribution is different from training distribution (covariate shift)\n\n(Ross & Bagnell, 2010): How are we sure these errors are not due to overfitting or underfitting?\n\nMaybe the network was too small (underfitting)\nMaybe the dataset was too small and the network overfit it\n\n\n\n\n\n\n\\(\\Rightarrow\\)\n\n\nSteering commands \\(\\pi_\\theta (s) = \\theta^\\top s\\) where s are image features\n\n\nEfficient reductions for imitation learning. Ross & Bagnell, AISTATS 2010.\n\n\nIt was not 1: they showed that even a linear policy can work well.\nIt was not 2: their error on held-out data was close to training error."
  },
  {
    "objectID": "lecs/w01/lec01.html#imitation-learning-neq-supervised-learning",
    "href": "lecs/w01/lec01.html#imitation-learning-neq-supervised-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation learning \\(\\neq\\) Supervised learning",
    "text": "Imitation learning \\(\\neq\\) Supervised learning\n\n\n\nTest distribution is different from training distribution (covariate shift)\n\n(Ross & Bagnell, 2010): IL is a sequential decision-making problem.\n• Your actions affect future observations/data.\n• This is not the case in supervised learning\n\n\nImitation Learning \\(\\qquad \\qquad \\LARGE \\longleftarrow\\)\nTrain/test data are not i.i.d.\nIf expected training error \\(\\delta\\) is \\(\\epsilon\\)\nExpected test error after T\ndecisions is up to \\[\nT^2 \\epsilon\n\\]\nErrors compound\n\nSupervised Learning\nAssumes train/test data are i.i.d.\nIf expected training error \\(\\delta\\) is \\(\\epsilon\\)\nExpected test error after T decisions \\[\nT \\epsilon\n\\]\n\nErrors are independent"
  },
  {
    "objectID": "lecs/w01/lec01.html#why-do-errors-accumulate-quadratically-if-we-use-a-behavioral-cloning-policy",
    "href": "lecs/w01/lec01.html#why-do-errors-accumulate-quadratically-if-we-use-a-behavioral-cloning-policy",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Why do errors accumulate quadratically if we use a behavioral cloning policy?",
    "text": "Why do errors accumulate quadratically if we use a behavioral cloning policy?\n\nEfficient reductions for imitation learning. Ross & Bagnell, AISTATS 2010."
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger",
    "href": "lecs/w01/lec01.html#dagger",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger",
    "text": "DAgger\n\n\n\n\n(Ross & Gordon & Bagnell, 2011): DAgger, or Dataset Aggregation\n• Imitation learning as interactive supervision\n• Aggregate training data from expert with test data from execution\n\n\nA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. Ross, Gordon, Bagnell, AISTATS 2010."
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger-1",
    "href": "lecs/w01/lec01.html#dagger-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger",
    "text": "DAgger\n\n\n\n\n(Ross & Gordon & Bagnell, 2011): DAgger, or Dataset Aggregation\n• Imitation learning as interactive supervision\n• Aggregate training data from expert with test data from execution\n\n\n\nImitation Learning via DAgger\nTrain/test data are not i.i.d.\nIf expected training error on aggr. dataset is \\(\\epsilon\\)\nExpected test error after T decisions is\n\\[\nO(T\\epsilon)\n\\]\nErrors do not compound\n\nSupervised Learning\nAssumes train/test data are i.i.d.\nIf expected training error is \\(\\epsilon\\)\nExpected test error after T decisions\n\\[\nT\\epsilon\n\\]\nErrors are independent"
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger-2",
    "href": "lecs/w01/lec01.html#dagger-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger",
    "text": "DAgger\n\n\n\n\nInitial expert trajectories\n\n\nSupervised learning\n\n\nDAgger\n\nhttps://www.youtube.com/watch?v=V00npNnWzSU"
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger-3",
    "href": "lecs/w01/lec01.html#dagger-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger",
    "text": "DAgger"
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger-4",
    "href": "lecs/w01/lec01.html#dagger-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger",
    "text": "DAgger\n\nQ: Any drawbacks of using it in a robotics setting?"
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger-5",
    "href": "lecs/w01/lec01.html#dagger-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger",
    "text": "DAgger\n\n\nhttps://www.youtube.com/watch?v=hNsP6-K3Hn4\nLearning Monocular Reactive UAV Control in Cluttered Natural Environments, Ross et al, 2013"
  },
  {
    "objectID": "lecs/w01/lec01.html#dagger-assumptions-for-theoretical-guarantees",
    "href": "lecs/w01/lec01.html#dagger-assumptions-for-theoretical-guarantees",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "DAgger: Assumptions for theoretical guarantees",
    "text": "DAgger: Assumptions for theoretical guarantees\n\n\n\nStrongly convex loss\nNo-regret online learner\n\n(Ross & Gordon & Bagnell, 2011): DAgger, or Dataset Aggregation\n• Imitation learning as interactive supervision\n• Aggregate training data from expert with test data from execution\n\n\n\nImitation Learning via DAgger\nTrain/test data are not i.i.d.\nIf expected training error on aggr. dataset is \\(\\epsilon\\)\nExpected test error after T decisions is\n\\[\nO(T\\epsilon)\n\\]\nErrors do not compound\n\nSupervised Learning\nAssumes train/test data are i.i.d.\nIf expected training error is \\(\\epsilon\\)\nExpected test error after T decisions\n\\[\nT\\epsilon\n\\]\nErrors are independent"
  },
  {
    "objectID": "lecs/w01/lec01.html#no-regret-online-learners",
    "href": "lecs/w01/lec01.html#no-regret-online-learners",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "No-Regret Online Learners",
    "text": "No-Regret Online Learners\nIntuition: No matter what the distribution of input data, your online policy/classifier will do asymptotically as well as the best-in-hindsight policy/classifier.\n\n\n\n\\[\nr_N = \\frac{1}{N} \\sum_{i=1}^{N} L_i(\\theta_i) - \\min_{\\theta \\in \\Theta} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} L_i(\\theta) \\right]\n\\]\n\n\\(\\qquad \\qquad \\quad \\big\\uparrow\\)\nPolicy has access to\ndata up to round N\n\n\n\\(\\qquad \\quad \\big\\uparrow\\)\nPolicy has access to\ndata up to round i\n\n\nNo-regret: \\(\\lim_{N \\to \\infty} r_N = 0\\)\n\n\nAnother way to say this: a no-regret online algorithm is one that outputs a sequence of policies \\(\\pi_1, \\ldots, \\pi_N\\) such that the average loss with respect to the best-in-hindsight policy goes to 0 as \\(N \\to \\infty\\)\nDAgger is a no-regret online learning algorithm"
  },
  {
    "objectID": "lecs/w01/lec01.html#no-regret-online-learners-1",
    "href": "lecs/w01/lec01.html#no-regret-online-learners-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "No-Regret Online Learners",
    "text": "No-Regret Online Learners\nIntuition: No matter what the distribution of input data, your online policy/classifier will do asymptotically as well as the best-in-hindsight policy/classifier.\n\n\n\n\\[\nr_N = \\frac{1}{N} \\sum_{i=1}^{N} L_i(\\theta_i) - \\min_{\\theta \\in \\Theta} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} L_i(\\theta) \\right]\n\\]\n\n\\(\\qquad \\qquad \\quad \\big\\uparrow\\)\nPolicy has access to\ndata up to round N\n\n\n\\(\\qquad \\quad \\big\\uparrow\\)\nPolicy has access to\ndata up to round i\n\n\nNo-regret: \\(\\lim_{N \\to \\infty} r_N = 0\\)\n\nWe can see Dagger as an adversarial game between the imitation learner (policy) and an adversary (environment):"
  },
  {
    "objectID": "lecs/w01/lec01.html#is-the-quadratic-regret-in-horizon-unavoidable-for-behavioral-cloningno",
    "href": "lecs/w01/lec01.html#is-the-quadratic-regret-in-horizon-unavoidable-for-behavioral-cloningno",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Is the quadratic regret in horizon unavoidable for behavioral cloning?No",
    "text": "Is the quadratic regret in horizon unavoidable for behavioral cloning?No"
  },
  {
    "objectID": "lecs/w01/lec01.html#is-the-quadratic-regret-in-horizon-unavoidable-for-behavioral-cloningno-1",
    "href": "lecs/w01/lec01.html#is-the-quadratic-regret-in-horizon-unavoidable-for-behavioral-cloningno-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Is the quadratic regret in horizon unavoidable for behavioral cloning?No",
    "text": "Is the quadratic regret in horizon unavoidable for behavioral cloning?No\n\n\n\n\n\n\n\n• Dagger’s analysis uses 0-1 loss to show quadratic regret in horizon for BC. It also only considers deterministic and linear policies.\n\n• If instead we use log-loss BC:\n\\[\n\\hat{\\pi} = \\arg\\min_{\\pi \\in \\Pi} \\left[ -\\sum_{i=1}^{N} \\sum_{t=1}^{T} \\log \\pi(a_t^i | x_t^i) \\right]\n\\]\nand we normalize the reward then\n\n• BC can be shown to have linear regret in horizon, even for neural network policies."
  },
  {
    "objectID": "lecs/w01/lec01.html#todays-agenda-2",
    "href": "lecs/w01/lec01.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Administrivia\n• Topics covered by the course\n• Behavioral cloning\n• Imitation learning\n\n• Teleoperation interfaces for manipulation\n• Imitation via multi-modal generative models (diffusion policy)\n• (Time permitting) Query the expert only when policy is uncertain"
  },
  {
    "objectID": "lecs/w01/lec01.html#appendix-2-why-do-behavioral-cloning-errors-accumulate-quadratically",
    "href": "lecs/w01/lec01.html#appendix-2-why-do-behavioral-cloning-errors-accumulate-quadratically",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix 2: Why do behavioral cloning errors accumulate quadratically?",
    "text": "Appendix 2: Why do behavioral cloning errors accumulate quadratically?\n\n\n\n\n\n\n\n\nEfficient reductions for imitation learning. Ross & Bagnell, AISTATS 2010."
  },
  {
    "objectID": "lecs/w01/lec01.html#appendix-3-types-of-uncertainty-query-efficient-imitation",
    "href": "lecs/w01/lec01.html#appendix-3-types-of-uncertainty-query-efficient-imitation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix 3: Types of Uncertainty & Query-Efficient Imitation",
    "text": "Appendix 3: Types of Uncertainty & Query-Efficient Imitation\nLet’s revisit the two main ideas from query-efficient imitation:\n\n1. DropoutDAgger:\n          Keep an ensemble of learner policies, and only query the expert when they significantly disagree\n\n2. SHIV, SafeDagger, MMD-IL:\n          (Roughly) Query expert only if input is too close to the decision boundary of the learner’s policy\n\n\nNeed to review a few concepts about different types of uncertainty."
  },
  {
    "objectID": "lecs/w01/lec01.html#biased-coin",
    "href": "lecs/w01/lec01.html#biased-coin",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Biased Coin",
    "text": "Biased Coin\n\n\\[\np(\\text{heads}_3 \\mid \\underbrace{\\text{heads}_1, \\text{heads}_2}_{\\textbf{observations}}) = ?\n\\]"
  },
  {
    "objectID": "lecs/w01/lec01.html#biased-coin-1",
    "href": "lecs/w01/lec01.html#biased-coin-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Biased Coin",
    "text": "Biased Coin\n\n\\[\np(\\text{heads}_3 \\mid \\text{heads}_1, \\text{heads}_2) = \\int p(\\text{heads}_3 \\mid \\theta) \\underbrace{p(\\theta \\mid \\text{heads}_1, \\text{heads}_2)}_{\\textbf{how biased is the coin?}} d\\theta\n\\]\n\nInduces uncertainty in the model, or epistemic uncertainty,\nwhich asymptotically goes to 0 with infinite observations"
  },
  {
    "objectID": "lecs/w01/lec01.html#biased-coin-2",
    "href": "lecs/w01/lec01.html#biased-coin-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Biased Coin",
    "text": "Biased Coin\n\n\\[\np(\\text{heads}_3 \\mid \\text{heads}_1, \\text{heads}_2) = \\int p(\\text{heads}_3 \\mid \\theta) p(\\theta \\mid \\text{heads}_1, \\text{heads}_2) d\\theta\n\\]\nQ: Even if you eventually discover the true model, can you predict if the next flip will be heads?\n\nA: No, there is irreducible uncertainty / observation noise in the system. This is called aleatoric uncertainty."
  },
  {
    "objectID": "lecs/w01/lec01.html#gaussian-process-regression",
    "href": "lecs/w01/lec01.html#gaussian-process-regression",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\n\n\n\nhttp://pyro.ai/examples/gp.html\n\n\n\\(p(\\text{y} \\mid \\text{x}, \\text{D}) = ?\\)"
  },
  {
    "objectID": "lecs/w01/lec01.html#gaussian-process-regression-1",
    "href": "lecs/w01/lec01.html#gaussian-process-regression-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\n\n\n\nhttp://pyro.ai/examples/gp.html\n\n\n\n\n\\[\np(\\text{y} \\mid \\text{x}, \\text{D}) = \\int p(\\text{y} \\mid f) {p(f \\mid x, D)} df\n\\]\n\n\n\n\\(f \\mid x, D \\sim \\mathcal{N}(f; 0, K) \\qquad\\) Zero mean prior over functions\n\\(y \\mid f \\sim \\mathcal{N}(y; f, \\sigma^2) \\qquad \\quad\\) Noisy observations"
  },
  {
    "objectID": "lecs/w01/lec01.html#gaussian-process-regression-2",
    "href": "lecs/w01/lec01.html#gaussian-process-regression-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\n\n\n\nhttp://pyro.ai/examples/gp.html\n\n\n\n\n\\[\np(\\text{y} \\mid \\text{x}, \\text{D}) = \\int p(\\text{y} \\mid f) {p(f \\mid x, D)} df\n\\]\n\n\n\n\\(f \\mid x, D \\sim \\mathcal{N}(f; 0, K) \\qquad\\) Zero mean prior over functions\n\\(y \\mid f \\sim \\mathcal{N}(y; f, \\sigma^2) \\qquad \\quad\\) Noisy observations"
  },
  {
    "objectID": "lecs/w01/lec01.html#gaussian-process-regression-3",
    "href": "lecs/w01/lec01.html#gaussian-process-regression-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Gaussian Process Regression",
    "text": "Gaussian Process Regression\n\n\n\nhttp://pyro.ai/examples/gp.html\n\n\n\n\n\\[\np(\\text{y} \\mid \\text{x}, \\text{D}) = \\int p(\\text{y} \\mid f) {p(f \\mid x, D)} df\n\\]\n\n\n\n\\(f \\mid x, D \\sim \\mathcal{N}(f; 0, K) \\qquad\\) Zero mean prior over functions\n\\(y \\mid f \\sim \\mathcal{N}(y; f, \\sigma^2) \\qquad \\quad\\) Noisy observations"
  },
  {
    "objectID": "lecs/w01/lec01.html#gaussian-process-classification",
    "href": "lecs/w01/lec01.html#gaussian-process-classification",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Gaussian Process Classification",
    "text": "Gaussian Process Classification\n\n\n\nGaussian Processes for Machine Learning, chapter 2\n\n\n\nGP handles uncertainty in f by averaging\nwhile SVM considers only best f for classification."
  },
  {
    "objectID": "lecs/w01/lec01.html#model-uncertainty-in-neural-networks",
    "href": "lecs/w01/lec01.html#model-uncertainty-in-neural-networks",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model Uncertainty in Neural Networks",
    "text": "Model Uncertainty in Neural Networks\nWant \\(p(y|x, D) = \\int p(y|x, f) \\, p(f|D) \\, df\\)\n\nBut easier to control network weights \\(p(y|x, D) = \\int p(y|x, w) \\, \\underline{\\color{red} p(w|D)} \\, dw\\)\n\n\n\n\\(\\LARGE \\color{red}\\nearrow\\) approximates\n\n\n\\[\nq(y|x) = \\int p(y|x, w) q_{\\theta^*}(w) dw\n\\]\n\n\\(\\color{red} \\LARGE \\big\\uparrow\\) Variational inference\n\n\n\n\\[\n\\theta^* = \\arg\\min_{\\theta} KL(q_{\\theta}(w) || p(w|D)) \\quad \\color{red} \\LARGE \\longleftarrow\n\\]\n\n\n\nHow do we represent posterior over network weights?\nHow do we quickly sample from it?\n\n\n\nMain ideas:\n\nUse an ensemble of networks trained on different copies of D (bootstrap method)\nUse an approximate distribution over weights (Dropout, Bayes by Backprop, …)\nUse MCMC to sample weights"
  },
  {
    "objectID": "lecs/w07/lec07.html#todays-agenda",
    "href": "lecs/w07/lec07.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n• Learning programs based on execution traces (NPI - Neural Programmer Interpreters)\n• Extending NPI for video-based robot imitation (NTP - Neural Task Programming)\n• Inferring sub-task boundaries (TACO - Temporal Alignment for Control)\n• Learning to search in Task and Motion Planning (TAMP)\n• Generalization through imitation – using hierarchical policies\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Zeqi Li, Angran Li, Zihan Fu"
  },
  {
    "objectID": "lecs/w07/lec07.html#neural-programmer-interpreters",
    "href": "lecs/w07/lec07.html#neural-programmer-interpreters",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Neural Programmer-Interpreters",
    "text": "Neural Programmer-Interpreters\nBy Scott Reed & Nando de Freitas"
  },
  {
    "objectID": "lecs/w07/lec07.html#motivation",
    "href": "lecs/w07/lec07.html#motivation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation",
    "text": "Motivation\nNeural Programmer-Interpreters (NPI) is an attempt to use neural methods to train machines to carry out simple tasks based on a small amount of training data."
  },
  {
    "objectID": "lecs/w07/lec07.html#recurrent-neural-network-rnn",
    "href": "lecs/w07/lec07.html#recurrent-neural-network-rnn",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recurrent neural network (RNN)",
    "text": "Recurrent neural network (RNN)\n\n• RNN is a neural network with feedback\n• Hidden state is to capture history information and current state of the network"
  },
  {
    "objectID": "lecs/w07/lec07.html#long-short-term-memory-lstm",
    "href": "lecs/w07/lec07.html#long-short-term-memory-lstm",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\n\n\n\n\n\n• LSTM is a special kind of RNN\n• Gates are used to control information flow. Just like a valve"
  },
  {
    "objectID": "lecs/w07/lec07.html#model",
    "href": "lecs/w07/lec07.html#model",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model",
    "text": "Model\n• The NPI core is a LSTM network that learns to represent and execute programs given their execution traces"
  },
  {
    "objectID": "lecs/w07/lec07.html#npi-core-module",
    "href": "lecs/w07/lec07.html#npi-core-module",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "NPI core module",
    "text": "NPI core module"
  },
  {
    "objectID": "lecs/w07/lec07.html#algorithm---inference",
    "href": "lecs/w07/lec07.html#algorithm---inference",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - inference",
    "text": "Algorithm - inference\n\nLine 3: \\(𝑀^{𝑝𝑟𝑜𝑔}\\) and \\(𝑀^{𝑘𝑒𝑦}\\) are memory banks to store program embeddings and program keys"
  },
  {
    "objectID": "lecs/w07/lec07.html#algorithm---inference-1",
    "href": "lecs/w07/lec07.html#algorithm---inference-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - inference",
    "text": "Algorithm - inference\n\nLine 7: \\((𝑀_{𝑗,}^{𝑘𝑒𝑦})^{𝑇}𝑘\\) is directly measurement for cosine similarity"
  },
  {
    "objectID": "lecs/w07/lec07.html#algorithm---inference-2",
    "href": "lecs/w07/lec07.html#algorithm---inference-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - inference",
    "text": "Algorithm - inference\n\n\n(1): \\(𝑀^{𝑝𝑟𝑜𝑔}\\) and \\(𝑀^{𝑘𝑒𝑦}\\) are memory storing program embeddings and program keys\n(2): \\(𝑓_{𝑒𝑛𝑐}\\) is a domain-specific encoder (for different tasks, have different encoders)\n(3): \\(𝑓_{𝑒𝑛𝑑}\\) is to calculate the probability of finishing the program\n(4): \\(𝑓_{𝑝𝑟𝑜𝑔}\\) is to retrieve the next program key from memory\n(5): \\(𝑓_{𝑎𝑟𝑔}\\) is to return the next program’s arguments\n(6): \\((𝑀_{𝑗,}^{𝑘𝑒𝑦})^{𝑇}𝑘\\) is to measure cosine similarity\n(7): \\(𝑓_{env}\\) is a domain-specific transition mapping"
  },
  {
    "objectID": "lecs/w07/lec07.html#training",
    "href": "lecs/w07/lec07.html#training",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Training",
    "text": "Training\nDirectly maximize the probability of the correct execution trace output \\(𝞷^{𝑜𝑢𝑡}\\) conditioned on \\(𝞷^{𝑖𝑛𝑝}\\):\n\\[\n𝜃^∗ = 𝑎𝑟𝑔 \\underset{𝜃}{max} \\sum_{(𝞷^{𝑖𝑛𝑝}, 𝞷^{𝑜𝑢𝑡})} 𝑙𝑜𝑔𝑃(𝞷^{𝑜𝑢𝑡}|𝞷^{𝑖𝑛𝑝}, 𝜃)\n\\]\nThen we can just run gradient ascent to optimize it"
  },
  {
    "objectID": "lecs/w07/lec07.html#tasks",
    "href": "lecs/w07/lec07.html#tasks",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Tasks",
    "text": "Tasks\n• Addition\n• Teach the model the standard grade school algorithm of adding 2 base-10 numbers\n• Sorting\n• Teach the model bubble sorting to sort an array of numbers in ascending order\n• Canonicalizing 3D models\n• Teach the model to generate a trajectory of actions that delivers the camera to the target view, e.g, frontal pose at a 15° elevation"
  },
  {
    "objectID": "lecs/w07/lec07.html#adding-numbers-together",
    "href": "lecs/w07/lec07.html#adding-numbers-together",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Adding numbers together",
    "text": "Adding numbers together"
  },
  {
    "objectID": "lecs/w07/lec07.html#addition-demo",
    "href": "lecs/w07/lec07.html#addition-demo",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Addition demo",
    "text": "Addition demo"
  },
  {
    "objectID": "lecs/w07/lec07.html#bubble-sort",
    "href": "lecs/w07/lec07.html#bubble-sort",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Bubble sort",
    "text": "Bubble sort"
  },
  {
    "objectID": "lecs/w07/lec07.html#sorting-demo",
    "href": "lecs/w07/lec07.html#sorting-demo",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Sorting demo",
    "text": "Sorting demo"
  },
  {
    "objectID": "lecs/w07/lec07.html#canonicalizing-3d-models",
    "href": "lecs/w07/lec07.html#canonicalizing-3d-models",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Canonicalizing 3D models",
    "text": "Canonicalizing 3D models"
  },
  {
    "objectID": "lecs/w07/lec07.html#canonicalizing-demo",
    "href": "lecs/w07/lec07.html#canonicalizing-demo",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Canonicalizing demo",
    "text": "Canonicalizing demo"
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments",
    "href": "lecs/w07/lec07.html#experiments",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments",
    "text": "Experiments\n\n• Data Efficiency\n• Generalization\n• Learning new programs with a fixed NPI cores"
  },
  {
    "objectID": "lecs/w07/lec07.html#data-efficiency---sorting",
    "href": "lecs/w07/lec07.html#data-efficiency---sorting",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Data Efficiency - Sorting",
    "text": "Data Efficiency - Sorting\n\n\n\n\n\n• Seq2Seq LSTM and NPI used the same number of layers and hidden units.\n• Trained on length up to 20 arrays of single-digit numbers.\n• NPI benefits from mining multiple subprogram examples per sorting instance, and additional parameters of the program memory."
  },
  {
    "objectID": "lecs/w07/lec07.html#generalization---sorting",
    "href": "lecs/w07/lec07.html#generalization---sorting",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Generalization - Sorting",
    "text": "Generalization - Sorting\n\n\n\n\n\n• For each length up to 20, they provided 64 example bubble sort traces, for a total of 1216 examples.\n• Then, they evaluated whether the network can learn to sort arrays beyond length 20"
  },
  {
    "objectID": "lecs/w07/lec07.html#generalization---adding",
    "href": "lecs/w07/lec07.html#generalization---adding",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Generalization - Adding",
    "text": "Generalization - Adding\n\n\n\n\n\n• NPI trained on 32 examples for sequence length up to 20\n• s2s-easy trained on twice as many examples as NPI (purple curve)\n• s2s-stack trained on 16 times more examples than NPI (orange curve)"
  },
  {
    "objectID": "lecs/w07/lec07.html#generalization---adding-1",
    "href": "lecs/w07/lec07.html#generalization---adding-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Generalization - Adding",
    "text": "Generalization - Adding\n\n\n\n\n\n• NPI trained on 32 examples for sequence length up to 20\n• s2s-easy trained on twice as many examples as NPI (purple curve)\n• s2s-stack trained on 16 times more examples than NPI (orange curve)"
  },
  {
    "objectID": "lecs/w07/lec07.html#learning-new-programs-with-a-fixed-npi-core",
    "href": "lecs/w07/lec07.html#learning-new-programs-with-a-fixed-npi-core",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning New Programs with a Fixed NPI Core",
    "text": "Learning New Programs with a Fixed NPI Core\n• Toy example: maximum-finding in an array\n• Simple (not optimal) way: call BUBBLESORT and then take the right-most element of the array. Two new programs:\n• RJMP: Move all pointers to the rightmost position in the array by repeatedly calling RSHIFT program\n• MAX: Call BUBBLESORT and then RJMP\n• Expand program memory by adding 2 slots. Then learn by backpropagation with the NPI core and all other parameters fixed."
  },
  {
    "objectID": "lecs/w07/lec07.html#learning-new-programs-with-a-fixed-npi-core-1",
    "href": "lecs/w07/lec07.html#learning-new-programs-with-a-fixed-npi-core-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning New Programs with a Fixed NPI Core",
    "text": "Learning New Programs with a Fixed NPI Core\n\n\n\n\n\n\nOnly the memory slots of\nthe new program are updated!\nAll other weights are\nfixed!\n\nProtocol:\n• Randomly initialize new program vectors in memory\n• Freeze core and other program vectors\n• Backpropagate gradients to new program vectors"
  },
  {
    "objectID": "lecs/w07/lec07.html#quantitative-results",
    "href": "lecs/w07/lec07.html#quantitative-results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Quantitative Results",
    "text": "Quantitative Results\n\n\n\n\n\n• Numbers are per-sequence % accuracy\n• + Max: indicates performance after addition of MAX program to memory\n• “unseen” uses a test set with disjoint car models from the training set"
  },
  {
    "objectID": "lecs/w07/lec07.html#todays-agenda-1",
    "href": "lecs/w07/lec07.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning programs based on execution traces (NPI - Neural Programmer Interpreters)\n\n• Extending NPI for video-based robot imitation (NTP - Neural Task Programming)\n• Inferring sub-task boundaries (TACO - Temporal Alignment for Control)\n• Learning to search in Task and Motion Planning (TAMP)\n• Generalization through imitation – using hierarchical policies\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Zeqi Li, Angran Li, Zihan Fu"
  },
  {
    "objectID": "lecs/w07/lec07.html#neural-task-programming-learning-to-generalize-across-hierarchical-tasks",
    "href": "lecs/w07/lec07.html#neural-task-programming-learning-to-generalize-across-hierarchical-tasks",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Neural Task Programming: Learning to Generalize Across Hierarchical Tasks",
    "text": "Neural Task Programming: Learning to Generalize Across Hierarchical Tasks\nDanfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, Silvio Savarese\nPresented by Angran Li\nFebruary 8, 2019"
  },
  {
    "objectID": "lecs/w07/lec07.html#how-the-algorithm-works",
    "href": "lecs/w07/lec07.html#how-the-algorithm-works",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "How the Algorithm works?",
    "text": "How the Algorithm works?\n\n\nTask Demonstration: state trajectory, first/third-person video demonstrations, or a list of language instructions.\nTask-Conditional Policy: a neural program.\nUsing callable primitive actions to interact with the environment."
  },
  {
    "objectID": "lecs/w07/lec07.html#how-the-algorithm-works-1",
    "href": "lecs/w07/lec07.html#how-the-algorithm-works-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "How the Algorithm works?",
    "text": "How the Algorithm works?\n\nTop-level program block_stacking is recursively decomposed to bottom-level API move_to and grip."
  },
  {
    "objectID": "lecs/w07/lec07.html#goals",
    "href": "lecs/w07/lec07.html#goals",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Goals",
    "text": "Goals\nLearning to Generalize Across Hierarchical Tasks\n\nGeneralizing the learned policies to new task objectives\n\nTask Length: more objects to transport\nTask Semantics: a different goal\nTask Topology: a different trajectory to the same goal\n\nHierarchical composition of primitive actions\n\nModularization and reusability\nLearn the latent structure in complex tasks, instead of fake dependencies"
  },
  {
    "objectID": "lecs/w07/lec07.html#implementation-neural-task-programming",
    "href": "lecs/w07/lec07.html#implementation-neural-task-programming",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Implementation: Neural Task Programming",
    "text": "Implementation: Neural Task Programming\n\n\nObservation Encoder: observation \\(o_i \\Rightarrow\\) state representation \\(s_i\\)\nTask Spec. Interpreter: \\(\\Rightarrow\\) API arguments \\(a_i\\) or task spec. \\(\\psi_{i+1}\\)\nTask Spec. Encoder: task spec. \\(\\psi_i \\Rightarrow\\) vector space \\(\\phi_i\\)\nCore Network: \\(s_i, P_i, \\phi_i \\Rightarrow P_{i+1}, r_i\\)"
  },
  {
    "objectID": "lecs/w07/lec07.html#implementation-standing-on-the-shoulder-of-npi",
    "href": "lecs/w07/lec07.html#implementation-standing-on-the-shoulder-of-npi",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Implementation: Standing on the shoulder of NPI",
    "text": "Implementation: Standing on the shoulder of NPI\nNeural Task Programming combines the idea of Few-Shot Learning from Demonstration and Neural Programmer-Interpreters.\n\nSimilarities when executing a program:\n\nWhen the EOP probability exceeds a threshold \\(\\alpha\\), control is returned to the caller program;\nWhen the program is not primitive, a sub-program with its arguments is called;\nWhen the program is primitive, a low-level basic action is performed.\n\nTwo similar modules:\n\nDomain-specific task encoders that map an observation to a state representation.\nA key-value memory that stores and retrieves program embeddings."
  },
  {
    "objectID": "lecs/w07/lec07.html#implementation-ntp-vs-npi",
    "href": "lecs/w07/lec07.html#implementation-ntp-vs-npi",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Implementation: NTP vs NPI",
    "text": "Implementation: NTP vs NPI\n\nNPI: one-shot generalization to tasks with longer lengths; can’t generalizing to novel programs without training.\nNTP: generalizes to sub-task permutations (topology) and success conditions (semantics).\nThree main differences of NTP than the original NPI:\n\nNTP can interpret task specifications and perform hierarchical decomposition and thus can be considered as a meta-policy;\nIt uses robot APls as the primitive actions to scale up neural programs for complex tasks;\nIt uses a reactive core network instead of a recurrent network, making the model less history-dependent, enabling feedback control for recovery from failures."
  },
  {
    "objectID": "lecs/w07/lec07.html#model-training",
    "href": "lecs/w07/lec07.html#model-training",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model Training",
    "text": "Model Training\n\nThe model is trained using rich supervision from program execution traces \\(\\{\\xi_t| \\xi_t = (\\Psi_t, p_t, s_t), t = 1 \\ldots T\\}\\).\nThe training objective is to maximize the probability of the correct executions over all the tasks in the dataset \\(D = \\{(\\xi_t, \\xi_{t+1})\\}\\).\nFor each task specification, the ground-truth hierarchical decomposition is provided by the expert policy, which is an agent with hard-coded rules."
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-setup",
    "href": "lecs/w07/lec07.html#experiments-setup",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Setup",
    "text": "Experiments: Setup\n\n\nGeneralization in 3 variations: semantics, topology, and length.\nUsing image-based input without access to ground truth state.\nWorking in real-world tasks combine these variations.\n\n→ Three tasks: Object Sorting, Block Stacking, and Table Clean-up"
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-object-sorting",
    "href": "lecs/w07/lec07.html#experiments-object-sorting",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Object Sorting",
    "text": "Experiments: Object Sorting\n\n\nFlat: non-hierarchical model, directly predicts the primitive APls instead of calling hierarchical programs.\nGRU: Gated Recurrent Unit."
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-block-stacking",
    "href": "lecs/w07/lec07.html#experiments-block-stacking",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Block Stacking",
    "text": "Experiments: Block Stacking\n\n\nNTPVID(E2E): Trained with only visual information.\nNTP(Full State): Trained with ground-truth hierarchical decomposition."
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-table-clean-up",
    "href": "lecs/w07/lec07.html#experiments-table-clean-up",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Table Clean-up",
    "text": "Experiments: Table Clean-up\n\n\nSort plastic bowls and forks into a stack, so they can be steadily carried away.\nTask variations:\n\nTask length: number of bowls and forks varies.\nTask topology: the ordering in which bowls are stacked varies."
  },
  {
    "objectID": "lecs/w07/lec07.html#discussion-future-work",
    "href": "lecs/w07/lec07.html#discussion-future-work",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Discussion & Future Work",
    "text": "Discussion & Future Work\n\nNeural Task Programming:\n\nA meta-learning framework that learns modular and reusable neural programs for hierarchical tasks.\nGeneralizing well towards the variation of task length, semantics, and topology for complex tasks.\n\nFuture work:\n\nImprove the state encoder to extract more task-salient information such as object relationships;\nDevise a richer set of APls such as velocity and torque-based controllers;\nExtend this framework to tackle more complex tasks on real robots."
  },
  {
    "objectID": "lecs/w07/lec07.html#section",
    "href": "lecs/w07/lec07.html#section",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=YWkBRPnGUqA"
  },
  {
    "objectID": "lecs/w07/lec07.html#questions",
    "href": "lecs/w07/lec07.html#questions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "lecs/w07/lec07.html#todays-agenda-2",
    "href": "lecs/w07/lec07.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning programs based on execution traces (NPI - Neural Programmer Interpreters)\n• Extending NPI for video-based robot imitation (NTP - Neural Task Programming)\n\n• Inferring sub-task boundaries (TACO - Temporal Alignment for Control)\n• Learning to search in Task and Motion Planning (TAMP)\n• Generalization through imitation – using hierarchical policies\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Zeqi Li, Angran Li, Zihan Fu"
  },
  {
    "objectID": "lecs/w07/lec07.html#taco-learning-task-decomposition-via-temporal-alignment-for-control",
    "href": "lecs/w07/lec07.html#taco-learning-task-decomposition-via-temporal-alignment-for-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "TACO: Learning Task Decomposition via Temporal Alignment for Control",
    "text": "TACO: Learning Task Decomposition via Temporal Alignment for Control\nKyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, Ingmar Posner"
  },
  {
    "objectID": "lecs/w07/lec07.html#motivation-block-stacking-task",
    "href": "lecs/w07/lec07.html#motivation-block-stacking-task",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation – Block Stacking Task",
    "text": "Motivation – Block Stacking Task\n\n\n\n\n\n\nComplex tasks can often be broken down into simpler sub-tasks\nMost Learning from Demonstration (LfD) algorithms can only learn a single policy for the whole task\nResulting in more complex policies, and also less reusable"
  },
  {
    "objectID": "lecs/w07/lec07.html#modular-lfd",
    "href": "lecs/w07/lec07.html#modular-lfd",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Modular LfD",
    "text": "Modular LfD\n\n\nModelling the task as a composition of sub-tasks\nReusable sub-policies (modules) are learned for each sub-task.\nSub-policies are easier to learn and can be composed in different ways to execute new tasks.\n\nKey approach: provide the learner with additional information about the demonstration"
  },
  {
    "objectID": "lecs/w07/lec07.html#taco-temporal-alignment-for-control",
    "href": "lecs/w07/lec07.html#taco-temporal-alignment-for-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "TACO: Temporal Alignment for Control",
    "text": "TACO: Temporal Alignment for Control\n\nPartly supervised\nDomain agnostic\nDemonstration is augmented by task sketches - a sequence of sub-tasks that occur within the demonstration\n\n\\[\n𝛕 = (b1, b2, . . . , bL),\n\\]\n\nSimultaneously aligns the sketches with the observed demonstrations and learns the required sub-policies"
  },
  {
    "objectID": "lecs/w07/lec07.html#example-block-stacking-task",
    "href": "lecs/w07/lec07.html#example-block-stacking-task",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Example: Block stacking task",
    "text": "Example: Block stacking task"
  },
  {
    "objectID": "lecs/w07/lec07.html#problem",
    "href": "lecs/w07/lec07.html#problem",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Problem",
    "text": "Problem\n\nHow to align task-sketches with the demonstration?\nSolution: Borrow temporal sequence alignment techniques from speech recognition!"
  },
  {
    "objectID": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-1",
    "href": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "TACO: Temporal Alignment for Control",
    "text": "TACO: Temporal Alignment for Control\n\n\n\n𝛕 = (b1, b2, . . . , bL),\nInput sequence ρ with length T\nA path \\(ζ = (ζ_1, ζ_2, ..., ζ_T )\\) is a sequence of sub- tasks of the same length as the input sequence ρ, describing the active sub-task \\(ζ_t\\) at every time-step\n\\(Z_{T,𝛕}\\) is the set of all possible paths of length T for a task sketch 𝛕\nEg. T = 6, 𝛕 = (b1, b2, b3), ζ = (b1, b1, b2, b3, b3, b3)"
  },
  {
    "objectID": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-2",
    "href": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "TACO: Temporal Alignment for Control",
    "text": "TACO: Temporal Alignment for Control\nObjective: Maximise the joint log likelihood of the task sequence and the actions conditioned on the states\n\\[\np(\\tau, \\mathbf{a}_\\rho \\mid s_\\rho) = \\sum_{\\zeta \\in \\mathbb{Z}_{T, \\tau}} p(\\zeta \\mid s_\\rho) \\prod_{t=1}^{T} \\pi_{\\theta_{\\zeta_t}} (a_t \\mid s_t)\n\\]\n\\(p(ζ |s_ρ)\\) is the product of the stop, \\(a_{STOP}\\) , and nonstop, \\(ā_{STOP}\\), probabilities associated with any given path.\nEg. T = 4, \\(s_⍴ = (s_0, s_1, s_2, s_3)\\), 𝛕 = (b1, b2), ζ = (b1, b1, b2, b2)\n\\(p(ζ |sρ) = π_{b1}(non-stop)^* π_{b1}(stop)^* π_{b2}(non-stop)^* π_{b2}(stop)\\)"
  },
  {
    "objectID": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-3",
    "href": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "TACO: Temporal Alignment for Control",
    "text": "TACO: Temporal Alignment for Control\nProblem: Impossible to compute all paths ζ in \\(Z_{T,tau}\\) for long sequence\nSolution: Dynamic Programming\nThe (joint) likelihood of a being at sub-task l at time t can be formulated in terms of forward variables:\n\\[\n\\alpha_t(l) := \\sum_{\\zeta_{1:t} \\in \\mathbb{Z}_t, \\tau_{1:l}} p(\\zeta \\mid s_\\rho) \\prod_{t' = 1}^{t} \\pi_{\\theta_{\\zeta_{t'}}}(a_{t'} \\mid s_{t'})\n\\]"
  },
  {
    "objectID": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-4",
    "href": "lecs/w07/lec07.html#taco-temporal-alignment-for-control-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "TACO: Temporal Alignment for Control",
    "text": "TACO: Temporal Alignment for Control\n\n\n\n\\(\\alpha_1(l) = \\begin{cases}\n\\pi_{\\theta_{b_1}}(a_1|s_1), & \\text{if } l = 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\\)\n\\(\\alpha_t(l) = \\pi_{\\theta_{b_l}}(a_t|s_t) \\left[ \\alpha_{t-1}(l-1) \\pi_{\\theta_{b_{l-1}}}(a_{STOP}|s_t) \\right.\\)\n\\(\\left. + \\alpha_{t-1}(l)(1 - \\pi_{\\theta_{b_l}}(a_{STOP}|s_t)) \\right].\\)\n\n\\(\\alpha_T(L) = p(\\tau, \\mathbf{a}_\\rho|\\mathbf{s}_\\rho).\\)\n\n\n\nTraining: Maximize \\(⍺_T(L)\\) over θ"
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-nav-world",
    "href": "lecs/w07/lec07.html#experiments-nav-world",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Nav-World",
    "text": "Experiments: Nav-World\n\n\nSetup:\n\nThe agent (blue) receives a route as a task sketch.\n𝛕 = (black, green, yellow, red)\nState space: (x, y) distance from each of the destination points\nAction space: \\((v_x, v_y)\\) - the velocity\n\nTraining:\n\nProvided with state-action trajectories ⍴ and the task sketch.\nAt the end of learning, the agent learns four sub-policies\n\n\n\n\nTest:\n\nGiven an unseen task sketch.\nConsidered successful if the agent visits all destinations in the correct order"
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-nav-world-1",
    "href": "lecs/w07/lec07.html#experiments-nav-world-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Nav-World",
    "text": "Experiments: Nav-World\n\n\n\n\n\nSuccess Rate\n\n\n\n\n\n\nAlignment Accuracy"
  },
  {
    "objectID": "lecs/w07/lec07.html#experiments-dial-domain",
    "href": "lecs/w07/lec07.html#experiments-dial-domain",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments: Dial Domain",
    "text": "Experiments: Dial Domain"
  },
  {
    "objectID": "lecs/w07/lec07.html#summary-taco---temporal-alignment-for-control",
    "href": "lecs/w07/lec07.html#summary-taco---temporal-alignment-for-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Summary: TACO - Temporal Alignment for Control",
    "text": "Summary: TACO - Temporal Alignment for Control\n\nModular LfD\nWeak supervision - task sketch\nOptimising the sub-policies over a distribution of possible alignments"
  },
  {
    "objectID": "lecs/w07/lec07.html#future-work-limitation",
    "href": "lecs/w07/lec07.html#future-work-limitation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Future Work & Limitation",
    "text": "Future Work & Limitation\nLimitation:\n\nSub-tasks in the task sketch has to be placed in the correct order\n\n\nFuture work:\n\nTask sketches are dissimilar to natural human communication. Combination of TACO with architectures that can handle natural language.\nHierarchical task decomposition."
  },
  {
    "objectID": "lecs/w07/lec07.html#todays-agenda-3",
    "href": "lecs/w07/lec07.html#todays-agenda-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning programs based on execution traces (NPI - Neural Programmer Interpreters)\n• Extending NPI for video-based robot imitation (NTP - Neural Task Programming)\n• Inferring sub-task boundaries (TACO - Temporal Alignment for Control)\n\n• Learning to search in Task and Motion Planning (TAMP)\n• Generalization through imitation – using hierarchical policies\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Zeqi Li, Angran Li, Zihan Fu"
  },
  {
    "objectID": "lecs/w07/lec07.html#task-and-motion-planning",
    "href": "lecs/w07/lec07.html#task-and-motion-planning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Task and Motion Planning",
    "text": "Task and Motion Planning\n\n\n\n\nGoal: move green box and blue box on the goal surface\nProblem: grey box is obstructing\nTask plan:\n\nmove grey box where it doesn’t obstruct\nmove blue box on goal surface\nmove green box on goal surface"
  },
  {
    "objectID": "lecs/w07/lec07.html#task-and-motion-planning-1",
    "href": "lecs/w07/lec07.html#task-and-motion-planning-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Task and Motion Planning",
    "text": "Task and Motion Planning\n\n\n\nDiscrete action space: 3 objects x 4 operations\nContinuous action space: 5 joint angles on the robot arm x T timesteps\n\nfind-grasp(b, hand)\nplace(b, hand, sur face)\nfind-traj(hand, goal)\ncollides(arm, b, objects)\n\n\\(b \\in \\{b_0, b_1, b_2 \\}\\)"
  },
  {
    "objectID": "lecs/w07/lec07.html#task-and-motion-planning-2",
    "href": "lecs/w07/lec07.html#task-and-motion-planning-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Task and Motion Planning",
    "text": "Task and Motion Planning\n\n\n\nDiscrete action space: M objects x N operations\nContinuous action space: 5 joint angles on the robot arm x T timesteps\n\nfind-grasp(b, hand)\nplace(b, hand, sur face)\nfind-traj(hand, goal)\ncollides(arm, b, objects)\npour(b, b’)\nstir(b)\nshake(b)\n.\n.\n."
  },
  {
    "objectID": "lecs/w07/lec07.html#task-and-motion-planning-3",
    "href": "lecs/w07/lec07.html#task-and-motion-planning-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Task and Motion Planning",
    "text": "Task and Motion Planning\n\n\n\nDiscrete action space: M objects x N operations\nContinuous action space: 5 joint angles on the robot arm x T timesteps\n\nDiscrete + Continuous Optimization\n\nExpanding 1 and 2 requires solving continuous optimization problems with constraints"
  },
  {
    "objectID": "lecs/w07/lec07.html#section-1",
    "href": "lecs/w07/lec07.html#section-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "Solubility experiment\n\nhttps://www.youtube.com/watch?v=NjpZmaKQWls"
  },
  {
    "objectID": "lecs/w07/lec07.html#constrained-motion-planner-to-avoid-spilling",
    "href": "lecs/w07/lec07.html#constrained-motion-planner-to-avoid-spilling",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Constrained Motion Planner to Avoid Spilling",
    "text": "Constrained Motion Planner to Avoid Spilling\n\nhttps://www.youtube.com/watch?v=NjpZmaKQWls"
  },
  {
    "objectID": "lecs/w07/lec07.html#section-2",
    "href": "lecs/w07/lec07.html#section-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "These plans are useful, but unfortunately discrete + continuous optimization is slow\nQ: How can we learn to plan from past experience of having solved similar problems?"
  },
  {
    "objectID": "lecs/w07/lec07.html#learning-to-rank-objects-and-operations-from-past-experience",
    "href": "lecs/w07/lec07.html#learning-to-rank-objects-and-operations-from-past-experience",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning to Rank Objects and Operations from Past Experience",
    "text": "Learning to Rank Objects and Operations from Past Experience"
  },
  {
    "objectID": "lecs/w07/lec07.html#learning-to-rank-objects-and-operations-from-past-experience-1",
    "href": "lecs/w07/lec07.html#learning-to-rank-objects-and-operations-from-past-experience-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning to Rank Objects and Operations from Past Experience",
    "text": "Learning to Rank Objects and Operations from Past Experience"
  },
  {
    "objectID": "lecs/w07/lec07.html#section-3",
    "href": "lecs/w07/lec07.html#section-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=pzzpR4wh_Zk&t=15s"
  },
  {
    "objectID": "lecs/w07/lec07.html#learned-informed-planner-finds-solutions-faster",
    "href": "lecs/w07/lec07.html#learned-informed-planner-finds-solutions-faster",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learned (Informed) Planner Finds Solutions Faster",
    "text": "Learned (Informed) Planner Finds Solutions Faster\n\n*Learning to Search in Task and Motion Planning with Streams, Khodeir et al, Robotics and Automation Letters. 2022"
  },
  {
    "objectID": "lecs/w07/lec07.html#todays-agenda-4",
    "href": "lecs/w07/lec07.html#todays-agenda-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning programs based on execution traces (NPI - Neural Programmer Interpreters)\n• Extending NPI for video-based robot imitation (NTP - Neural Task Programming)\n• Inferring sub-task boundaries (TACO - Temporal Alignment for Control)\n• Learning to search in Task and Motion Planning (TAMP)\n\n• Generalization through imitation – using hierarchical policies\nsource: https://www.youtube.com/watch?v=hlvRmLlYHZ0&t=111s&ab_channel=RoboticsScienceandSystems\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Zeqi Li, Angran Li, Zihan Fu"
  },
  {
    "objectID": "lecs/w11/lec11.html#learning-to-act-from-videos",
    "href": "lecs/w11/lec11.html#learning-to-act-from-videos",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning to act from videos",
    "text": "Learning to act from videos\n\n• The argument: we have billions of videos on the internet, many of which show how humans interact with objects. Let’s try to use these videos to teach robots how to act. A tantalizing possibility for scaling up robot learning.\n• Problem #1: Actions labels are missing!\n• Problem #2: Morphology and mechanics of the human body are different than the robot’s\n• Problem #3: The camera viewpoint during the recording of the video is likely different than the one during deployment.\n\n\nWhat can we realistically learn from videos?\n• Learn representations for pretraining policy backbones\n• Learn video dynamics\n• Extract actions from videos\n• Learn affordances (how humans interact with objects)"
  },
  {
    "objectID": "lecs/w11/lec11.html#todays-agenda",
    "href": "lecs/w11/lec11.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nExtracting actions from videos\n\nPhysics–based human motion estimation & prediction\nHand motion estimation\nMotion retargeting to a robot\n\nLearning video dynamics\n\nVideo predictive models\nAction spaces for learning from videos\n\nLearning object affordances from video"
  },
  {
    "objectID": "lecs/w11/lec11.html#smpl-skinned-multi-person-linear-model",
    "href": "lecs/w11/lec11.html#smpl-skinned-multi-person-linear-model",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "SMPL (Skinned Multi-Person Linear Model)",
    "text": "SMPL (Skinned Multi-Person Linear Model)"
  },
  {
    "objectID": "lecs/w11/lec11.html#section",
    "href": "lecs/w11/lec11.html#section",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=kuBlUyHeV5U"
  },
  {
    "objectID": "lecs/w11/lec11.html#smpl-hands",
    "href": "lecs/w11/lec11.html#smpl-hands",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "SMPL + Hands",
    "text": "SMPL + Hands\n\nhttps://mano.is.tue.mpg.de"
  },
  {
    "objectID": "lecs/w11/lec11.html#biomechanics-models",
    "href": "lecs/w11/lec11.html#biomechanics-models",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Biomechanics models",
    "text": "Biomechanics models\n• SMPL is a kinematic model of the human body\n• For more accurate models we could try to model the muscle structure of the human body\n\n\nhttps://myosuite.readthedocs.io/en/latest/index.html\nhttps://sites.google.com/view/myodex"
  },
  {
    "objectID": "lecs/w11/lec11.html#hand-motion-estimation-in-arvr-headsets",
    "href": "lecs/w11/lec11.html#hand-motion-estimation-in-arvr-headsets",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Hand motion estimation in AR/VR headsets",
    "text": "Hand motion estimation in AR/VR headsets"
  },
  {
    "objectID": "lecs/w11/lec11.html#frame-by-frame-human-pose-detectors",
    "href": "lecs/w11/lec11.html#frame-by-frame-human-pose-detectors",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Frame-by-frame human pose detectors",
    "text": "Frame-by-frame human pose detectors\n\n\nKAMA: 3D Keypoint Aware Body Mesh Articulation. Iqbal, Xie et al. 3DV 2021. https://arxiv.org/abs/2104.13502"
  },
  {
    "objectID": "lecs/w11/lec11.html#motivation",
    "href": "lecs/w11/lec11.html#motivation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation",
    "text": "Motivation\n\nFrame-by-frame human pose estimators exist but are not good enough. We can correct them with physics.\n\n\n\n\n\n\\(\\Rightarrow\\)"
  },
  {
    "objectID": "lecs/w11/lec11.html#system-overview",
    "href": "lecs/w11/lec11.html#system-overview",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "System overview",
    "text": "System overview\n\n\nPhysics-based Human Motion Estimation and Synthesis from Videos.\nXie, Wang, Iqbal, Guo, Fidler, Shkurti. ICCV ’21.\n\n\n\nKevin Xie"
  },
  {
    "objectID": "lecs/w11/lec11.html#physics-constraints-reduce-slip",
    "href": "lecs/w11/lec11.html#physics-constraints-reduce-slip",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Physics constraints reduce slip",
    "text": "Physics constraints reduce slip\n\n\n\nPhysics-based Human Motion Estimation and Synthesis from Videos.\nXie, Wang, Iqbal, Guo, Fidler, Shkurti. ICCV ’21."
  },
  {
    "objectID": "lecs/w11/lec11.html#section-2",
    "href": "lecs/w11/lec11.html#section-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "Physics-based Human Motion Estimation and Synthesis from Videos.\nXie, Wang, Iqbal, Guo, Fidler, Shkurti. ICCV ’21."
  },
  {
    "objectID": "lecs/w11/lec11.html#motion-generation",
    "href": "lecs/w11/lec11.html#motion-generation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motion generation",
    "text": "Motion generation"
  },
  {
    "objectID": "lecs/w11/lec11.html#todays-agenda-1",
    "href": "lecs/w11/lec11.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nExtracting actions from videos\n\nPhysics–based human motion estimation & prediction\nHand motion estimation\nMotion retargeting to a robot\n\nLearning video dynamics\n\nVideo predictive models\nAction spaces for learning from videos\n\nLearning object affordances from video"
  },
  {
    "objectID": "lecs/w11/lec11.html#motion-retargeting-formulating-a-loss-function",
    "href": "lecs/w11/lec11.html#motion-retargeting-formulating-a-loss-function",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motion retargeting: formulating a loss function",
    "text": "Motion retargeting: formulating a loss function\n\n\n\n\nMotion retargeting loss function:\n\nTracking error\nRespecting joint limits\nAvoiding self-collisions\nRespecting dynamics\nSmooth motion\n\nIssue: source and target system have different dimensionality!\nPotential fix: need to decide joint mappings from source to target"
  },
  {
    "objectID": "lecs/w11/lec11.html#motion-retargeting-from-human-to-robot-hands",
    "href": "lecs/w11/lec11.html#motion-retargeting-from-human-to-robot-hands",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motion retargeting from human to robot hands",
    "text": "Motion retargeting from human to robot hands\n  \n\n\n\n\nMotion retargeting loss terms:\n\nTracking error\nRespecting target joint limits\nAvoiding self-collisions in target\nRespecting target dynamics\nSmooth motion for target\n\nIssue: source and target system have different dimensionality!\nPotential fix: need to decide joint mappings from source to target"
  },
  {
    "objectID": "lecs/w11/lec11.html#modeling-human-object-interaction",
    "href": "lecs/w11/lec11.html#modeling-human-object-interaction",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Modeling human-object interaction",
    "text": "Modeling human-object interaction\n\n\nNIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis. Kulkarni et al. 2023"
  },
  {
    "objectID": "lecs/w11/lec11.html#section-4",
    "href": "lecs/w11/lec11.html#section-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "Need to estimate human hand poses + unknown object geometry\n\n\n\nLearning to Imitate Object Interactions from Internet Videos. Patel et al. 2022"
  },
  {
    "objectID": "lecs/w11/lec11.html#section-5",
    "href": "lecs/w11/lec11.html#section-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "A lot of nuance and engineering details that matter in estimating the motions of the human body, the hands, and the geometry of objects they interact with.\n\nOpen questions: Do we have to infer 3D geometry and actions from video? Do we have to do 3D perception?"
  },
  {
    "objectID": "lecs/w11/lec11.html#todays-agenda-2",
    "href": "lecs/w11/lec11.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\nExtracting actions from videos\n\nPhysics–based human motion estimation & prediction\nHand motion estimation\nMotion retargeting to a robot\n\n\n\n\nLearning video dynamics\n\nVideo predictive models\nAction spaces for learning from videos\n\nLearning object affordances from video"
  },
  {
    "objectID": "lecs/w11/lec11.html#large-video-reconstruction-models",
    "href": "lecs/w11/lec11.html#large-video-reconstruction-models",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Large video reconstruction models",
    "text": "Large video reconstruction models\n\n\nIf you learn a good token representation for videos, then you can learn a latent dynamics model over it.\nCosmos model from NVIDIA. https://research.nvidia.com/labs/dir/cosmos-tokenizer/"
  },
  {
    "objectID": "lecs/w11/lec11.html#large-video-reconstruction-models-1",
    "href": "lecs/w11/lec11.html#large-video-reconstruction-models-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Large video reconstruction models",
    "text": "Large video reconstruction models\n\n\n\n\n\nPrevious SOTA\n\n\n\nCosmos\n\n\nCosmos model from NVIDIA. https://research.nvidia.com/labs/dir/cosmos-tokenizer/"
  },
  {
    "objectID": "lecs/w11/lec11.html#large-video-reconstruction-models-2",
    "href": "lecs/w11/lec11.html#large-video-reconstruction-models-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Large video reconstruction models",
    "text": "Large video reconstruction models\n\n\n\n\n\nPrevious SOTA\n\n\n\nCosmos\n\n\nCosmos model from NVIDIA. https://research.nvidia.com/labs/dir/cosmos-tokenizer/"
  },
  {
    "objectID": "lecs/w11/lec11.html#how-can-we-use-video-models-in-robotics",
    "href": "lecs/w11/lec11.html#how-can-we-use-video-models-in-robotics",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "How can we use video models in robotics?",
    "text": "How can we use video models in robotics?\n\n\nVideo prediction models as rewards for RL. Escontrela et al. NeurIPS 2023."
  },
  {
    "objectID": "lecs/w11/lec11.html#another-idea-dynamics-models-with-learned-action-spaces",
    "href": "lecs/w11/lec11.html#another-idea-dynamics-models-with-learned-action-spaces",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Another idea: dynamics models with learned action spaces",
    "text": "Another idea: dynamics models with learned action spaces"
  },
  {
    "objectID": "lecs/w11/lec11.html#need-to-manually-map-robot-actions-to-latent-actions",
    "href": "lecs/w11/lec11.html#need-to-manually-map-robot-actions-to-latent-actions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Need to manually map robot actions to latent actions",
    "text": "Need to manually map robot actions to latent actions"
  },
  {
    "objectID": "lecs/w11/lec11.html#genie",
    "href": "lecs/w11/lec11.html#genie",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Genie",
    "text": "Genie"
  },
  {
    "objectID": "lecs/w05/lec05.html#todays-agenda",
    "href": "lecs/w05/lec05.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nLearning linear rewards from trajectory demonstrations in 2D\nLearning nonlinear rewards from trajectory demonstrations in 2D\n\nGuided cost learning in any D\nUpdating distributions over reward parameters using preference elicitation\nHuman-robot dialog with uncertainty quantification\n\n\n\nAcknowledgments:\nToday’s slides are based on student presentations from 2019 by: Sergio Casas, Sean Liu, Jacky Liao"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-inverse-reinforcement-learning",
    "href": "lecs/w05/lec05.html#maximum-entropy-inverse-reinforcement-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy Inverse Reinforcement Learning",
    "text": "Maximum Entropy Inverse Reinforcement Learning\nZiebart, Maas, Bagnell and Dey\nPresented by Sergio Casas"
  },
  {
    "objectID": "lecs/w05/lec05.html#imitation-learning-approaches",
    "href": "lecs/w05/lec05.html#imitation-learning-approaches",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning approaches",
    "text": "Imitation Learning approaches\n• In Imitation Learning, we want to learn to predict the behavior an expert agent would choose.\n• So far, we have seen two main paradigms to tackle this problem"
  },
  {
    "objectID": "lecs/w05/lec05.html#imitation-learning-approaches-1",
    "href": "lecs/w05/lec05.html#imitation-learning-approaches-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning approaches",
    "text": "Imitation Learning approaches\n• In Imitation Learning, we want to learn to predict the behavior an expert agent would choose.\n• Today, we introduce a third paradigm: Inverse Reinforcement Learning (IRL)"
  },
  {
    "objectID": "lecs/w05/lec05.html#basic-principle",
    "href": "lecs/w05/lec05.html#basic-principle",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Basic Principle",
    "text": "Basic Principle\n\n\n\nIRL reduces the imitation problem to:\n\nRecovering a reward function given a set of demonstrations.\nSolving the MDP using RL to recover the policy, conditioned on our learned reward.\n\nIRL assumes that the reward function provides the most concise and transferable definition of the task"
  },
  {
    "objectID": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004",
    "href": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Background [Ng & Russell 2000, Abbeel & Ng 2004]",
    "text": "Background [Ng & Russell 2000, Abbeel & Ng 2004]\n\nMore formally, we want to find a reward function R* that explains the expert behavior such that:\n\n\\[\n\\mathbb{E} \\left[ \\sum_{t} R^{*}(s_{t}) \\mid \\pi^{*} \\right] \\geq \\mathbb{E} \\left[ \\sum_{t} R^{*}(s_{t}) \\mid \\pi \\right] \\quad \\forall \\pi\n\\]\n\nIRL Challenges:\n\nAssumes we know the expert policy \\(\\pi^*\\), but we only observe sample trajectories\nAssumes optimality of the expert\nAssumes we can enumerate all policies\nReward function ambiguity (eg. R=0 is a solution)"
  },
  {
    "objectID": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004-1",
    "href": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Background [Ng & Russell 2000, Abbeel & Ng 2004]",
    "text": "Background [Ng & Russell 2000, Abbeel & Ng 2004]\n\n\nWe define feature expectations (or feature counts) as:\n\n\\[\n\\mathbf{f}_{\\pi} = \\mathbb{E} \\left[ \\sum_{t} \\mathbf{f}_{s_t} \\mid \\pi \\right]\n\\]\n\nLet the reward be a linear function of the state features:\n\n\\[\nR(s) = \\theta^{\\top} \\mathbf{f}_s\n\\]\n\nTherefore, we can calculate the expected reward of a policy as:\n\n\\[\n\\mathbb{E} \\left[ \\sum_{t} R(s_t) \\mid \\pi \\right] =\n\\mathbb{E} \\left[ \\sum_{t} \\theta^{\\top} \\mathbf{f}_{s_t} \\mid \\pi \\right] =\n\\theta^{\\top} \\mathbb{E} \\left[ \\sum_{t} \\mathbf{f}_{s_t} \\mid \\pi \\right] =\n\\theta^{\\top} \\mathbf{f}_{\\pi}\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004-2",
    "href": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Background [Ng & Russell 2000, Abbeel & Ng 2004]",
    "text": "Background [Ng & Russell 2000, Abbeel & Ng 2004]\n\n\nWe can also define the feature counts of a trajectory \\(\\tau\\) :\n\n\\[\nf_{\\tau} = \\sum_{s_t \\in \\tau} f_{s_t}\n\\]\n\nAnd the expected empirical feature count from \\(m\\) sample trajectories of a policy:\n\n\\[\n\\tilde{f}_{\\pi} = \\frac{1}{m} \\sum_i f_{\\tau_i}\n\\]\n\nFinally, we can obtain an unbiased estimate of the expected reward of a policy as:\n\n\\[\n\\mathbb{E} \\left[ \\sum_t R(s_t) \\mid \\pi \\right] \\approx \\theta^\\top \\tilde{f}_{\\pi}\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004-3",
    "href": "lecs/w05/lec05.html#background-ng-russell-2000-abbeel-ng-2004-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Background [Ng & Russell 2000, Abbeel & Ng 2004]",
    "text": "Background [Ng & Russell 2000, Abbeel & Ng 2004]\n\nTherefore, we can rewrite our inequality as:\n\n\\[\n\\theta^{*\\top} f_{\\pi^*} \\geq \\theta^{*\\top} f_{\\pi}\n\\]\nwhich can in turn be approximated when having a dataset \\(D\\) of expert demonstrated trajectories \\(D\\) as:\n\\[\n\\theta^{*\\top} f_D \\geq \\theta^{*\\top} f_{\\pi} \\quad \\text{where} \\quad f_D = \\tilde{f}_{\\pi^*}\n\\]\n\nBy sampling expert trajectories to compute the feature count estimate, we tackle the challenge of the partial observability of the expert policy."
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]\n\nLet’s recap the IRL Challenges:\n\nAssumes we know the expert policy \\(\\pi^*\\)\nAssumes optimality of the expert\nAssumes we can enumerate all policies\nReward function ambiguity (e.g. R=0 is a solution)"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-principle",
    "href": "lecs/w05/lec05.html#maximum-entropy-principle",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy Principle",
    "text": "Maximum Entropy Principle\n\n\\[\nH(p) = -\\int_x p(x)\\log p(x)dx\n\\]\n\n\n\\[\np(x) = ?\n\\qquad\n\\begin{cases}\n\\underset{p(x)}{\\text{argmax}} \\, \\mathcal{H}(p) \\\\[0.5em]\n\\text{subject to} \\int_a^b p(x)dx = 1\n\\qquad\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-principle-1",
    "href": "lecs/w05/lec05.html#maximum-entropy-principle-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy Principle",
    "text": "Maximum Entropy Principle\n\n\\[\np(x) = ?\n\\qquad\n\\begin{cases}\n\\underset{p(x)}{\\text{argmax}} \\, \\mathcal{H}(p) \\\\[0.5em]\n\\text{subject to} & \\int_x p(x)dx = 1 \\\\[0.5em]\n& \\mathbb{E}_{x \\sim p(x)}[x] = \\frac{1}{|D|} \\sum_{x_i \\in D} x_i = \\hat{\\mu} \\\\[0.5em]\n& \\mathbb{V}_{x \\sim p(x)}[x] = \\frac{1}{|D|} \\sum_{x_i \\in D} (x_i - \\hat{\\mu})^2 = \\hat{\\sigma}^2\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-1",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]\n\\[\np(\\tau|\\theta) = ?\n\\qquad\n\\begin{cases}\n\\underset{p(\\tau|\\theta)}{\\text{argmax}} \\, \\mathcal{H}(p) \\\\[0.5em]\n\\text{subject to} & \\sum_{\\tau} p(\\tau|\\theta) = 1 \\\\[0.5em]\n& \\mathbb{E}_{\\tau \\sim p(\\tau|\\theta)}[\\mathbf{f}_{\\tau}] = \\frac{1}{|D|} \\sum_{\\tau \\in D} \\mathbf{f}_{\\tau}\n\\end{cases}\n\\]\n\nAssumption: Trajectories (states and action sequences) here are discrete"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-2",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]\n\nApplying the principle of maximum entropy breaks the ambiguity of the reward function.\nLeads us to a distribution over behaviors constrained to match feature expectations of the demonstrations while having no preference to any particular path that fits this constraint."
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-3",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]\n\\[\np(\\tau|\\theta) = \\frac{\\exp(\\theta^{\\top}\\mathbf{f}_{\\tau})}{Z(\\theta)}\n\\qquad\n\\begin{cases}\n\\underset{p(\\tau|\\theta)}{\\text{argmax}} \\, \\mathcal{H}(p) \\\\[0.5em]\n\\text{subject to} & \\sum_{\\tau} p(\\tau|\\theta) = 1 \\\\[0.5em]\n& \\mathbb{E}_{\\tau \\sim p(\\tau|\\theta)}[\\mathbf{f}_{\\tau}] = \\frac{1}{|D|} \\sum_{\\tau \\in D} \\mathbf{f}_{\\tau}\n\\end{cases}\n\\]\nLinear Reward Function\n\\(R_{\\theta}(\\tau) = \\theta^{\\top}\\mathbf{f}_{\\tau}\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-4",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]\n\n\\[\n\\left.\n\\begin{aligned}\np(\\tau \\mid \\theta) &= \\frac{\\exp\\!\\bigl(\\theta^\\top \\mathbf{f}_\\tau\\bigr)}{Z(\\theta)} \\\\[8pt]\n\\textbf{Linear Reward Function}\\\\\nR_\\theta(\\tau) &= \\theta^\\top \\mathbf{f}_\\tau\n\\end{aligned}\n\\right\\}\n\\qquad\np(\\tau \\mid \\theta) \\;=\\; p(x_0)\\,\n\\prod_{t=0}^{T-1} p(x_{t+1}\\!\\mid\\!x_t,u_t)\\,\\pi_\\theta(u_t\\!\\mid\\!x_t)\n\\;=\\; \\frac{\\exp\\!\\bigl(R_\\theta(\\tau)\\bigr)}{Z(\\theta)}\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-principle-ziebart-et-al.-2008",
    "href": "lecs/w05/lec05.html#maximum-entropy-principle-ziebart-et-al.-2008",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy Principle [Ziebart et al. 2008]",
    "text": "Maximum Entropy Principle [Ziebart et al. 2008]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-5",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-6",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-6",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-7",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-7",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]"
  },
  {
    "objectID": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-8",
    "href": "lecs/w05/lec05.html#maximum-entropy-irl-ziebart-et-al.-2008-8",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Maximum Entropy IRL [Ziebart et al. 2008]",
    "text": "Maximum Entropy IRL [Ziebart et al. 2008]"
  },
  {
    "objectID": "lecs/w05/lec05.html#state-visitation-distribution",
    "href": "lecs/w05/lec05.html#state-visitation-distribution",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "State visitation distribution",
    "text": "State visitation distribution\n\n\n\nThe exponential growth of paths with the MDPs time horizon makes enumeration-based approaches infeasible.\nThe authors proposed a DP algorithm similar to value iteration to compute the state visitation distribution efficiently."
  },
  {
    "objectID": "lecs/w05/lec05.html#learning-from-demonstrations-ziebart-et-al.-2008",
    "href": "lecs/w05/lec05.html#learning-from-demonstrations-ziebart-et-al.-2008",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning from demonstrations [Ziebart et al. 2008]",
    "text": "Learning from demonstrations [Ziebart et al. 2008]\n\nAs we have seen, maximizing the entropy subject to the feature counts constraint is equivalent to maximize the likelihood of the demonstrated trajectories D with an exponential family as our path distribution: \\[\n\\theta^{*} = \\arg\\max_{\\theta} L(\\theta) = \\arg\\max_{\\theta} \\sum_{\\tilde{\\tau} \\in D} \\log P(\\tilde{\\tau} \\mid \\theta, T)\n\\]\nFor deterministic MDPs, this function is convex and can be optimized using gradient descent:\n\n\\[\n\\nabla L(\\theta) =\n\\underbrace{\\tilde{\\mathbf{f}}}\n- \\sum_{\\tau} P(\\tau \\mid \\theta, T) \\mathbf{f}_{\\tau} =\n\\tilde{\\mathbf{f}} -\n\\underbrace{\\sum_{s_i} \\mu_{s_i}} \\mathbf{f}_{s_i}\n\\]\n\nIn practice we use empirical, sample-based\nexpectations of the expert agent\n\n\nState visitation distribution"
  },
  {
    "objectID": "lecs/w05/lec05.html#maxent-high-level-algorithm",
    "href": "lecs/w05/lec05.html#maxent-high-level-algorithm",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "MaxEnt high-level algorithm",
    "text": "MaxEnt high-level algorithm"
  },
  {
    "objectID": "lecs/w05/lec05.html#application-driver-route-modelling",
    "href": "lecs/w05/lec05.html#application-driver-route-modelling",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Application: Driver Route Modelling",
    "text": "Application: Driver Route Modelling\n\nInterested in predicting driver behavior and route recommendation\nPittsburgh’s road network as an MDP\n\n\\(&gt;\\) 300,000 states or road segments\n\\(&gt;\\) 900,000 actions or transitions at intersections\n\nDestination is represented as an absorbing state with zero-cost. Thus, trips with different destinations will have slightly different MDPs\nAssumption: the reward weights are independent of the goal state. A single reward weight is then learned from many MDPs that only differ in the goal"
  },
  {
    "objectID": "lecs/w05/lec05.html#application-driver-route-modelling-1",
    "href": "lecs/w05/lec05.html#application-driver-route-modelling-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Application: Driver Route Modelling",
    "text": "Application: Driver Route Modelling\n\nDataset\n\nGPS data from &gt;100,000 miles and 3,000 hours of taxi trips\nFit the sparse GPS data to the road network using a particle filter\nSegmented the traces into 13,000 trips using a time-based threshold to determine stops\n\nPath features (low dimensional, 22 counts)\n\nRoad type: from interstate to local road\nSpeed: from high speed to low speed\nLanes: from single-lane to many-lanes\nTransitions: straight, left, right, hard left, hard right"
  },
  {
    "objectID": "lecs/w05/lec05.html#application-driver-route-modelling-2",
    "href": "lecs/w05/lec05.html#application-driver-route-modelling-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Application: Driver Route Modelling",
    "text": "Application: Driver Route Modelling\n\n\nMaximize the probability of demonstrated paths using MaxEnt IRL*\nBaselines:\n\nTime-based: Based on expected time travels. Weights the cost of a unit distance of road to be inversely proportional to the speed of the road.\nMax-margin [Ratliff et al. 2006]: Model capable of predicting new paths, but incapable of density estimation. Directly measures disagreement between the expert and learned policy\nAction-based [Ramachandran et al. 2007, Neu et al. 2007]: The choice of an action is distributed according to the future expected reward of the best policy after taking that action. Suffers from label bias (local distribution of probability mass):\n\n\n\n\n\n\nMaxEnt: paths 1, 2, 3 will have 33% probability\nAction-based: 50% path 3, 25% paths 1 and 2\n\n*applied to a “fixed class of reasonably good paths” instead of the full training set"
  },
  {
    "objectID": "lecs/w05/lec05.html#application-driver-route-modelling-3",
    "href": "lecs/w05/lec05.html#application-driver-route-modelling-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Application: Driver Route Modelling",
    "text": "Application: Driver Route Modelling\n\n\nMatching: Average percentage of distance matching\n90% Match: Percentage of examples with at least 90% matching distance\nLog Prob: Average log probability"
  },
  {
    "objectID": "lecs/w05/lec05.html#application-driver-route-modelling-4",
    "href": "lecs/w05/lec05.html#application-driver-route-modelling-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Application: Driver Route Modelling",
    "text": "Application: Driver Route Modelling\n\nLearned costs:\n\nAdditionally, learned a fixed per edge cost of 1.4 seconds to penalize roads composed of many short paths"
  },
  {
    "objectID": "lecs/w05/lec05.html#application-driver-route-modelling-5",
    "href": "lecs/w05/lec05.html#application-driver-route-modelling-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Application: Driver Route Modelling",
    "text": "Application: Driver Route Modelling\n\nPredicting destination: so far we have only described situations where the driver intended destination is known. We can use Bayes rule to predict destination* given our current model.\n\n\n\n\n\\[\nP(\\text{dest} \\mid \\tilde{\\tau}_{A \\rightarrow B}) \\propto P(\\tilde{\\tau}_{A \\rightarrow B} \\mid \\text{dest}) \\, P(\\text{dest})\n\\]\n\\[\n\\propto \\frac{\\sum_{\\tau_{B \\rightarrow \\text{dest}}} e^{\\theta^{\\top} f_{\\tau}}}{\\sum_{\\tau_{A \\rightarrow \\text{dest}}} e^{\\theta^{\\top} f_{\\tau}}} \\, P(\\text{dest})\n\\]\n\n\n\n\n\n\n\n*posed as a multiclass classification problem over 5 possible destinations"
  },
  {
    "objectID": "lecs/w05/lec05.html#reflections",
    "href": "lecs/w05/lec05.html#reflections",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Reflections",
    "text": "Reflections\n\n\nSolves the reward ambiguity problem by applying the Maximum Entropy theorem, i.e. using path distributions in the exponential family\nSOTA performance and guarantees a t the time for linear reward functions\n\n\n\n\nDerivations assume linear reward function\nMaxEnt IRL requires to know the environment dynamics T (model-given)\nNeed to solve full RL problem at each iteration. Only reasonable for small MDPs, i.e. low-dimensional state-action spaces"
  },
  {
    "objectID": "lecs/w05/lec05.html#todays-agenda-1",
    "href": "lecs/w05/lec05.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nLearning linear rewards from trajectory demonstrations in 2D\nLearning nonlinear rewards from trajectory demonstrations in 2D\nGuided cost learning in any D\nUpdating distributions over reward parameters using preference elicitation\nHuman-robot dialog with uncertainty quantification\n\n\n\nAcknowledgments:\nToday’s slides are based on student presentations from 2019 by: Sergio Casas, Sean Liu, Jacky Liao"
  },
  {
    "objectID": "lecs/w05/lec05.html#large-scale-cost-function-learning-for-path-planning-using-deep-inverse-reinforcement-learning",
    "href": "lecs/w05/lec05.html#large-scale-cost-function-learning-for-path-planning-using-deep-inverse-reinforcement-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Large scale cost function learning for path planning using deep inverse reinforcement learning",
    "text": "Large scale cost function learning for path planning using deep inverse reinforcement learning\nWulfmeier et. al (IJRR 2017)"
  },
  {
    "objectID": "lecs/w05/lec05.html#deep-maximum-entropy-irl-wulfmeier-et-al.-2017",
    "href": "lecs/w05/lec05.html#deep-maximum-entropy-irl-wulfmeier-et-al.-2017",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Deep Maximum Entropy IRL [Wulfmeier et al. 2017]",
    "text": "Deep Maximum Entropy IRL [Wulfmeier et al. 2017]"
  },
  {
    "objectID": "lecs/w05/lec05.html#benchmarking",
    "href": "lecs/w05/lec05.html#benchmarking",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Benchmarking",
    "text": "Benchmarking\n\n\n• Reward function FCN:\n     • Two hidden layers\n     • ReLU activation\n     • 1x1 filter weights\n• Evaluation metric: expected value difference\n• Compared against Linear MaxEnt, GPIRL, NPB-FIRL\n\n\n\n\nObjectWorld"
  },
  {
    "objectID": "lecs/w05/lec05.html#benchmarking-1",
    "href": "lecs/w05/lec05.html#benchmarking-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Benchmarking",
    "text": "Benchmarking\n\n\n\n\n\n\nObjectWorld\n\n\n\n\n\n\n\nBinaryWorld"
  },
  {
    "objectID": "lecs/w05/lec05.html#proposed-network-architectures",
    "href": "lecs/w05/lec05.html#proposed-network-architectures",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Proposed Network Architectures",
    "text": "Proposed Network Architectures"
  },
  {
    "objectID": "lecs/w05/lec05.html#large-scale-demonstration",
    "href": "lecs/w05/lec05.html#large-scale-demonstration",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Large-scale Demonstration",
    "text": "Large-scale Demonstration\n\n\n\n13 drivers\n\\(&gt;\\) 25,000 trajectories 12m-15m long\n\n\n\nGoal: reward map given features\n\nSteepness\nCorner cases (underpasses, stairs)\n\n\n\n\n\n\nMobile research platform: a modified GEM golf cart"
  },
  {
    "objectID": "lecs/w05/lec05.html#network-input-data",
    "href": "lecs/w05/lec05.html#network-input-data",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Network Input Data",
    "text": "Network Input Data"
  },
  {
    "objectID": "lecs/w05/lec05.html#evaluation",
    "href": "lecs/w05/lec05.html#evaluation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Evaluation",
    "text": "Evaluation\n\nNo absolute ground truth\nCompared against manual cost functions\nMetrics:\n\nNLL – negative log-likelihood\nMHD – Hausdorff distance\nFNR – False negative rate\nFPR – False positive rate"
  },
  {
    "objectID": "lecs/w05/lec05.html#robustness-to-systematic-noise",
    "href": "lecs/w05/lec05.html#robustness-to-systematic-noise",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Robustness to Systematic Noise",
    "text": "Robustness to Systematic Noise"
  },
  {
    "objectID": "lecs/w05/lec05.html#pretraining",
    "href": "lecs/w05/lec05.html#pretraining",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Pretraining",
    "text": "Pretraining"
  },
  {
    "objectID": "lecs/w05/lec05.html#limitations",
    "href": "lecs/w05/lec05.html#limitations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Limitations",
    "text": "Limitations\n\nDoes not address velocity profiles\nDoes not consider temporal consistency between consecutive cost maps\n\nPossibly introduce RNNs or temporal convolutions"
  },
  {
    "objectID": "lecs/w05/lec05.html#todays-agenda-2",
    "href": "lecs/w05/lec05.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning linear rewards from trajectory demonstrations in 2D\n• Learning nonlinear rewards from trajectory demonstrations in 2D\n\n• Guided cost learning in any D\n• Updating distributions over reward parameters using preference elicitation\n• Human-robot dialog with uncertainty quantification\n\n\nAcknowledgments:\nToday’s slides are based on student presentations from 2019 by: Sergio Casas, Sean Liu, Jacky Liao"
  },
  {
    "objectID": "lecs/w05/lec05.html#guided-cost-learning-finn-levine-abbeel-et-al.-2016",
    "href": "lecs/w05/lec05.html#guided-cost-learning-finn-levine-abbeel-et-al.-2016",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Guided Cost Learning [Finn, Levine, Abbeel et al. 2016]",
    "text": "Guided Cost Learning [Finn, Levine, Abbeel et al. 2016]\n\n\n\\[\np(\\tau|\\theta) = \\frac{\\exp(-c_{\\theta}(\\tau))}{Z(\\theta)}\n\\]\nNonlinear Reward Function\nLearned Features\n\n\n\n\\(p(\\tau|\\theta) = p(x_0) \\prod_{t=0}^{T-1} \\underbrace{p(x_{t+1}|x_t, u_t)} \\pi_{\\theta}(u_t|x_t) = \\frac{\\exp(-c_{\\theta}(\\tau))}{Z(\\theta)}\\)\n\nTrue and stochastic dynamics (unknown)\n\n Log-likelihood of observed dataset D of trajectories\n\\[\nL(\\theta) = \\frac{1}{|D|} \\sum_{\\tau \\in D} \\log p(\\tau|\\theta) = \\frac{1}{|D|} \\sum_{\\tau \\in D} -c_{\\theta}(\\tau) - \\log Z(\\theta)\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#approximating-the-gradient-of-the-log-likelihood",
    "href": "lecs/w05/lec05.html#approximating-the-gradient-of-the-log-likelihood",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Approximating the gradient of the log-likelihood",
    "text": "Approximating the gradient of the log-likelihood\n\n\n\\[\np(\\tau|\\theta) = \\frac{\\exp(-c_{\\theta}(\\tau))}{Z(\\theta)}\n\\]\nNonlinear Reward Function\nLearned Features\n\n\n\n\\[\n\\nabla_{\\theta} L(\\theta) = -\\frac{1}{|D|} \\sum_{\\tau \\in D} \\nabla_{\\theta} c_{\\theta}(\\tau) + \\underbrace{\\sum_{\\tau} p(\\tau \\mid \\theta) \\nabla_{\\theta} c_{\\theta}(\\tau)}\n\\]\n\nHow do you approximate this expectation?\n\nIdea #1: sample from \\(p(\\tau | \\theta)\\)\n(can you do this)"
  },
  {
    "objectID": "lecs/w05/lec05.html#approximating-the-gradient-of-the-log-likelihood-1",
    "href": "lecs/w05/lec05.html#approximating-the-gradient-of-the-log-likelihood-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Approximating the gradient of the log-likelihood",
    "text": "Approximating the gradient of the log-likelihood\n\n\n\\[\np(\\tau|\\theta) = \\frac{\\exp(-c_{\\theta}(\\tau))}{Z(\\theta)}\n\\]\nNonlinear Reward Function\nLearned Features\n\n\n\n\\[\n\\nabla_{\\theta} L(\\theta) = -\\frac{1}{|D|} \\sum_{\\tau \\in D} \\nabla_{\\theta} c_{\\theta}(\\tau) + \\underbrace{\\sum_{\\tau} p(\\tau \\mid \\theta) \\nabla_{\\theta} c_{\\theta}(\\tau)}\n\\]\n\nHow do you approximate this expectation?\nIdea #1: sample from \\(p(\\tau | \\theta)\\)\n(don’t know the dynamics)\n\nIdea #2: sample from an easier distribution \\(q(\\tau | \\theta)\\)\nthat approximates \\(p(\\tau | \\theta)\\)\nImportance Sampling\nsee Relative Entropy Inverse RL by Boularias, Kober, Peters"
  },
  {
    "objectID": "lecs/w05/lec05.html#importance-sampling",
    "href": "lecs/w05/lec05.html#importance-sampling",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nHow to estimate properties/statistics of one distribution (p) given samples from another distribution (q)\n\n\n\n\n\\[\n\\begin{align}\n\\mathbb{E}_{x \\sim p(x)}[f(x)] &= \\int f(x)p(x)\\,dx \\\\\n&= \\int \\frac{q(x)}{q(x)} f(x)p(x)\\,dx \\\\\n&= \\int f(x)p(x)\\frac{q(x)}{q(x)}\\,dx \\\\\n&= \\mathbb{E}_{x\\sim q(x)}\\left[ f(x)\\frac{p(x)}{q(x)}\\right] \\\\\n&= \\mathbb{E}_{x\\sim q(x)}[f(x)w(x)]\n\\end{align}\n\\]\n\n\nWeights = likelihood ratio, i.e. how to reweigh samples to obtain statistics of p from samples of q"
  },
  {
    "objectID": "lecs/w05/lec05.html#importance-sampling-pitfalls-and-drawbacks",
    "href": "lecs/w05/lec05.html#importance-sampling-pitfalls-and-drawbacks",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Importance Sampling: Pitfalls and Drawbacks",
    "text": "Importance Sampling: Pitfalls and Drawbacks\nWhat can go wrong?\n\n\n\n\n\\[\n\\begin{align}\n\\mathbb{E}_{x \\sim p(x)}[f(x)] &= \\int f(x)p(x)\\,dx \\\\\n&= \\int \\frac{q(x)}{q(x)} f(x)p(x)\\,dx \\\\\n&= \\int f(x)p(x)\\frac{q(x)}{q(x)}\\,dx \\\\\n&= \\mathbb{E}_{x\\sim q(x)}\\left[ f(x)\\frac{p(x)}{q(x)}\\right] \\\\\n&= \\mathbb{E}_{x\\sim q(x)}[f(x)w(x)]\n\\end{align}\n\\]\n\nProblem #1:\nIf q(x) = 0 but f(x)p(x) &gt; 0\nfor x in non-measure-zero\nset then there is estimation bias\n\n\nProblem #2:\nWeights measure mismatch between q(x) and p(x). If mismatch is large then some weights will dominate. If x lives in high dimensions a single weight may dominate\n\n\n\nProblem #3:\nVariance of estimator is high if (q – fp)(x) is high\n\n\n\nFor more info see:\n#1, #3: Monte Carlo theory, methods, and examples, Art Owen, chapter 9\n#2: Bayesian reasoning and machine learning, David Barber, chapter 27.6 on importance sampling"
  },
  {
    "objectID": "lecs/w05/lec05.html#importance-sampling-1",
    "href": "lecs/w05/lec05.html#importance-sampling-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nWhat is the best approximating distribution q?\n\n\n\n\n\\[\n\\begin{align}\n\\mathbb{E}_{x \\sim p(x)}[f(x)] &= \\int f(x)p(x)\\,dx \\\\\n&= \\int \\frac{q(x)}{q(x)} f(x)p(x)\\,dx \\\\\n&= \\int f(x)p(x)\\frac{q(x)}{q(x)}\\,dx \\\\\n&= \\mathbb{E}_{x\\sim q(x)}\\left[ f(x)\\frac{p(x)}{q(x)}\\right] \\\\\n&= \\mathbb{E}_{x\\sim q(x)}[f(x)w(x)]\n\\end{align}\n\\]\n\nBest approximation \\(q(x) \\propto f(x)p(x)\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#importance-sampling-2",
    "href": "lecs/w05/lec05.html#importance-sampling-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nHow does this connect back to partition function estimation?\n\n\n\n\n\\[\n\\begin{align}\nZ(\\theta) &= \\sum_{\\tau} \\exp(-c_{\\theta}(\\tau)) \\\\\n&= \\sum_{\\tau} \\exp(-c_{\\theta}(\\tau)) \\\\\n&= \\sum_{\\tau} \\frac{q(\\tau|\\theta)}{q(\\tau|\\theta)} \\exp(-c_{\\theta}(\\tau)) \\\\\n&= \\mathbb{E}_{\\tau \\sim q(\\tau|\\theta)} \\left[ \\frac{\\exp(-c_{\\theta}(\\tau))}{q(\\tau|\\theta)} \\right]\n\\end{align}\n\\]\n\n\nBest approximation \\(q(\\tau | \\theta) \\propto exp(-c_{\\theta} (\\tau))\\)\n\n\n\n\nCost function estimate changes at each gradient step\nTherefore the best approximating distribution should change as well"
  },
  {
    "objectID": "lecs/w05/lec05.html#approximating-the-gradient-of-the-log-likelihood-2",
    "href": "lecs/w05/lec05.html#approximating-the-gradient-of-the-log-likelihood-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Approximating the gradient of the log-likelihood",
    "text": "Approximating the gradient of the log-likelihood\n\n\n\\[\np(\\tau|\\theta) = \\frac{\\exp(-c_{\\theta}(\\tau))}{Z(\\theta)}\n\\]\nNonlinear Reward Function\nLearned Features\n\n\n\n\\[\n\\nabla_{\\theta} L(\\theta) = -\\frac{1}{|D|} \\sum_{\\tau \\in D} \\nabla_{\\theta} c_{\\theta}(\\tau) + \\underbrace{\\sum_{\\tau} p(\\tau \\mid \\theta) \\nabla_{\\theta} c_{\\theta}(\\tau)}\n\\]\n\nHow do you approximate this expectation?\nIdea #1: sample from \\(p(\\tau | \\theta)\\)\n(don’t know the dynamics)\nIdea #2: sample from an easier distribution \\(q(\\tau | \\theta)\\)\nthat approximates \\(p(\\tau | \\theta)\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#guided-cost-learning",
    "href": "lecs/w05/lec05.html#guided-cost-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Guided Cost Learning",
    "text": "Guided Cost Learning\nHow do you select q?\n\nHow do you adapt it as the cost c changes?"
  },
  {
    "objectID": "lecs/w05/lec05.html#guided-cost-learning-the-punchline",
    "href": "lecs/w05/lec05.html#guided-cost-learning-the-punchline",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Guided Cost Learning: the punchline",
    "text": "Guided Cost Learning: the punchline\nHow do you select q?\n\nHow do you adapt it as the cost c changes?\n\nGiven a fixed cost function c, the distribution of trajectories that Guided Policy Search computes is close to \\(\\frac{\\exp(-c(\\tau))}{Z}\\)\ni.e. it is good for importance sampling of the partition function Z"
  },
  {
    "objectID": "lecs/w05/lec05.html#recall-finite-horizon-lqr",
    "href": "lecs/w05/lec05.html#recall-finite-horizon-lqr",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recall: Finite-Horizon LQR",
    "text": "Recall: Finite-Horizon LQR\n\\(P_0 = Q\\)\n// \\(n\\) is the # of steps left\nfor \\(n = 1 \\dots N\\)\n\\(K_n = -(R + B^T P_{n-1} B)^{-1} B^T P_{n-1} A\\)\n\\(P_n = Q + K_n^T R K_n + (A + B K_n)^T P_{n-1} (A + B K_n)\\)\nOptimal control for time \\(t = N - n\\) is \\(u_t = K_t x_t\\) with cost-to-go \\(J_t(x) = x^T P_t x\\)\nwhere the states are predicted forward in time according to linear dynamics."
  },
  {
    "objectID": "lecs/w05/lec05.html#recall-lqg-lqr-with-stochastic-dynamics",
    "href": "lecs/w05/lec05.html#recall-lqg-lqr-with-stochastic-dynamics",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recall: LQG = LQR with stochastic dynamics",
    "text": "Recall: LQG = LQR with stochastic dynamics\nAssume \\(x_{t+1} = Ax_t + Bu_t + w_t\\) and \\(c(x_t, u_t) = x_t^T Q x_t + u_t^T R u_t\\)\n\n\\(\\uparrow\\)\nzero mean Gaussian\n\n\nThen the form of the optimal policy is the same as in LQR: \\(u_t = K \\hat{x}_t\\) \\(\\color{red} \\leftarrow\\) estimate of the state\nNo need to change the algorithm, as long as you observe the state at each step (closed-loop policy)\n\nLinear Quadratic Gaussian LQG"
  },
  {
    "objectID": "lecs/w05/lec05.html#deterministic-nonlinear-cost-deterministic-nonlinear-dynamics",
    "href": "lecs/w05/lec05.html#deterministic-nonlinear-cost-deterministic-nonlinear-dynamics",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Deterministic Nonlinear Cost & Deterministic Nonlinear Dynamics",
    "text": "Deterministic Nonlinear Cost & Deterministic Nonlinear Dynamics\n\\[\n\\begin{align}\nu_0^*, \\ldots, u_{N-1}^* &= \\underset{{u_0, \\ldots, u_N}}{\\arg\\min} \\sum_{t=0}^{N} c(x_t, u_t) \\\\\n& \\text{s.t.} \\\\\n& x_1 = f(x_0, u_0) \\\\\n& x_2 = f(x_1, u_1) \\\\\n& ... \\\\\n& x_N = f(x_{N-1}, u_{N-1})\n\\end{align}\n\\]\n\nArbitrary differentiable functions \\(c\\), \\(f\\)\n\n\niLQR: iteratively approximate solution by solving linearized versions of the problem via LQR"
  },
  {
    "objectID": "lecs/w05/lec05.html#deterministic-nonlinear-cost-stochastic-nonlinear-dynamics",
    "href": "lecs/w05/lec05.html#deterministic-nonlinear-cost-stochastic-nonlinear-dynamics",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Deterministic Nonlinear Cost & Stochastic Nonlinear Dynamics",
    "text": "Deterministic Nonlinear Cost & Stochastic Nonlinear Dynamics\n\\[\n\\begin{align}\nu_0^*, \\ldots, u_{N-1}^* &= \\underset{{u_0, \\ldots, u_N}}{\\arg\\min} \\sum_{t=0}^{N} c(x_t, u_t) \\\\\n& \\text{s.t.} \\\\\n& x_1 = f(x_0, u_0) + w_0 \\\\\n& x_2 = f(x_1, u_1) + w_1 \\\\\n& ... \\\\\n& x_N = f(x_{N-1}, u_{N-1}) + w_{N-1}\n\\end{align}\n\\]\n\nArbitrary differentiable functions \\(c\\), \\(f\\)\n\\(w_t \\sim N(0, W_t)\\)\n\n\niLQG: iteratively approximate solution by solving linearized versions of the problem via LQG"
  },
  {
    "objectID": "lecs/w05/lec05.html#recall-from-guided-policy-search",
    "href": "lecs/w05/lec05.html#recall-from-guided-policy-search",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recall from Guided Policy Search",
    "text": "Recall from Guided Policy Search\n\n\\(\\underset{q(\\tau)}{\\text{argmin}} \\quad \\mathbb{E}_{\\tau \\sim q(\\tau)}[c(\\tau)]\\)\n\\(\\text{subject to} \\quad q(x_{t+1}|x_t, u_t) = \\mathcal{N}(x_{t+1}; f_{xt}x_t + f_{ut}u_t, F_t) \\qquad \\color{red}\\Leftarrow \\quad \\text{Learn linear Gaussian dynamics}\\)\n\\(\\qquad \\qquad \\text{KL}(q(\\tau) || q_{\\text{prev}}(\\tau)) \\leq \\epsilon\\)\n\n\n\\(q_{\\text{gps}}(\\tau) = \\underset{q(\\tau)}{\\text{argmin}} \\quad \\mathbb{E}_{\\tau \\sim q(\\tau)}[c(\\tau)] - \\mathcal{H}(q(\\tau))\\)\n\\(\\qquad \\qquad \\text{subject to} \\quad q(x_{t+1}|x_t, u_t) = \\mathcal{N}(x_{t+1}; f_{xt}x_t + f_{ut}u_t, F_t)\\)\n\n\n\n\\(q_{\\text{gps}}(\\tau) = q(x_0) \\prod_{t=0}^{T-1} q(x_{t+1}|x_t, u_t)q(u_t|x_t)\\)\n\n\\(\\qquad \\color{red} \\uparrow \\qquad \\quad \\uparrow\\)\nLinear Gaussian\ndynamics and controller\n\n\n\n\n\nRun controller on the robot\nCollect trajectories\n\n\n\\(q_{prev} = q_{gps}\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#recall-from-guided-policy-search-1",
    "href": "lecs/w05/lec05.html#recall-from-guided-policy-search-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recall from Guided Policy Search",
    "text": "Recall from Guided Policy Search\n\n\\(\\arg\\min_{q(\\tau)} \\; \\mathbb{E}_{\\tau \\sim q(\\tau)} [c(\\tau)]\\)\n\\(\\begin{align} \\text{subject to} & \\quad q(x_{t+1} \\mid x_t, u_t) = \\mathcal{N}(x_{t+1}; f_{xt}x_t + f_{ut}u_t, F_t) \\color{red} \\qquad \\Leftarrow \\text{Learn Linear Gaussian dynamics} \\\\\n& \\text{KL}(q(\\tau) \\parallel q_{\\text{prev}}(\\tau)) \\leq \\epsilon \\end{align}\\)\n\n\n\nGiven a fixed cost function c, the linear\nGaussian controllers that GPS computes\ninduce a distribution of trajectories close to\n\\(\\frac{\\exp(-c(\\tau))}{Z}\\)\ni.e. good for importance sampling of the partition function Z"
  },
  {
    "objectID": "lecs/w05/lec05.html#guided-cost-learning-rough-sketch",
    "href": "lecs/w05/lec05.html#guided-cost-learning-rough-sketch",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Guided Cost Learning [rough sketch]",
    "text": "Guided Cost Learning [rough sketch]\nCollect demonstration trajectories D\nInitialize cost parameters \\(\\theta_0\\)\n\n\n\n\nDo forward optimization using Guided Policy Search for cost function \\(c_{\\theta_t} (\\tau)\\)\nand compute linear Gaussian distribution of trajectories \\(q_{gps} (\\tau)\\)\n\\(\\nabla_{\\theta} L(\\theta) = -\\frac{1}{|D|} \\sum_{\\tau \\in D} \\nabla_{\\theta} c_{\\theta}(\\tau) + \\underbrace{\\sum_{\\tau} p(\\tau \\mid \\theta) \\nabla_{\\theta} c_{\\theta}(\\tau)}\\)\n\nImportance sample trajectories from \\(q_{gps} (\\tau)\\)\n\n\n\\(\\theta_{t+1} = \\theta_t + \\gamma \\nabla_{\\theta} L(\\theta_t)\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#regularization-of-learned-cost-functions",
    "href": "lecs/w05/lec05.html#regularization-of-learned-cost-functions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Regularization of learned cost functions",
    "text": "Regularization of learned cost functions\n\\[\ng_{\\text{lcr}}(\\tau) = \\sum_{x_t \\in \\tau} \\left[ \\left(c_{\\theta}(x_{t+1}) - c_{\\theta}(x_t)\\right) - \\left(c_{\\theta}(x_t) - c_{\\theta}(x_{t-1})\\right) \\right]^2\n\\]\n\n\\[\ng_{\\text{mono}}(\\tau) = \\sum_{x_t \\in \\tau} \\left[ \\max\\left(0, c_{\\theta}(x_t) - c_{\\theta}(x_{t-1}) - 1\\right) \\right]^2\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#section-1",
    "href": "lecs/w05/lec05.html#section-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "Source: https://www.youtube.com/watch?v=hXxaepw0zAw&ab_channel=RAIL"
  },
  {
    "objectID": "lecs/w05/lec05.html#todays-agenda-3",
    "href": "lecs/w05/lec05.html#todays-agenda-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning linear rewards from trajectory demonstrations in 2D\n• Learning nonlinear rewards from trajectory demonstrations in 2D\n• Guided cost learning in any D\n\n• Updating distributions over reward parameters using preference elicitation\n• Human-robot dialog with uncertainty quantification\n\n\nAcknowledgments:\nToday’s slides are based on student presentations from 2019 by: Sergio Casas, Sean Liu, Jacky Liao"
  },
  {
    "objectID": "lecs/w05/lec05.html#active-preference-based-learning-of-reward-functions",
    "href": "lecs/w05/lec05.html#active-preference-based-learning-of-reward-functions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Active Preference-Based Learning of Reward Functions",
    "text": "Active Preference-Based Learning of Reward Functions\nBy: Dorsa Sadigh, Anca D. Dragan, Shankar Sastry, and Sanjit A. Seshia"
  },
  {
    "objectID": "lecs/w05/lec05.html#section-2",
    "href": "lecs/w05/lec05.html#section-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "Source: https://rss2017.lids.mit.edu/static/slides/04.mp4"
  },
  {
    "objectID": "lecs/w05/lec05.html#preference-based-learning",
    "href": "lecs/w05/lec05.html#preference-based-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Preference Based Learning",
    "text": "Preference Based Learning\n\n\nLearn rewards from expert preference\n\nHave an estimate of reward function\nPick two candidate trajectories\nAsk the human which trajectory is preferred\nUse preference as feedback to update reward function\n\n• Preference based learning is active\n• Rewards updated directly\n  • No inner RL loop\n  • No probability estimation required"
  },
  {
    "objectID": "lecs/w05/lec05.html#problem-statement-autonomous-driving",
    "href": "lecs/w05/lec05.html#problem-statement-autonomous-driving",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Problem Statement: Autonomous Driving",
    "text": "Problem Statement: Autonomous Driving\n\n\n\n2 vehicles on the road:\n\nOur orange vehicle denoted 𝐻\nOther white vehicle/robot denoted 𝑅\n\nStates: \\((x_H, x_R)\\)\nInputs: \\((u_H, u_R)\\)\nDynamics: \\(x^{t+1} = f_{HR}(x^t, u_H, u_R)\\)\nFinite Trajectories: \\(\\xi = \\{(x^0, u^0_H, u^0_R), \\ldots, (x^N, u^N_H, u^N_R)\\}\\)\nFeasible Trajectories: \\(\\xi \\in \\Xi\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#reward-function",
    "href": "lecs/w05/lec05.html#reward-function",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Reward Function",
    "text": "Reward Function\n\nReward at a state:\n\n\\[\nr_H(x^t, u_H^t, u_R^t) = w^T \\phi(x^t, u_H^t, u_R^t)\n\\]\n\nReward over a finite trajectory: \\[\n\\begin{align}\nR_H(\\xi) &= R_H(x_0, \\mathbf{u}_H, \\mathbf{u}_R) = \\sum_{t=0}^{N} r_H(x^t, u_H^t, u_R^t) \\\\\n&= w^T \\Phi(\\xi)\n\\end{align}\n\\]"
  },
  {
    "objectID": "lecs/w05/lec05.html#preference",
    "href": "lecs/w05/lec05.html#preference",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Preference",
    "text": "Preference\n\n\n\nGiven 2 trajectories \\(\\xi_A\\) and \\(\\xi_B\\)\nPreference variable 𝐼\n\n\\[\nI =\n\\begin{cases}\n+1, & \\text{if } \\xi_A \\text{ is preferred} \\\\\n-1, & \\text{if } \\xi_B \\text{ is preferred}\n\\end{cases} \\\\\n\\]\n\\(\\xi_A \\text{ or } \\xi_B \\rightarrow I\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#weight-update",
    "href": "lecs/w05/lec05.html#weight-update",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Weight Update",
    "text": "Weight Update\n\n\n\nAssume probabilistic model: weights come from a distribution\n\nPreference is noisy:\n\n\\[\nP(I|w) =\n\\begin{cases}\n\\frac{\\exp(R_H(\\xi_A))}{\\exp(R_H(\\xi_A)) + \\exp(R_H(\\xi_B))}, & \\text{if } I = +1 \\\\\n\\frac{\\exp(R_H(\\xi_B))}{\\exp(R_H(\\xi_A)) + \\exp(R_H(\\xi_B))}, & \\text{if } I = -1\n\\end{cases}\n\\]\n\nSome simplification:\n\n\\(\\varphi = \\Phi(\\xi_A) - \\Phi(\\xi_B) \\qquad \\quad f_{\\varphi}(w) = P(I|w) = \\frac{1}{1 + \\exp(-I w^T \\varphi)}\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#generate-trajectories",
    "href": "lecs/w05/lec05.html#generate-trajectories",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Generate Trajectories",
    "text": "Generate Trajectories\n\n\n\nTwo feasible trajectories: \\(𝜉_𝐴\\),\\(𝜉_𝐵\\)\nWant each update to give most information\nMaximize minimum volume removed with a query:\n\n\\(\\underset{{\\xi_A, \\xi_B}}{\\max} \\min\\left( \\mathbb{E}_W[1 - f_{\\phi}(w)], \\; \\mathbb{E}_W[1 - f_{-\\phi}(w)] \\right)\\)\n\nA binary query corresponds to selecting sides of hyperplane \\(𝒘^𝑻 𝜑=0\\)\nResponse increases probability of weights on one side of hyperplane and decreases the other side."
  },
  {
    "objectID": "lecs/w05/lec05.html#algorithm-summary",
    "href": "lecs/w05/lec05.html#algorithm-summary",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm Summary",
    "text": "Algorithm Summary"
  },
  {
    "objectID": "lecs/w05/lec05.html#algorithm-summary-1",
    "href": "lecs/w05/lec05.html#algorithm-summary-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm Summary",
    "text": "Algorithm Summary"
  },
  {
    "objectID": "lecs/w05/lec05.html#algorithm-summary-2",
    "href": "lecs/w05/lec05.html#algorithm-summary-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm Summary",
    "text": "Algorithm Summary"
  },
  {
    "objectID": "lecs/w05/lec05.html#algorithm-summary-3",
    "href": "lecs/w05/lec05.html#algorithm-summary-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm Summary",
    "text": "Algorithm Summary"
  },
  {
    "objectID": "lecs/w05/lec05.html#algorithm-summary-4",
    "href": "lecs/w05/lec05.html#algorithm-summary-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm Summary",
    "text": "Algorithm Summary"
  },
  {
    "objectID": "lecs/w05/lec05.html#results",
    "href": "lecs/w05/lec05.html#results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results",
    "text": "Results\n\nWeights begin with uniform probability\nConvergence after 200 iterations"
  },
  {
    "objectID": "lecs/w05/lec05.html#results-1",
    "href": "lecs/w05/lec05.html#results-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results",
    "text": "Results\n\nRate of convergence, active synthesis is faster!\nBlue curve: generated feasible trajectories not optimized for weight updates\nBlack curve: non active trajectories, equivalent to expert dataset\nLighter colours: training on non feasible trajectories"
  },
  {
    "objectID": "lecs/w05/lec05.html#results-2",
    "href": "lecs/w05/lec05.html#results-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results",
    "text": "Results\n\n\n\nPerturbation of weights\n\nLearned weights: \\(𝒘^∗\\)\nSlightly perturbed weights: \\(𝒘^1\\)\nLargely perturbed weights: \\(𝒘^𝟐\\)\n\nUsers prefer \\(𝒘^∗\\)"
  },
  {
    "objectID": "lecs/w05/lec05.html#check-out-their-library-for-preference-learning",
    "href": "lecs/w05/lec05.html#check-out-their-library-for-preference-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Check out their library for preference learning",
    "text": "Check out their library for preference learning\n\nhttps://github.com/Stanford-ILIAD/APReL"
  },
  {
    "objectID": "lecs/w05/lec05.html#todays-agenda-4",
    "href": "lecs/w05/lec05.html#todays-agenda-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Learning linear rewards from trajectory demonstrations in 2D\n• Learning nonlinear rewards from trajectory demonstrations in 2D\n• Guided cost learning in any D\n• Updating distributions over reward parameters using preference elicitation\n\n• Human-robot dialog with uncertainty quantification\n\n\nAcknowledgments:\nToday’s slides are based on student presentations from 2019 by: Sergio Casas, Sean Liu, Jacky Liao"
  },
  {
    "objectID": "lecs/w05/lec05.html#section-4",
    "href": "lecs/w05/lec05.html#section-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=xCXx09gfhx4&t=3s"
  },
  {
    "objectID": "lecs/w03/lec03.html#todays-agenda",
    "href": "lecs/w03/lec03.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Reinforcement Learning Terminology\n• Distribution Shift in Offline RL\n• Offline RL with Policy Constraints\n• Offline RL with Conservative Q-Estimates\n\n\n\n\n\nAcknowledgments\nToday’s slides borrow very heavily from: Sergey Levine & Aviral Kumar (CSC285 and NeurIPS Offline RL workshop), Joelle Pineau (DLSS’16)"
  },
  {
    "objectID": "lecs/w03/lec03.html#rl-terminology",
    "href": "lecs/w03/lec03.html#rl-terminology",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "RL Terminology",
    "text": "RL Terminology\n• Episodic vs Non-episodic\n• Tabular vs Function Approximation\n• Exploration vs Exploitation\n• Model-based vs Model-free\n• Policy Optimization vs Value Function Estimation\n• On-policy vs Off-Policy\n• Batch (Offline) vs Online"
  },
  {
    "objectID": "lecs/w03/lec03.html#episodic-vs-non-episodic-rl-methods",
    "href": "lecs/w03/lec03.html#episodic-vs-non-episodic-rl-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Episodic vs Non-episodic RL methods",
    "text": "Episodic vs Non-episodic RL methods\nEpisodic: optimize expected reward-to-go for finite time horizon\n\\[\nV_T^{\\pi}(s_0) = \\mathbb{E}_{a_t \\sim \\pi_t(a|s_t), s_{t+1} \\sim p(s_{t+1}|s_t, a_t)} \\left[ \\sum_{t=0}^{T} r(s_t, a_t) \\right]\n\\]\nOften need to have a reset mechanism to bring back the system to state \\(s_0\\)\n\n\nNon-episodic: optimize expected discounted reward-to-go for infinite time horizon, i.e. a task may go on forever, no resets\n\\[\nV^{\\pi}(s_0) = \\mathbb{E}_{a_t \\sim \\pi(a|s_t), s_{t+1} \\sim p(s_{t+1}|s_t, a_t)} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right] \\qquad \\gamma \\in (0, 1)\n\\]"
  },
  {
    "objectID": "lecs/w03/lec03.html#tabular-vs-function-approximation-methods",
    "href": "lecs/w03/lec03.html#tabular-vs-function-approximation-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Tabular vs Function Approximation Methods",
    "text": "Tabular vs Function Approximation Methods\nTabular: discretize states and actions, represent them as a grid, and compute a policy or value function directly on states and actions (typically can enumerate small number of states and actions)\n\n\nFunction approximation: discrete or continuous states and actions, associate a feature representation \\(\\phi(s, a)\\) to each state action pair and compute a policy or value function in terms of features (typically useful for continuous or very large, but discrete, state-action spaces)"
  },
  {
    "objectID": "lecs/w03/lec03.html#exploration-vs-exploitation-in-rl",
    "href": "lecs/w03/lec03.html#exploration-vs-exploitation-in-rl",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Exploration vs Exploitation in RL",
    "text": "Exploration vs Exploitation in RL\nExploitation: act by using current estimates of dynamics and value function to improve task performance in the short term\n\n\nExploration: act to improve current estimates of dynamics and value function to improve task performance in the long-term, even if it hurts short-term performance"
  },
  {
    "objectID": "lecs/w03/lec03.html#model-based-vs-model-free-rl-methods",
    "href": "lecs/w03/lec03.html#model-based-vs-model-free-rl-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model-based vs Model-free RL Methods",
    "text": "Model-based vs Model-free RL Methods\nModel-based: estimate predictive models of the instantaneous reward as well as the dynamics model, and act by making value function predictions based on these models and optimizing the estimated value function. I.e. plan with “imagined” state transition and reward data.\nModel-free: do not try to estimate models of reward or dynamics, but interact with the environment to optimize policy. Plan with real state transition and reward data.\nMixed: only trust your dynamics and reward model (imagined data) for a few steps in the near future, and then use real data."
  },
  {
    "objectID": "lecs/w03/lec03.html#policy-optimization-vs-value-function-estimation",
    "href": "lecs/w03/lec03.html#policy-optimization-vs-value-function-estimation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Policy Optimization vs Value Function Estimation",
    "text": "Policy Optimization vs Value Function Estimation\n\nCredit: John Schulman"
  },
  {
    "objectID": "lecs/w03/lec03.html#on-policy-vs-off-policy-methods",
    "href": "lecs/w03/lec03.html#on-policy-vs-off-policy-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "On-policy vs Off-policy Methods",
    "text": "On-policy vs Off-policy Methods\n\n\n\n\n\nOn-policy RL methods: improve the policy that acts on the environment using data collected from that same policy\n\n\n\n\n\n\nOff-policy RL methods: improve the policy that acts on the environment using data collected from any policy"
  },
  {
    "objectID": "lecs/w03/lec03.html#batch-offline-vs-online-methods",
    "href": "lecs/w03/lec03.html#batch-offline-vs-online-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Batch (Offline) vs Online Methods",
    "text": "Batch (Offline) vs Online Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnline RL methods: Can collect data over multiple rounds. Data distribution changes over time.\n\nBatch/Offline RL methods: Can collect data only once from any policy. Data distribution is stationary."
  },
  {
    "objectID": "lecs/w03/lec03.html#batch-offline-vs-online-methods-1",
    "href": "lecs/w03/lec03.html#batch-offline-vs-online-methods-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Batch (Offline) vs Online Methods",
    "text": "Batch (Offline) vs Online Methods"
  },
  {
    "objectID": "lecs/w03/lec03.html#section-1",
    "href": "lecs/w03/lec03.html#section-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\(\\mathbf{s} \\in \\mathcal{S}\\) – discrete or continuous state\n\\(\\mathbf{a} \\in \\mathcal{A}\\) – discrete or continuous action\n\\(\\tau = \\{s_0, a_0, s_1, a_1, \\ldots, s_T, a_T\\}\\) - trajectory\n\\(\\underbrace{\\pi(s_0, a_0, \\ldots, s_T, a_T)}_{\\pi(\\tau)} = p(s_1) \\prod_{t=0}^{T} \\pi(a_t | s_t) p(s_{t+1} | s_t, a_t)\\)\n\\(d_t^{\\pi}(s_t)\\) – state marginal of \\(\\pi(\\tau)\\) at \\(t\\)\n\\(d^{\\pi}(s) = \\frac{1}{1-\\gamma} \\sum_{t=0}^{T} \\gamma^t d_t^{\\pi}(s_t) \\quad \\text{– \"visitation frequency\"}\\)\n\n\\(Q^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim p(s_{t+1} | s_t, a_t), a_{t+1} \\sim \\pi(a_{t+1} | s_{t+1})} \\left[Q^{\\pi}(s_{t+1}, a_{t+1})\\right]\\)\n\\(V^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim \\pi(a_t | s_t)} \\left[Q^{\\pi}(s_t, a_t)\\right]\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#on-policy-actor-critic-with-function-approximation",
    "href": "lecs/w03/lec03.html#on-policy-actor-critic-with-function-approximation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "On-policy actor-critic with function approximation",
    "text": "On-policy actor-critic with function approximation\n\n\n\n\n\n\n\nupdate \\(Q_\\phi\\) to decrease \\(E_{s \\sim d^{\\pi_\\theta}(s), a \\sim \\pi_\\theta(a|s)} \\left[\\left(Q_\\phi(s,a) - (r(s,a) + \\gamma E_{\\pi_\\theta}[Q_\\phi(s',a')])\\right)^2\\right]\\)\n\n\n\nupdate \\(\\pi_\\theta\\) to increase \\(E_{s \\sim d^{\\pi_\\theta}(s), a \\sim \\pi_\\theta(a|s)} [Q_\\phi(s,a)]\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#off-policy-actor-critic-with-function-approximation",
    "href": "lecs/w03/lec03.html#off-policy-actor-critic-with-function-approximation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Off-policy actor-critic with function approximation",
    "text": "Off-policy actor-critic with function approximation\n\n\n\n\n\n\nupdate \\(Q_\\phi\\) to decrease \\(E_{s \\sim d^{\\pi_\\beta}(s), a \\sim \\pi_\\beta(a|s)} \\left[\\left(Q_\\phi(s,a) - (r(s,a) + \\gamma E_{\\pi_\\theta}[Q_\\phi(s',a')])\\right)^2\\right]\\)\n\n\n\nupdate \\(\\pi_\\theta\\) to increase \\(E_{s \\sim d^{\\pi_\\beta}(s), a \\sim \\pi_\\theta(a|s)} [Q_\\phi(s,a)]\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#q-learningfitted-q-iteration-with-function-approximation-off-policy",
    "href": "lecs/w03/lec03.html#q-learningfitted-q-iteration-with-function-approximation-off-policy",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Q-Learning/Fitted Q-Iteration with function approximation (off-policy)",
    "text": "Q-Learning/Fitted Q-Iteration with function approximation (off-policy)\n\n\n\n\n\n\n\n\n\n\nupdate \\(Q_\\phi\\) to decrease \\(E_{s \\sim d^{\\pi_\\beta}(s), a \\sim \\pi_\\beta(a|s)} \\left[\\left(Q_\\phi(s,a) - (r(s,a) + \\gamma E_{\\pi_\\theta}[Q_\\phi(s',a')])\\right)^2\\right]\\)\n\n\n\nupdate \\(\\pi_\\theta\\) to increase \\(E_{s \\sim d^{\\pi_\\beta}(s), a \\sim \\pi_\\theta(a|s)} [Q_\\phi(s,a)]\\)\n\nchoose \\(\\pi\\) according to: \\(\\pi(a_t | s_t) = \\begin{cases} 1 & \\text{if } a_t = \\arg\\max_{a_t} Q_\\phi(s_t, a_t) \\\\ 0 & \\text{otherwise} \\end{cases}\\)"
  },
  {
    "objectID": "lecs/w03/lec03.html#policy-gradients-on-policy",
    "href": "lecs/w03/lec03.html#policy-gradients-on-policy",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Policy gradients (on-policy)",
    "text": "Policy gradients (on-policy)\nRL objective: \\(\\max_{\\pi} \\sum_{t=0}^{T} E_{s_t \\sim d^{\\pi}(s), a_t \\sim \\pi(a|s)} [\\gamma^t r(s_t, \\mathbf{a}_t)]\\)\n\\(\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\uparrow\\) exactly the same thing!\n\\[J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} \\left[\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\gamma^t r(s_{t,i}, a_{t,i})\\]\n\\[\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} \\left[\\nabla_\\theta \\log \\pi_\\theta(\\tau) \\sum_{t=0}^{T} \\gamma^t r(s_t, \\mathbf{a}_t)\\right] \\text{ simple algebraic derivation}\\]\n\\[= E_{\\tau \\sim \\pi_\\theta(\\tau)} \\left[\\left(\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right) \\left(\\sum_{t=0}^{T} \\gamma^t r(s_t, \\mathbf{a}_t)\\right)\\right] \\text{ from definition of } \\tau\\]"
  },
  {
    "objectID": "lecs/w03/lec03.html#policy-gradients-on-policy-1",
    "href": "lecs/w03/lec03.html#policy-gradients-on-policy-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Policy gradients (on-policy)",
    "text": "Policy gradients (on-policy)\n\n\n\nRL objective: \\(\\max_{\\pi} \\sum_{t=0}^{T} E_{s_t \\sim d^{\\pi}(s), a_t \\sim \\pi(a|s)} [\\gamma^t r(s_t, \\mathbf{a}_t)]\\)\n\\(\\qquad \\qquad \\qquad \\qquad \\qquad \\nearrow\\) exactly the same thing!\n\\(J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} \\left[\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)\\right] \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\gamma^t r(s_{t,i}, a_{t,i})\\)\n\\(\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} \\left[\\nabla_\\theta \\log \\pi_\\theta(\\tau) \\sum_{t=0}^{T} \\gamma^t r(s_t, \\mathbf{a}_t)\\right] \\text{ simple algebraic derivation}\\)\n(REINFORCE gradient estimator)"
  },
  {
    "objectID": "lecs/w03/lec03.html#todays-agenda-1",
    "href": "lecs/w03/lec03.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Reinforcement Learning Terminology\n• Distribution Shift in Offline RL\n• Offline RL with Policy Constraints\n• Offline RL with Conservative Q-Estimates\n\n\n\n\n\nAcknowledgments\nToday’s slides borrow very heavily from: Sergey Levine & Aviral Kumar (CSC285 and NeurIPS Offline RL workshop), Joelle Pineau (DLSS’16)"
  },
  {
    "objectID": "lecs/w03/lec03.html#qt-opt-roughly-continuous-action-q-learning",
    "href": "lecs/w03/lec03.html#qt-opt-roughly-continuous-action-q-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "QT-Opt (roughly: continuous-action Q-Learning)",
    "text": "QT-Opt (roughly: continuous-action Q-Learning)"
  },
  {
    "objectID": "lecs/w03/lec03.html#qt-opt-roughly-continuous-action-q-learning-1",
    "href": "lecs/w03/lec03.html#qt-opt-roughly-continuous-action-q-learning-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "QT-Opt (roughly: continuous-action Q-Learning)",
    "text": "QT-Opt (roughly: continuous-action Q-Learning)"
  },
  {
    "objectID": "lecs/w03/lec03.html#todays-agenda-2",
    "href": "lecs/w03/lec03.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Reinforcement Learning Terminology\n• Distribution Shift in Offline RL\n• Offline RL with Policy Constraints\n• Offline RL with Conservative Q-Estimates\n\n\n\n\n\nAcknowledgments\nToday’s slides borrow very heavily from: Sergey Levine & Aviral Kumar (CSC285 and NeurIPS Offline RL workshop), Joelle Pineau (DLSS’16)"
  },
  {
    "objectID": "lecs/w03/lec03.html#addressing-distribution-shift-via-pessimism",
    "href": "lecs/w03/lec03.html#addressing-distribution-shift-via-pessimism",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Addressing Distribution Shift via Pessimism",
    "text": "Addressing Distribution Shift via Pessimism"
  },
  {
    "objectID": "lecs/w03/lec03.html#different-types-of-policy-constraint-methods",
    "href": "lecs/w03/lec03.html#different-types-of-policy-constraint-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Different Types of Policy Constraint Methods",
    "text": "Different Types of Policy Constraint Methods"
  },
  {
    "objectID": "lecs/w03/lec03.html#note-kl-divergence-is-not-symmetric",
    "href": "lecs/w03/lec03.html#note-kl-divergence-is-not-symmetric",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Note: KL divergence is not symmetric",
    "text": "Note: KL divergence is not symmetric"
  },
  {
    "objectID": "lecs/w03/lec03.html#how-should-we-evaluate-offline-rl-methods",
    "href": "lecs/w03/lec03.html#how-should-we-evaluate-offline-rl-methods",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "How should we evaluate offline RL methods",
    "text": "How should we evaluate offline RL methods"
  },
  {
    "objectID": "lecs/w03/lec03.html#evaluating-offline-rl-d4rl",
    "href": "lecs/w03/lec03.html#evaluating-offline-rl-d4rl",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Evaluating Offline RL – D4RL",
    "text": "Evaluating Offline RL – D4RL"
  },
  {
    "objectID": "lecs/w03/lec03.html#evaluating-offline-rl-d4rl-1",
    "href": "lecs/w03/lec03.html#evaluating-offline-rl-d4rl-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Evaluating Offline RL – D4RL",
    "text": "Evaluating Offline RL – D4RL"
  },
  {
    "objectID": "lecs/w03/lec03.html#standardized-benchmark-for-offline-rl",
    "href": "lecs/w03/lec03.html#standardized-benchmark-for-offline-rl",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Standardized Benchmark for Offline RL",
    "text": "Standardized Benchmark for Offline RL"
  },
  {
    "objectID": "lecs/w03/lec03.html#value-function-regularization-for-offline-rl",
    "href": "lecs/w03/lec03.html#value-function-regularization-for-offline-rl",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Value Function Regularization for Offline RL",
    "text": "Value Function Regularization for Offline RL"
  },
  {
    "objectID": "lecs/w03/lec03.html#learning-lower-bounded-q-values",
    "href": "lecs/w03/lec03.html#learning-lower-bounded-q-values",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning Lower-Bounded Q-values",
    "text": "Learning Lower-Bounded Q-values"
  },
  {
    "objectID": "lecs/w03/lec03.html#a-tighter-lower-bound",
    "href": "lecs/w03/lec03.html#a-tighter-lower-bound",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "A Tighter Lower Bound",
    "text": "A Tighter Lower Bound"
  },
  {
    "objectID": "lecs/w03/lec03.html#practical-cql-algorithm",
    "href": "lecs/w03/lec03.html#practical-cql-algorithm",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Practical CQL Algorithm",
    "text": "Practical CQL Algorithm"
  },
  {
    "objectID": "lecs/w03/lec03.html#the-need-for-safe-exploration-in-rl-for-robotics",
    "href": "lecs/w03/lec03.html#the-need-for-safe-exploration-in-rl-for-robotics",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The need for safe exploration in RL for robotics",
    "text": "The need for safe exploration in RL for robotics\n   \n\nWhen applying RL to robotics we need to guarantee that the algorithm will not visit unsafe states very often during learning."
  },
  {
    "objectID": "lecs/w03/lec03.html#the-need-for-safe-exploration-in-rl-for-robotics-1",
    "href": "lecs/w03/lec03.html#the-need-for-safe-exploration-in-rl-for-robotics-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The need for safe exploration in RL for robotics",
    "text": "The need for safe exploration in RL for robotics\n   \nWhen applying RL to robotics we need to guarantee that the algorithm will not visit unsafe states very often during learning."
  },
  {
    "objectID": "lecs/w03/lec03.html#the-need-for-safe-exploration-in-rl-for-robotics-2",
    "href": "lecs/w03/lec03.html#the-need-for-safe-exploration-in-rl-for-robotics-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The need for safe exploration in RL for robotics",
    "text": "The need for safe exploration in RL for robotics\n   \nWhen applying RL to robotics we need to guarantee that the algorithm will not visit unsafe states very often during learning.\n\nOur proposed method:\nThe learned policy should be safe at each iteration, not just when optimization has converged"
  },
  {
    "objectID": "lecs/w03/lec03.html#our-solution-constrained-safety-critics-csc",
    "href": "lecs/w03/lec03.html#our-solution-constrained-safety-critics-csc",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Our solution: Constrained Safety Critics (CSC)",
    "text": "Our solution: Constrained Safety Critics (CSC)\n\n\n\\(\\text{maximize}_{\\theta} \\mathcal{V}_{\\text{task}}^{\\pi_\\theta}(s_0)\\)\nsubject to \\(\\boxed{\\mathcal{V}_{\\text{accident}}^{\\pi_\\theta}(s_0) &lt; \\epsilon} \\qquad\\)\n\\(\\text{KL}(\\pi_\\theta || \\pi_{\\text{prev}}) &lt; \\delta \\qquad\\)\n\n\nProbability of an accident should be upper bounded\n\nProblem: estimation errors for value function \\(\\mathcal{V}_{\\text{accident}}^{\\pi_\\theta}(s_0)\\) can make the constraint falsely confident\n\n\n\n\nFix: use the CQL update rule to guarantee that the probability of an accident is overestimated\n\n\\(\\min_Q \\alpha \\mathbb{E}_{s \\sim \\mathcal{D}} \\left[\\log \\sum_a \\exp(Q(s,a)) - \\mathbb{E}_{a \\sim \\hat{\\pi}_\\beta(a|s)} [Q(s,a)]\\right] + \\frac{1}{2} \\mathbb{E}_{s,a,s' \\sim \\mathcal{D}} \\left[\\left(Q - \\hat{B}^{\\pi} \\hat{Q}^k\\right)^2\\right] \\Rightarrow \\mathcal{V}_{\\text{accident}}^{\\pi_\\theta}(s_0) \\leq \\hat{\\mathcal{V}}_{\\text{accident}}^{\\pi_\\theta}(s_0) &lt; \\epsilon\\)\n\n\n\nConservative Safety Critics for Exploration, https://arxiv.org/abs/2010.14497, Bharadhwaj, Kumar, Rhinehart, Levine, Shkurti, Garg"
  },
  {
    "objectID": "lecs/w03/lec03.html#results-fewer-accidents",
    "href": "lecs/w03/lec03.html#results-fewer-accidents",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results: fewer accidents",
    "text": "Results: fewer accidents\n\nConstrained Policy Optimization, https://arxiv.org/abs/1705.10528, Achiam, Held, Tamam, Abbeel"
  },
  {
    "objectID": "lecs/w03/lec03.html#results-task-value-vs-safety",
    "href": "lecs/w03/lec03.html#results-task-value-vs-safety",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results: task value vs safety",
    "text": "Results: task value vs safety"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTue & Thu\n1:45 pm - 3:00 pm\nReuben-Cooke Building 130\n\n\nSection 1\nMon\n1:45 pm - 3:00 pm\nPerkins LINK 087 (Classroom 3)\n\n\nSection 2\nMon\n3:30 pm - 4:45 pm\nOld Chemistry 003\n\n\nSection 3\nMon\n5:15 pm - 6:30 pm\nSocial Sciences 311"
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to…\n\nanalyze real-world data to answer questions about multivariable relationships.\nfit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\ncommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "course-syllabus.html#community",
    "href": "course-syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nDuke Community Standard\nAs a student in this course, you have agreed to uphold the Duke Community Standard as well as the practices specific to this course.\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website at sta210-s22.github.io/website.\nI will regularly send course announcements via email and Sakai, make sure to check one or the other of these regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course forum Conversations. There is a chance another student has already asked a similar question, so please check the other posts in Conversations before adding a new question. If you know the answer to a question posted in the discussion forum, I encourage you to respond!\nEmails should be reserved for questions not appropriate for the public forum. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nWhile there is no official textbook for the course, we will be assigning readings from the following textbooks.\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler"
  },
  {
    "objectID": "course-syllabus.html#lectures-and-labs",
    "href": "course-syllabus.html#lectures-and-labs",
    "title": "Syllabus",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded. In addition to application exercises will be periodic activities help build a learning community. These will be short, fun activities that will help everyone in the class connect throughout the semester.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. More information on loaner laptops can be found here."
  },
  {
    "objectID": "course-syllabus.html#teams",
    "href": "course-syllabus.html#teams",
    "title": "Syllabus",
    "section": "Teams",
    "text": "Teams\nYou will be assigned to a team at the beginning of the semester. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of the labs and project and you will be asked to evaluate your team members throughout the semester. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark.\nYou are expected to make use of the provided GitHub repository as their central collaborative platform. Commits to this repository will be used as a metric (one of several) of each team member’s relative contribution for each project."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nAssessment for the course is comprised of six components: application exercises, homework assignments, labs, exams, projects, and teamwork.\n\nApplication exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice apply the statistical concepts and code introduced in the readings and lectures. These AEs are due within three days of the corresponding lecture period. Specifically, AEs from Tuesday lectures are due Friday by 11:59 pm ET, and AEs from Thursday lectures are due Sunday by 11:59 pm ET.\nBecause these AEs are for practice, they will be graded based on completion, i.e., a good-faith effort has been made in attempting all parts. Successful on-time completion of at least 80% of AEs will result in full credit for AEs in the final course grade.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation. Most lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team’s GitHub repository on the course’s GitHub organization as the central platform for collaboration. Commits to this repository will be used as a metric of each team member’s relative contribution for each lab, and there will be periodic peer evaluation on the team collaboration. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted for grading in Gradescope.\nThe lowest lab grade will be dropped at the end of the semester.\n\n\nHomework\nIn homework, you will apply what you’ve learned during lecture and lab to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you’re learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be three, take-home, open-note exams. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on the conceptual understanding of the content, and they may also include small analysis and computational tasks. The content of the exam will be related to the content in the prepare, practice, and perform assignments. More detail about the exams will be given during the semester.\n\n\nProject\nThe purpose of the project is to apply what you’ve learned throughout the semester to analyze an interesting, data-driven research question. The project will be completed with your lab teams, and each team will present their work in video and in writing during the final exam period. More information about the project will be provided during the semester."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2.33% x 6)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n3%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60"
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TAs and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TAs, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework and lab.The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon’t procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually you won’t know where to begin asking questions. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours, and let me help you identify a good (re)starting point."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic integrity\nTL;DR: Don’t cheat!\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard:\nStudents affirm their commitment to uphold the values of the Duke University community by signing a pledge that states:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;\nI will act if the Standard is compromised\n\nRegardless of course delivery format, it is your responsibility to understand and follow Duke policies regarding academic integrity, including doing one’s own work, following proper citation of sources, and adhering to guidance around group work projects. Ignoring these requirements is a violation of the Duke Community Standard. If you have any questions about how to follow these requirements, please contact Jeanna McCullers (jeanna.mccullers@duke.edu), Director of the Office of Student Conduct.\n\n\nCollaboration policy\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\n\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 3 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThere is no late work accepted for application exercises, since these are designed to help you prepare for labs and homework.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email Dr. Çetinkaya-Rundel and our head TA Rick Presman before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let Dr. Çetinkaya-Rundel know if you need help contacting your academic dean.\n\n\nRegrade request policy\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Trinity attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Lab time is dedicated to working on your lab assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a lab session and you’re feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health at 919-681-9355. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class.\n\n\nPolicy on video recording course content\nAll lectures will be recorded and available on Panopto, so students should not need to create their own recordings of lectures. If you feel that you need record the lectures yourself, you must get permission from me ahead of time and these recordings should be used for personal study only, no for distribution. The full policy on recording of lectures falls under the Duke University Policy on Intellectual Property Rights, available at provost.duke.edu/sites/default/files/FHB_App_P.pdf. Unauthorized distribution is a cause for disciplinary action by the Judicial Board."
  },
  {
    "objectID": "course-syllabus.html#learning-during-a-pandemic",
    "href": "course-syllabus.html#learning-during-a-pandemic",
    "title": "Syllabus",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis.\n\nNote: If you’ve read this far in the syllabus, email me a picture of your pet if you have one or your favourite meme!"
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJanuary 5: Classes begin (Monday meeting schedule)\nJanuary 6: Regular class meeting schedule begins\nJanuary 17: Martin Luther King, Jr. Day holiday, no classes are held\nJanuary 19: Drop/add ends\nMarch 7-11: Spring recess, no classes are held\nMarch 23: Last day to withdraw with W\nApril 20: Classes end\nApril 21-24: Reading period\nApril 25-30: Final exams\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "exams/exam-2.html",
    "href": "exams/exam-2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 25 at 9 am ET and must be completed by Mon, Feb 28 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-2- repo to complete Part 2 of your exam. Add your answers to the exam-2.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 28 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-2.html#overview",
    "href": "exams/exam-2.html#overview",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 25 at 9 am ET and must be completed by Mon, Feb 28 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-2- repo to complete Part 2 of your exam. Add your answers to the exam-2.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 28 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-2.html#academic-integrity",
    "href": "exams/exam-2.html#academic-integrity",
    "title": "Exam 2",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-2.html#rules-notes",
    "href": "exams/exam-2.html#rules-notes",
    "title": "Exam 2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-2.html#submission",
    "href": "exams/exam-2.html#submission",
    "title": "Exam 2",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-1.html#overview",
    "href": "exams/exam-1.html#overview",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-1.html#academic-integrity",
    "href": "exams/exam-1.html#academic-integrity",
    "title": "Exam 1",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-1.html#rules-notes",
    "href": "exams/exam-1.html#rules-notes",
    "title": "Exam 1",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-1.html#submission",
    "href": "exams/exam-1.html#submission",
    "title": "Exam 1",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "course-schedule.html",
    "href": "course-schedule.html",
    "title": "Fall 2024 - CSC 2626: Imitation Learning for Robotics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nNotebooks\nAssignments\nProject\n\n\n\n\n1\nMon, Sep 9\nImitation learning vs supervised learning\n📖\n🖥️\n📋\n✍️\n\n\n\n\n\nMitigating compunding errors via dataset aggregation\n📖\n\n📋\n\n\n\n\n\n\nMitigating compounding errors by choosing loss functions\n📖\n\n📋\n\n\n\n\n\n\nImitation via multi-modal generative models\n📖\n\n📋\n\n\n\n\n\n\nTraining instabilities of behavioral cloning\n📖\n\n📋\n\n\n\n\n\n\nTeleoperation interfaces for manipulation (optional)\n📖\n\n📋\n\n\n\n\n\n\nQuerying experts only when necessary (optional)\n📖\n\n📋\n\n\n\n\n\n\nImitation for visual navigation and autonomous driving (optional)\n📖\n\n📋\n\n\n\n\n2\nMon, Sep 16\nIntro to optimal control\n📖\n🖥️\n📋\n\n\n\n\n\n\nIntro to model-based reinforcement learning\n📖\n\n📋\n\n\n\n\n\n\nIntro to model-free reinforcement learning\n📖\n\n📋\n\n\n\n\n\n\nMonotonic improvement of the value function (optional)\n📖\n\n📋\n\n\n\n\n\n\nLearning dynamics well only where it matters for the value function (optional)\n📖\n\n📋\n📂\n\n\n\n\n\nParallelizing MPC on the GPU (optional)\n📖\n\n📋\n\n\n\n\n3\nMon, Sep 23\nOffline / batch reinforcement learning\n📖\n🖥️\n📋\n✍️\n\n\n\n\n\nTransitioning from offline to online RL\n📖\n\n📋\n\n\n\n\n4\nMon, Sep 30\nImitation learning combined with RL and planning\n📖\n🖥️\n📋\n\n\n\n\n\n\nMaking cost-to-go queries to experts\n📖\n\n📋\n\n\n\n\n\n\nExpert iteration\n📖\n\n📋\n\n\n\n\n\n\nImitation can improve search and exploration strategies\n📖\n\n📋\n\n\n\n\n\n\nLearning from experts that have privileged information\n📖\n\n📋\n📂\n✍️\n\n\n\n\nDynamic movement primitives\n📖\n\n📋\n📂\n✍️\n\n\n5\nMon, Oct 7\nInverse reinforcement learning\n📖\n🖥️\n📋\n\n\n\n\n\n\nInferring rewards from preferences\n📖\n\n📋\n\n\n\n\n\n\nTask specification and human-robot dialog\n📖\n\n📋\n\n\n\n\n\n\nValue alignment\n📖\n\n📋\n\n\n\n\n6\nMon, Oct 14\nThanksgiving Monday\n\n\n\n\n📂\n\n\n\n\nNo in-person lecture or office hours on Monday\n\n\n\n\n\n\n\n\n\nTA office hours are on\n\n\n\n\n\n\n\n7\nMon, Oct 21\nImitation as program induction\n📖\n🖥️\n📋\n\n\n\n\n\n\nModular decomposition of demonstrations into skills\n📖\n\n📋\n\n\n\n\n\n\n(Hierarchical) imitation of multi-goal tasks\n📖\n\n📋\n\n\n\n\n\n\nInferring grammars and planning domains\n📖\n\n📋\n\n\n\n\n8\nMon, Oct 28\nFall Reading Week\n\n\n\n\n\n\n\n\n\nNo lecture\n\n\n\n\n\n\n\n\n\nNo office hours\n\n\n\n\n\n\n\n9\nMon, Nov 4\nAdversarial imitation learning\n📖\n🖥️\n📋\n\n\n\n\n10\nMon, Nov 11\nShared autonomy\n📖\n🖥️\n📋\n\n\n\n\n\n\nImitation with a human in the loop\n📖\n\n📋\n\n\n\n\n\n\nTeleoperation\n📖\n\n📋\n\n\n\n\n11\nMon, Nov 18\nImitation learning from videos\n📖\n🖥️\n📋\n\n\n\n\n\n\nCausal confusion in imitation learning\n📖\n\n📋\n\n\n\n\n12\nMon, Nov 25\nRepresentation learning for imitation\n📖\n🖥️\n📋\n\n\n\n\n\n\nGeneralization and safety guarantees for imitation\n📖\n\n📋\n\n\n\n\n13\nMon, Dec 2\nProject presentations\n\n\n\n\n\n\n\n14\nMon, Dec 9\nFinal project submission\n\n\n\n\n📂\n\n\n\n\n\nWeek 1\nImitation learning vs supervised learning\nAn invitation to imitation\nALVINN: An autonomous land vehicle in a neural network\nMitigating compunding errors via dataset aggregation\nDAgger: A reduction of imitation learning and structured prediction to no-regret online learning\nMitigating compounding errors by choosing loss functions\nIs Behavior Cloning All You Need? Understanding Horizon in Imitation Learning\nImitation via multi-modal generative models\nDiffusion policy\nTraining instabilities of behavioral cloning\nButterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression\n\n\nTeleoperation interfaces for manipulation [optional]\n\nUMI: Universal Manipulator Interface\nALOHA: Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware\nReal-Time Bimanual Dexterous Teleoperation for Imitation Learning\nTeleoperation with Immersive Active Visual Feedback\nACE: A Cross-platform Visual-Exoskeleton for Low-Cost Dexterous Teleoperation\n\n\n\nQuerying experts only when necessary [optional]\n\nMaximum mean discrepancy imitation learning\nDropoutDAgger: A Bayesian approach to safe imitation learning\nSHIV: Reducing supervisor burden in DAgger using support vectors\nQuery-efficient imitation learning for end-to-end autonomous driving\nConsistent estimators for learning to defer to an expert Selective sampling and imitation learning via online regression\n\n\n\nImitation for visual navigation and autonomous driving [optional]\n\nVisual path following on a manifold in unstructured three-dimensional terrain\nEnd-to-end learning for self-driving cars\nA machine learning approach to visual perception of forest trails for mobile robots\nLearning monocular reactive UAV control in cluttered natural environments\n\n\n\nBehavioral cloning with energy-based models [optional]\n\nImplicit behavioral cloning\nRevisiting energy based models as policies: ranking noise contrastive estimation and interpolating energy models\n\n\n\nWeek 2\nIntro to Optimal Control\nLinear Quadratic Regulator and some examples\nIterative Linear Quadratic Regulator\nModel Predictive Control\nBen Recht: An outsider’s tour of RL (watch his ICML’18 tutorial, too)\n\n\nIntro to model-based RL [optional]\n\nPILCO: Probabilistic inference for learning control\nDeep reinforcement learning in a handful of trials using probabilistic dynamics models\nLearning particle dynamics for manipulating rigid bodies, deformable objects, and fluids\nEnd-to-end differentiable physics for learning and control\nSynthesizing neural network controllers with probabilistic model based reinforcement learning\nA survey on policy search algorithms for learning robot controllers in a handful of trials\nReinforcement learning in robotics: a survey\nDeepMPC: Learning deep latent features for model predictive control\nLearning latent dynamics for planning from pixels\n\n\n\nMonotonic improvement of the value function [optional]\n\nAlgorithmic framework for model-based deep reinforcement learning with theoretical guarantees\nWhen to Trust Your Model: Model-Based Policy Optimization\n\n\n\nLearning dynamics where it matters for the value function [optional]\n\nValue Gradient Weighted Model-Based Reinforcement Learning\n\n\n\nParallelizing MPC on the GPU [optional]\n\nMPCGPU: Real-Time Nonlinear Model Predictive Control through Preconditioned Conjugate Gradient on the GPU\nSTORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation\n\n\n\nWeek 3\nOffline / Batch Reinforcement Learning\nConservative Q-Learning for offline reinforcement learning\nIQ-Learn: inverse soft-Q learning for imitation\nScaling data-driven robotics with reward sketching and batch reinforcement learning\nOff-policy deep reinforcement learning without exploration\nD4RL: Datasets for deep data-driven reinforcement learning\nWhat matters in learning from offline human demonstrations for robot manipulation\nNeurIPS 2020 tutorial on offline RL\nTransitioning from offline to online RL\nCal-QL: calibrated offline RL pre-training for efficient online fine-tuning\n\n\nOptional reading\n\nOffline reinforcement learning: tutorial, review, and perspectives on open problems\nShould I run offline reinforcement learning or behavioral cloning?\nWhy should I trust you, Bellman? The Bellman error is a poor replacement for value error\nA minimalist approach to offline reinforcement learning\nBenchmarking batch deep reinforcement learning algorithms\nStabilizing off-policy Q-Learning via bootstrapping error reduction\nAn optimistic perspective on offline reinforcement learning\nCOG: Connecting new skills to past experience with offline reinforcement learning\nIRIS: Implicit reinforcement without interaction at scale for learning control from offline robot manipulation data\n(Batch) reinforcement learning for robot soccer\nInstabilities of offline RL with pre-trained neural representation\nTargeted environment design from offline data\n\n\n\nWeek 4\nImitation learning combined with RL and planning\nLearning neural network policies with guided policy search under unknown dynamics\nPlanning with diffusion for flexible behavior synthesis\nExpert iteration\nThinking fast and slow with deep learning and tree search\nDual policy iteration\nLearning to search via retrospective imitation\nLearning from experts that have privileged information\nPLATO: Policy learning using adaptive trajectory optimization\nDynamic movement primitives\nDynamic Movement Primitives in robotics: a tutorial survey\nUsing probabilistic movement primitives in robotics\n\n\nMaking cost-to-go queries to experts (optional)\n\nAggreVaTe: Reinforcement and imitation learning via interactive no-regret learning\nDeeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction\nTruncated Horizon Policy Search: Combining RL & Imitation Learning\n\n\n\nImitation can improve search and exploration strategies (optional)\n\nLearning to gather information via imitation\nOvercoming exploration in reinforcement learning with demonstrations\nData-driven planning via imitation learning\n\n\n\nWeek 5\nInverse reinforcement learning\nMaximum entropy inverse reinforcement learning\nGuided Cost Learning: Deep inverse optimal control via policy optimization\nBayesian inverse reinforcement learning\nInferring rewards from preferences\nActive preference-based learning of reward functions\nExtrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations\nInferring constraints from demonstrations\nInverse KKT: Learning cost functions of manipulation tasks from demonstrations\nTask specification and human-robot dialog\nRobots that ask for help: uncertainty alignment for LLM planners\nValue alignment\nInverse reward design\n\n\nOptional reading\n\nNonlinear inverse reinforcement learning with gaussian processes\nMaximum margin planning\nCompatible reward inverse reinforcement learning\nLearning the preferences of ignorant, inconsistent agents\nImputing a convex objective function\nBetter-than-demonstrator imitation learning via automatically-ranked demonstrations\n\n\n\nApplications of Inverse RL (optional)\n\nSocially compliant mobile robot navigation via inverse reinforcement learning\nModel-based probabilistic pursuit via inverse reinforcement learning\nFirst-person activity forecasting with online inverse reinforcement learning\nLearning strategies in table tennis using inverse reinforcement learning\nPlanning-based prediction for pedestrians\nActivity forecasting\nLarge-scale cost function learning for path planning using deep inverse reinforcement learning\n\n\n\nWeek 6\nThanksgiving week. No lecture on Monday.\n\n\nWeek 7\nImitation as program induction\nNeural programmer-interpreters\nNeural Task Programming: Learning to generalize across hierarchical tasks\nModular decomposition of demonstrations into skills\nTACO: Learning task decomposition via temporal alignment for control\n(Hierarchical) imitation of multi-goal tasks\nLearning to generalize across long-horizon tasks from human demonstrations\nInferring goals, grammars, and planning domains\nThe motion grammar: analysis of a linguistic method for robot control\nAction understanding as inverse planning\n\n\nOptional reading\n\nIncremental learning of subtasks from unsegmented demonstration\nInducing probabilistic context-free grammars for the sequencing of movement primitives\nNeural Task Graphs: Generalizing to unseen tasks from a single video demonstration\nNeural program synthesis from diverse demonstration videos\nAutomata guided reinforcement learning with demonstrations\nA syntactic approach to robot imitation learning using probabilistic activity grammars\nRobot learning from demonstration by constructing skill trees\nLearning to sequence movement primitives from demonstrations\nImitation-projected programmatic reinforcement learning\nReinforcement and imitation learning for diverse visuomotor skills\nInferring task goals and constraints using Bayesian nonparametric inverse reinforcement learning\nYou only demonstrate once: category-level manipulation from single visual demonstration\nBottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation\n\n\n\nWeek 8\nFall reading week. No lecture.\n\n\nWeek 9\nAdversarial imitation learning\nGAIL: Generative adversarial imitation learning\nLearning robust rewards with adversarial inverse reinforcement learning\nInfoGAIL: interpretable imitation learning from visual demonstrations\nWhat matters for adversarial imitation learning?\nA divergence minimization perspective on imitation learning methods\nMulti-agent generative adversarial imitation learning\n\n\nOptional reading\n\nModel-free imitation learning with policy optimization\nImitation learning via off-policy distribution matching\nDomain adaptive imitation learning\n\n\n\nWeek 10\nTeleoperation\nRelaxedIK: Real-time synthesis of accurate and feasible robot arm motion\nError-aware imitation learning from teleoperation data for mobile manipulation\nControlling assistive robots with learned latent actions\nShared autonomy\nShared autonomy via deep reinforcement learning\nShared autonomy via hindsight optimization\nImitation with a human in the loop\nLearning models for shared control of human-machine systems with unknown dynamics\nHuman-in-the-loop imitation learning using remote teleoperation\n\n\nOptional reading\n\nOptional Reading Designing robot learners that ask good questions\nBlending human and robot inputs for sliding scale autonomy\nInferring and assisting with constraints in shared autonomy\nCollaborative control for a robotic wheelchair: evaluation of performance, attention, and workload\nDirector: A user interface designed for robot operation with shared autonomy\nLearning multi-arm manipulation through collaborative teleoperation\nInteractive autonomous driving through adaptation from participation\n\n\n\nWeek 11\nImitation learning from videos\nK-VIL: Keypoints-based visual imitation learning\nTrack2Act: Predicting point tracks from internet videos enables generalizable robot manipulation\nVideoDex: Learning dexterity from internet videos\nMotion Retargeting\nRobotic Telekinesis: Learning a robotic hand imitator by watching humans on YouTube\nCausal confusion in imitation learning\nCausal confusion in imitation learning\n\n\nOptional reading\n\nTowards generalist robot learning from internet video: a survey\nAVID: Learning multi-stage tasks via pixel-level translation of human videos\nDreamitate: Real-world visuomotor policy learning via video generation\nUnderstanding Human Hands in Contact at Internet Scale\nZero-shot robot manipulation from passive human videos\nSFV: Reinforcement Learning of Physical Skills from Videos\nDexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video\nDiffusion Reward: Learning Rewards via Conditional Video Diffusion\nTask Success is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors\nVideo Prediction Models as Rewards for Reinforcement Learning\nGiving Robots a Hand: Learning generalizable manipulation with eye-in-hand human video demonstrations\nEstimating Q(s,s’) with Deep Deterministic Dynamics Gradients\nRT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\nHuman-to-robot imitation in the wild\nVision-Language Models as Success Detectors\nLearning Generalizable Robotic Reward Functions from “In-The-Wild” Human Videos\nSemantic Visual Navigation by Watching YouTube Videos\nRobotic Offline RL from Internet Videos via Value-Function Pre-Training\nHumanPlus: Humanoid Shadowing and Imitation from Humans\nViViDex: Learning Vision-based Dexterous Manipulation from Human Videos\nGen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation\nOK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics\nUnifying 3D Representation and Control of Diverse Robots with a Single Camera\n\n\n\nWeek 12\nRepresentation learning for imitation\nGeneralization guarantees for imitation learning\nProvable representation learning for imitation with contrastive Fourier features\nTRAIL: near-optimal imitation learning with suboptimal data\nRepresentation matters: offline pretraining for sequential decision making\nSelf-supervised correspondence in visuomotor policy learning\nThe surprising effectiveness of representation learning for visual imitation\nGeneralization and safety guarantees for imitation\nProvable guarantees for generative behavior cloning: bridging low-level stability and high-level behavior\nImitation learning with stability and safety guarantees\n\n\nWeek 13\nProject presentations\n\n\nWeek 14\nFinal project submission",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──\n\n\n✓ broom        0.7.10         ✓ rsample      0.1.1     \n✓ dials        0.0.10         ✓ tune         0.1.6     \n✓ infer        1.0.1.9000     ✓ workflows    0.2.4     \n✓ modeldata    0.1.1          ✓ workflowsets 0.1.0     \n✓ parsnip      0.1.7          ✓ yardstick    0.0.9     \n✓ recipes      0.2.0          \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_… .fitted  .resid    .hat .sigma .cooksd\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# … with 332 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Equation 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Equation 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Equation 2 for interpretations and predictions, we will use Equation 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Equation 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Equation 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Equation 4 and Equation 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Equation 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Equation 4 using Equation 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Equation 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Equation 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Equation 12 below is mathematically equivalent to Equation 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "slides/lec-2.html#announcements",
    "href": "slides/lec-2.html#announcements",
    "title": "Simple Linear Regression",
    "section": "Announcements",
    "text": "Announcements\n\nIf you’re just joining the class, welcome! Go to the course website and review content you’ve missed, read the syllabus, and complete the Getting to know you survey.\nLab 1 is due Friday, at 5pm, on Gradescope."
  },
  {
    "objectID": "slides/lec-2.html#dorianne-gray-says",
    "href": "slides/lec-2.html#dorianne-gray-says",
    "title": "Simple Linear Regression",
    "section": "Dorianne Gray says…",
    "text": "Dorianne Gray says…"
  },
  {
    "objectID": "slides/lec-2.html#outline",
    "href": "slides/lec-2.html#outline",
    "title": "Simple Linear Regression",
    "section": "Outline",
    "text": "Outline\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable\nEstimate the slope and intercept of the regression line using the least squares method\nInterpret the slope and intercept of the regression line"
  },
  {
    "objectID": "slides/lec-2.html#computational-setup",
    "href": "slides/lec-2.html#computational-setup",
    "title": "Simple Linear Regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/lec-2.html#movie-ratings",
    "href": "slides/lec-2.html#movie-ratings",
    "title": "Simple Linear Regression",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/lec-2.html#data-prep",
    "href": "slides/lec-2.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango %&gt;%\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/lec-2.html#data-overview",
    "href": "slides/lec-2.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A…\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,…\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,…\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,…\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8…\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4…\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5…\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0…\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4…\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3…\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4…\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3…\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3…\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0…\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0…\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0…\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5…\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5…\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54…\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, …\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, …\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…"
  },
  {
    "objectID": "slides/lec-2.html#data-visualization",
    "href": "slides/lec-2.html#data-visualization",
    "title": "Simple Linear Regression",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/lec-2.html#fit-a-line",
    "href": "slides/lec-2.html#fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Fit a line",
    "text": "Fit a line\n… to describe the relationship between the critics and audience score"
  },
  {
    "objectID": "slides/lec-2.html#terminology",
    "href": "slides/lec-2.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nOutcome, Y: variable describing the outcome of interest\nPredictor, X: variable used to help understand the variability in the outcome"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-1",
    "href": "slides/lec-2.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-2",
    "href": "slides/lec-2.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-2.html#regression-model-residuals",
    "href": "slides/lec-2.html#regression-model-residuals",
    "title": "Simple Linear Regression",
    "section": "Regression model + residuals",
    "text": "Regression model + residuals\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[8pt]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec-2.html#simple-linear-regression-1",
    "href": "slides/lec-2.html#simple-linear-regression-1",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationthip between a quantitative outcome (\\(Y\\)) and a single quantitative predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/lec-2.html#simple-linear-regression-2",
    "href": "slides/lec-2.html#simple-linear-regression-2",
    "title": "Simple Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/lec-2.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/lec-2.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/lec-2.html#residuals",
    "href": "slides/lec-2.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\\]"
  },
  {
    "objectID": "slides/lec-2.html#least-squares-line",
    "href": "slides/lec-2.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/lec-2.html#properties-of-least-squares-regression",
    "href": "slides/lec-2.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/lec-2.html#estimating-the-slope",
    "href": "slides/lec-2.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\n\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-2.html#estimating-the-intercept",
    "href": "slides/lec-2.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n\\[\\begin{aligned}\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n\n\\[\n\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-2.html#interpreting-the-slope",
    "href": "slides/lec-2.html#interpreting-the-slope",
    "title": "Simple Linear Regression",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\nPoll: The slope of the model for predicting audience score from critics score is 32.3142. Which of the following is the best interpretation of this value?\n\nFor every one point increase in the critics score, the audience score goes up by 0.5187 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.5187 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.5187 points.\nFor every one point increase in the audience score, the critics score goes up by 0.5187 points, on average."
  },
  {
    "objectID": "slides/lec-2.html#interpreting-slope-intercept",
    "href": "slides/lec-2.html#interpreting-slope-intercept",
    "title": "Simple Linear Regression",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\\widehat{\\text{audience}} = 32.3142 + 0.5187 \\times \\text{critics}\\]\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.5187 points, on average.\nIntercept: If the critics score is 0 points, we expect the audience score to be 32.3142 points."
  },
  {
    "objectID": "slides/lec-2.html#is-the-intercept-meaningful",
    "href": "slides/lec-2.html#is-the-intercept-meaningful",
    "title": "Simple Linear Regression",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n✅ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\n🛑 Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/lec-2.html#making-a-prediction",
    "href": "slides/lec-2.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 50. According to this model, what is the movie’s predicted audience score?\n\\[\n\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 50 \\\\\n&= 58.2492\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-2.html#extrapolation",
    "href": "slides/lec-2.html#extrapolation",
    "title": "Simple Linear Regression",
    "section": "Extrapolation",
    "text": "Extrapolation\nSuppose that a movie has a critics score of 0. According to this model, what is the movie’s predicted audience score?"
  },
  {
    "objectID": "slides/lec-2.html#recap-1",
    "href": "slides/lec-2.html#recap-1",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.å\nWe interpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/lec-18.html#announcements",
    "href": "slides/lec-18.html#announcements",
    "title": "Logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nSchedule changes for the remainder of the semester\nThursday office hours in my office: 213 Old Chem\nAny questions on project proposals?"
  },
  {
    "objectID": "slides/lec-18.html#topics",
    "href": "slides/lec-18.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUse logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/lec-18.html#computational-setup",
    "href": "slides/lec-18.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-18.html#types-of-outcome-variables",
    "href": "slides/lec-18.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house in Levittown, NY\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcone variable:\n\nHigh risk of coronary heart disease\nModel: Probability an adult is high risk of heart disease given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/lec-18.html#models-for-categorical-outcomes",
    "href": "slides/lec-18.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n1: Yes, 0: No\n\nMultinomial logistic regression\n3+ Outcomes\n1: Democrat, 2: Republican, 3: Independent"
  },
  {
    "objectID": "slides/lec-18.html#election-forecasts",
    "href": "slides/lec-18.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2020 election forecasts",
    "text": "2020 election forecasts\n\nSource: FiveThirtyEight Election Forcasts"
  },
  {
    "objectID": "slides/lec-18.html#nba-finals-predictions",
    "href": "slides/lec-18.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "NBA finals predictions",
    "text": "NBA finals predictions\n\nSource: FiveThirtyEight 2019-20 NBA Predictions"
  },
  {
    "objectID": "slides/lec-18.html#do-teenagers-get-7-hours-of-sleep",
    "href": "slides/lec-18.html#do-teenagers-get-7-hours-of-sleep",
    "title": "Logistic regression",
    "section": "Do teenagers get 7+ hours of sleep?",
    "text": "Do teenagers get 7+ hours of sleep?\n\n\nStudents in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.\nSleep7\n1: yes\n0: no\n\n\ndata(YouthRisk2009)\nsleep &lt;- YouthRisk2009 %&gt;%\n  as_tibble() %&gt;%\n  filter(!is.na(Age), !is.na(Sleep7))\nsleep %&gt;%\n  relocate(Age, Sleep7)\n\n# A tibble: 446 × 6\n     Age Sleep7 Sleep           SmokeLife SmokeDaily MarijuaEver\n   &lt;int&gt;  &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;     &lt;fct&gt;            &lt;int&gt;\n 1    16      1 8 hours         Yes       Yes                  1\n 2    17      0 5 hours         Yes       Yes                  1\n 3    18      0 5 hours         Yes       Yes                  1\n 4    17      1 7 hours         Yes       No                   1\n 5    15      0 4 or less hours No        No                   0\n 6    17      0 6 hours         No        No                   0\n 7    17      1 7 hours         No        No                   0\n 8    16      1 8 hours         Yes       No                   0\n 9    16      1 8 hours         No        No                   0\n10    18      0 4 or less hours Yes       Yes                  1\n# … with 436 more rows"
  },
  {
    "objectID": "slides/lec-18.html#plot-the-data",
    "href": "slides/lec-18.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data\n\nggplot(sleep, aes(x = Age, y = Sleep7)) +\n  geom_point() + \n  labs(y = \"Getting 7+ hours of sleep\")"
  },
  {
    "objectID": "slides/lec-18.html#lets-fit-a-linear-regression-model",
    "href": "slides/lec-18.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let’s fit a linear regression model",
    "text": "Let’s fit a linear regression model\nOutcome: \\(Y\\) = 1: yes, 0: no"
  },
  {
    "objectID": "slides/lec-18.html#lets-use-proportions",
    "href": "slides/lec-18.html#lets-use-proportions",
    "title": "Logistic regression",
    "section": "Let’s use proportions",
    "text": "Let’s use proportions\nOutcome: Probability of getting 7+ hours of sleep"
  },
  {
    "objectID": "slides/lec-18.html#what-happens-if-we-zoom-out",
    "href": "slides/lec-18.html#what-happens-if-we-zoom-out",
    "title": "Logistic regression",
    "section": "What happens if we zoom out?",
    "text": "What happens if we zoom out?\nOutcome: Probability of getting 7+ hours of sleep\n\n🛑 This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/lec-18.html#lets-try-another-model",
    "href": "slides/lec-18.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let’s try another model",
    "text": "Let’s try another model\n\n✅ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/lec-18.html#the-code",
    "href": "slides/lec-18.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nggplot(sleep_age, aes(x = Age, y = prop)) +\n  geom_point() + \n  geom_hline(yintercept = c(0,1), lty = 2) + \n  stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE) +\n  labs(y = \"P(7+ hours of sleep)\") +\n  xlim(1, 40) +\n  ylim(-0.5, 1.5)"
  },
  {
    "objectID": "slides/lec-18.html#different-types-of-models",
    "href": "slides/lec-18.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLinear regression (transform Y)\nQuantitative\n\\(\\log(Y) = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)"
  },
  {
    "objectID": "slides/lec-18.html#binary-response-variable",
    "href": "slides/lec-18.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\\(Y = 1: \\text{ yes}, 0: \\text{ no}\\)\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/lec-18.html#odds",
    "href": "slides/lec-18.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won’t rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)"
  },
  {
    "objectID": "slides/lec-18.html#are-teenagers-getting-enough-sleep",
    "href": "slides/lec-18.html#are-teenagers-getting-enough-sleep",
    "title": "Logistic regression",
    "section": "Are teenagers getting enough sleep?",
    "text": "Are teenagers getting enough sleep?\n\nsleep %&gt;%\n  count(Sleep7) %&gt;%\n  mutate(p = round(n / sum(n), 3))\n\n# A tibble: 2 × 3\n  Sleep7     n     p\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1      0   150 0.336\n2      1   296 0.664\n\n\n\n\\(P(\\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664\\)\n\n\n\\(P(\\text{&lt; 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336\\)\n\n\n\\(P(\\text{odds of 7+ hours of sleep}) = \\frac{0.664}{0.336} = 1.976\\)"
  },
  {
    "objectID": "slides/lec-18.html#from-odds-to-probabilities",
    "href": "slides/lec-18.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nodds\n\\[\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{\\omega}{1 + \\omega}\\]"
  },
  {
    "objectID": "slides/lec-18.html#logistic-regression",
    "href": "slides/lec-18.html#logistic-regression",
    "title": "Logistic regression",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "slides/lec-18.html#from-odds-to-probabilities-1",
    "href": "slides/lec-18.html#from-odds-to-probabilities-1",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\nLogistic model: log odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/lec-18.html#logistic-regression-model",
    "href": "slides/lec-18.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\n\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\n\\]"
  },
  {
    "objectID": "slides/lec-18.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-18.html#risk-of-coronary-heart-disease",
    "title": "Logistic regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use age to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)"
  },
  {
    "objectID": "slides/lec-18.html#data-heart",
    "href": "slides/lec-18.html#data-heart",
    "title": "Logistic regression",
    "section": "Data: heart",
    "text": "Data: heart\n\nheart_disease &lt;- read_csv(here::here(\"slides\", \"data/framingham.csv\")) %&gt;%\n  select(age, TenYearCHD) %&gt;%\n  drop_na() %&gt;%\n  mutate(high_risk = as.factor(TenYearCHD)) %&gt;%\n  select(age, high_risk)\n\nheart_disease\n\n# A tibble: 4,240 × 2\n     age high_risk\n   &lt;dbl&gt; &lt;fct&gt;    \n 1    39 0        \n 2    46 0        \n 3    48 0        \n 4    61 1        \n 5    46 0        \n 6    43 0        \n 7    63 1        \n 8    45 0        \n 9    52 0        \n10    43 0        \n# … with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#high-risk-vs.-age",
    "href": "slides/lec-18.html#high-risk-vs.-age",
    "title": "Logistic regression",
    "section": "High risk vs. age",
    "text": "High risk vs. age\n\nggplot(heart_disease, aes(x = high_risk, y = age)) +\n  geom_boxplot() +\n  labs(x = \"High risk - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. High risk of heart disease\")"
  },
  {
    "objectID": "slides/lec-18.html#lets-fit-the-model",
    "href": "slides/lec-18.html#lets-fit-the-model",
    "title": "Logistic regression",
    "section": "Let’s fit the model",
    "text": "Let’s fit the model\n\nheart_disease_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0"
  },
  {
    "objectID": "slides/lec-18.html#the-model",
    "href": "slides/lec-18.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0\n\n\n\n\n\n. . .\n\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.075 \\times \\text{age}\\] where \\(\\hat{\\pi}\\) is the predicted probability of being high risk"
  },
  {
    "objectID": "slides/lec-18.html#predicted-log-odds",
    "href": "slides/lec-18.html#predicted-log-odds",
    "title": "Logistic regression",
    "section": "Predicted log odds",
    "text": "Predicted log odds\n\naugment(heart_disease_fit$fit)\n\n# A tibble: 4,240 × 8\n   high_risk   age .fitted .resid .std.resid     .hat .sigma   .cooksd\n   &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 0            39  -2.65  -0.370     -0.370 0.000466  0.895 0.0000165\n 2 0            46  -2.13  -0.475     -0.475 0.000322  0.895 0.0000192\n 3 0            48  -1.98  -0.509     -0.509 0.000288  0.895 0.0000199\n 4 1            61  -1.01   1.62       1.62  0.000706  0.895 0.000968 \n 5 0            46  -2.13  -0.475     -0.475 0.000322  0.895 0.0000192\n 6 0            43  -2.35  -0.427     -0.427 0.000384  0.895 0.0000183\n 7 1            63  -0.858  1.56       1.56  0.000956  0.895 0.00113  \n 8 0            45  -2.20  -0.458     -0.458 0.000342  0.895 0.0000189\n 9 0            52  -1.68  -0.585     -0.585 0.000262  0.895 0.0000244\n10 0            43  -2.35  -0.427     -0.427 0.000384  0.895 0.0000183\n# … with 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted odds} = \\hat{\\omega} = \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp\\{-2.650\\} = 0.071\\]"
  },
  {
    "objectID": "slides/lec-18.html#predicted-probabilities",
    "href": "slides/lec-18.html#predicted-probabilities",
    "title": "Logistic regression",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"prob\")\n\n# A tibble: 4,240 × 2\n   .pred_0 .pred_1\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1   0.934  0.0660\n 2   0.894  0.106 \n 3   0.878  0.122 \n 4   0.733  0.267 \n 5   0.894  0.106 \n 6   0.913  0.0870\n 7   0.702  0.298 \n 8   0.900  0.0996\n 9   0.843  0.157 \n10   0.913  0.0870\n# … with 4,230 more rows\n\n\n\n\\[\\text{predicted probabilities} = \\hat{\\pi} = \\frac{\\exp\\{-2.650\\}}{1 + \\exp\\{-2.650\\}} = 0.066\\]"
  },
  {
    "objectID": "slides/lec-18.html#predicted-classes",
    "href": "slides/lec-18.html#predicted-classes",
    "title": "Logistic regression",
    "section": "Predicted classes",
    "text": "Predicted classes\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"class\")\n\n# A tibble: 4,240 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#default-prediction",
    "href": "slides/lec-18.html#default-prediction",
    "title": "Logistic regression",
    "section": "Default prediction",
    "text": "Default prediction\nFor a logistic regression, the default prediction is the class.\n\npredict(heart_disease_fit, new_data = heart_disease)\n\n# A tibble: 4,240 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 4,230 more rows"
  },
  {
    "objectID": "slides/lec-18.html#observed-vs.-predicted",
    "href": "slides/lec-18.html#observed-vs.-predicted",
    "title": "Logistic regression",
    "section": "Observed vs. predicted",
    "text": "Observed vs. predicted\n\nWhat does the following table show?\n\n\npredict(heart_disease_fit, new_data = heart_disease) %&gt;%\n  bind_cols(heart_disease) %&gt;%\n  count(high_risk, .pred_class)\n\n# A tibble: 2 × 3\n  high_risk .pred_class     n\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt;\n1 0         0            3596\n2 1         0             644"
  },
  {
    "objectID": "slides/lec-18.html#recap",
    "href": "slides/lec-18.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUsed logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/lec-18.html#application-exercise",
    "href": "slides/lec-18.html#application-exercise",
    "title": "Logistic regression",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-9-odds"
  },
  {
    "objectID": "slides/lec-25.html#topics",
    "href": "slides/lec-25.html#topics",
    "title": "MultiLR: Predictive models",
    "section": "Topics",
    "text": "Topics\n\nBuilding predictive multinomial logistic regression models\nComparing models"
  },
  {
    "objectID": "slides/lec-25.html#computational-setup",
    "href": "slides/lec-25.html#computational-setup",
    "title": "MultiLR: Predictive models",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)\nlibrary(themis)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-25.html#terminology",
    "href": "slides/lec-25.html#terminology",
    "title": "MultiLR: Predictive models",
    "section": "Terminology",
    "text": "Terminology\n\nWhat’s the difference between regression and classification?\n\nLogistic regression / binary classification\nMultinomial logistic regression / multinomial classification"
  },
  {
    "objectID": "slides/lec-25.html#volcanoes",
    "href": "slides/lec-25.html#volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Volcanoes",
    "text": "Volcanoes\nThe data come from The Smithsonian Institution, via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"slides\", \"data/volcano.csv\"))\nnames(volcano)\n\n [1] \"volcano_number\"           \"volcano_name\"            \n [3] \"primary_volcano_type\"     \"last_eruption_year\"      \n [5] \"country\"                  \"region\"                  \n [7] \"subregion\"                \"latitude\"                \n [9] \"longitude\"                \"elevation\"               \n[11] \"tectonic_settings\"        \"evidence_category\"       \n[13] \"major_rock_1\"             \"major_rock_2\"            \n[15] \"major_rock_3\"             \"major_rock_4\"            \n[17] \"major_rock_5\"             \"minor_rock_1\"            \n[19] \"minor_rock_2\"             \"minor_rock_3\"            \n[21] \"minor_rock_4\"             \"minor_rock_5\"            \n[23] \"population_within_5_km\"   \"population_within_10_km\" \n[25] \"population_within_30_km\"  \"population_within_100_km\""
  },
  {
    "objectID": "slides/lec-25.html#volcanoes-1",
    "href": "slides/lec-25.html#volcanoes-1",
    "title": "MultiLR: Predictive models",
    "section": "Volcanoes",
    "text": "Volcanoes\n\nglimpse(volcano)\n\nRows: 958\nColumns: 26\n$ volcano_number           &lt;dbl&gt; 283001, 355096, 342080, 213004, 321040, 28317…\n$ volcano_name             &lt;chr&gt; \"Abu\", \"Acamarachi\", \"Acatenango\", \"Acigol-Ne…\n$ primary_volcano_type     &lt;chr&gt; \"Shield(s)\", \"Stratovolcano\", \"Stratovolcano(…\n$ last_eruption_year       &lt;chr&gt; \"-6850\", \"Unknown\", \"1972\", \"-2080\", \"950\", \"…\n$ country                  &lt;chr&gt; \"Japan\", \"Chile\", \"Guatemala\", \"Turkey\", \"Uni…\n$ region                   &lt;chr&gt; \"Japan, Taiwan, Marianas\", \"South America\", \"…\n$ subregion                &lt;chr&gt; \"Honshu\", \"Northern Chile, Bolivia and Argent…\n$ latitude                 &lt;dbl&gt; 34.500, -23.292, 14.501, 38.537, 46.206, 37.6…\n$ longitude                &lt;dbl&gt; 131.600, -67.618, -90.876, 34.621, -121.490, …\n$ elevation                &lt;dbl&gt; 641, 6023, 3976, 1683, 3742, 1728, 1733, 1250…\n$ tectonic_settings        &lt;chr&gt; \"Subduction zone / Continental crust (&gt;25 km)…\n$ evidence_category        &lt;chr&gt; \"Eruption Dated\", \"Evidence Credible\", \"Erupt…\n$ major_rock_1             &lt;chr&gt; \"Andesite / Basaltic Andesite\", \"Dacite\", \"An…\n$ major_rock_2             &lt;chr&gt; \"Basalt / Picro-Basalt\", \"Andesite / Basaltic…\n$ major_rock_3             &lt;chr&gt; \"Dacite\", \" \", \" \", \"Basalt / Picro-Basalt\", …\n$ major_rock_4             &lt;chr&gt; \" \", \" \", \" \", \"Andesite / Basaltic Andesite\"…\n$ major_rock_5             &lt;chr&gt; \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", …\n$ minor_rock_1             &lt;chr&gt; \" \", \" \", \"Basalt / Picro-Basalt\", \" \", \"Daci…\n$ minor_rock_2             &lt;chr&gt; \" \", \" \", \" \", \" \", \" \", \"Basalt / Picro-Basa…\n$ minor_rock_3             &lt;chr&gt; \" \", \" \", \" \", \" \", \" \", \" \", \" \", \"Andesite …\n$ minor_rock_4             &lt;chr&gt; \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", …\n$ minor_rock_5             &lt;chr&gt; \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", …\n$ population_within_5_km   &lt;dbl&gt; 3597, 0, 4329, 127863, 0, 428, 101, 51, 0, 98…\n$ population_within_10_km  &lt;dbl&gt; 9594, 7, 60730, 127863, 70, 3936, 485, 6042, …\n$ population_within_30_km  &lt;dbl&gt; 117805, 294, 1042836, 218469, 4019, 717078, 1…\n$ population_within_100_km &lt;dbl&gt; 4071152, 9092, 7634778, 2253483, 393303, 5024…"
  },
  {
    "objectID": "slides/lec-25.html#types-of-volcanoes",
    "href": "slides/lec-25.html#types-of-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Types of volcanoes",
    "text": "Types of volcanoes\nProbably too many types!\n\nvolcano %&gt;%\n  count(primary_volcano_type, sort = TRUE) %&gt;%\n  print(n = 26)\n\n# A tibble: 26 × 2\n   primary_volcano_type     n\n   &lt;chr&gt;                &lt;int&gt;\n 1 Stratovolcano          353\n 2 Stratovolcano(es)      107\n 3 Shield                  85\n 4 Volcanic field          71\n 5 Pyroclastic cone(s)     70\n 6 Caldera                 65\n 7 Complex                 46\n 8 Shield(s)               33\n 9 Submarine               27\n10 Lava dome(s)            26\n11 Fissure vent(s)         12\n12 Caldera(s)               9\n13 Compound                 9\n14 Maar(s)                  8\n15 Pyroclastic shield       7\n16 Tuff cone(s)             7\n17 Crater rows              5\n18 Subglacial               5\n19 Pyroclastic cone         4\n20 Lava dome                3\n21 Complex(es)              1\n22 Lava cone                1\n23 Lava cone(es)            1\n24 Lava cone(s)             1\n25 Stratovolcano?           1\n26 Tuff cone                1"
  },
  {
    "objectID": "slides/lec-25.html#relevel-volcanoes",
    "href": "slides/lec-25.html#relevel-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Relevel volcanoes",
    "text": "Relevel volcanoes\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  )\n\nvolcano %&gt;%\n  count(volcano_type)\n\n# A tibble: 3 × 2\n  volcano_type      n\n  &lt;fct&gt;         &lt;int&gt;\n1 Stratovolcano   461\n2 Shield          118\n3 Other           379"
  },
  {
    "objectID": "slides/lec-25.html#data-prep",
    "href": "slides/lec-25.html#data-prep",
    "title": "MultiLR: Predictive models",
    "section": "Data prep",
    "text": "Data prep\n\nSelect a few variables as predictors for the model with\nConvert all character variables to factors\n\n\n\nvolcano &lt;- volcano %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "slides/lec-25.html#mapping-the-volcanoes",
    "href": "slides/lec-25.html#mapping-the-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Mapping the volcanoes",
    "text": "Mapping the volcanoes"
  },
  {
    "objectID": "slides/lec-25.html#world-map-data",
    "href": "slides/lec-25.html#world-map-data",
    "title": "MultiLR: Predictive models",
    "section": "World map data",
    "text": "World map data\n\nworld &lt;- map_data(\"world\")\n\nworld %&gt;% as_tibble()\n\n# A tibble: 99,338 × 6\n    long   lat group order region subregion\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 -69.9  12.5     1     1 Aruba  &lt;NA&gt;     \n 2 -69.9  12.4     1     2 Aruba  &lt;NA&gt;     \n 3 -69.9  12.4     1     3 Aruba  &lt;NA&gt;     \n 4 -70.0  12.5     1     4 Aruba  &lt;NA&gt;     \n 5 -70.1  12.5     1     5 Aruba  &lt;NA&gt;     \n 6 -70.1  12.6     1     6 Aruba  &lt;NA&gt;     \n 7 -70.0  12.6     1     7 Aruba  &lt;NA&gt;     \n 8 -70.0  12.6     1     8 Aruba  &lt;NA&gt;     \n 9 -69.9  12.5     1     9 Aruba  &lt;NA&gt;     \n10 -69.9  12.5     1    10 Aruba  &lt;NA&gt;     \n# … with 99,328 more rows"
  },
  {
    "objectID": "slides/lec-25.html#draw-world-map",
    "href": "slides/lec-25.html#draw-world-map",
    "title": "MultiLR: Predictive models",
    "section": "Draw world map",
    "text": "Draw world map\n\n\nworld_map &lt;- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map"
  },
  {
    "objectID": "slides/lec-25.html#add-volcanoes",
    "href": "slides/lec-25.html#add-volcanoes",
    "title": "MultiLR: Predictive models",
    "section": "Add volcanoes",
    "text": "Add volcanoes\n\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(\n      x = longitude, y = latitude, \n      color = volcano_type, \n      shape = volcano_type),\n    alpha = 0.5\n  ) +\n  scale_color_OkabeIto() +\n  labs(color = NULL, shape = NULL)"
  },
  {
    "objectID": "slides/lec-25.html#your-turn",
    "href": "slides/lec-25.html#your-turn",
    "title": "MultiLR: Predictive models",
    "section": "Your turn",
    "text": "Your turn\n\n📋 github.com/sta210-s22/ae-11-volcanoes - Exercise 1"
  },
  {
    "objectID": "slides/lec-25.html#split-into-testingtraining",
    "href": "slides/lec-25.html#split-into-testingtraining",
    "title": "MultiLR: Predictive models",
    "section": "Split into testing/training",
    "text": "Split into testing/training\n\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)"
  },
  {
    "objectID": "slides/lec-25.html#create-a-recipe",
    "href": "slides/lec-25.html#create-a-recipe",
    "title": "MultiLR: Predictive models",
    "section": "Create a recipe",
    "text": "Create a recipe\nStart with a model that doesn’t use geographic information:\n\nvolcano_rec1 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_rm(latitude, longitude) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())"
  },
  {
    "objectID": "slides/lec-25.html#specify-a-model",
    "href": "slides/lec-25.html#specify-a-model",
    "title": "MultiLR: Predictive models",
    "section": "Specify a model",
    "text": "Specify a model\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_spec\n\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-25.html#create-a-workflow",
    "href": "slides/lec-25.html#create-a-workflow",
    "title": "MultiLR: Predictive models",
    "section": "Create a workflow",
    "text": "Create a workflow\n\nvolcano_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec1) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_center()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-25.html#create-cross-validation-folds",
    "href": "slides/lec-25.html#create-cross-validation-folds",
    "title": "MultiLR: Predictive models",
    "section": "Create cross validation folds",
    "text": "Create cross validation folds\n\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5"
  },
  {
    "objectID": "slides/lec-25.html#fit-resamples",
    "href": "slides/lec-25.html#fit-resamples",
    "title": "MultiLR: Predictive models",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nvolcano_fit_rs1 &lt;- volcano_wflow1 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs1\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1 &lt;split [574/144]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n2 &lt;split [574/144]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n3 &lt;split [574/144]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n4 &lt;split [575/143]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n5 &lt;split [575/143]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;"
  },
  {
    "objectID": "slides/lec-25.html#collect-metrics",
    "href": "slides/lec-25.html#collect-metrics",
    "title": "MultiLR: Predictive models",
    "section": "Collect metrics",
    "text": "Collect metrics\n\ncollect_metrics(volcano_fit_rs1)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.596     5  0.0146 Preprocessor1_Model1\n2 roc_auc  hand_till  0.703     5  0.0244 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve",
    "href": "slides/lec-25.html#roc-curve",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve",
    "text": "ROC curve\nROC curves for multiclass outcomes use a one-vs-all approach: calculate multiple curves, one per level vs. all other levels.\n\nvolcano_fit_rs1 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve-output",
    "href": "slides/lec-25.html#roc-curve-output",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve",
    "text": "ROC curve"
  },
  {
    "objectID": "slides/lec-25.html#roc-curve---under-the-hood",
    "href": "slides/lec-25.html#roc-curve---under-the-hood",
    "title": "MultiLR: Predictive models",
    "section": "ROC curve - under the hood",
    "text": "ROC curve - under the hood\nAn additional column, .level, identifies the “one” column in the one-vs-all calculation:\n\nvolcano_fit_rs1 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  )\n\n# A tibble: 2,175 × 5\n# Groups:   id [5]\n   id    .level        .threshold specificity sensitivity\n   &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 Fold1 Stratovolcano  -Inf           0                1\n 2 Fold1 Stratovolcano     0.0621      0                1\n 3 Fold1 Stratovolcano     0.0786      0.0123           1\n 4 Fold1 Stratovolcano     0.0869      0.0247           1\n 5 Fold1 Stratovolcano     0.0957      0.0370           1\n 6 Fold1 Stratovolcano     0.104       0.0494           1\n 7 Fold1 Stratovolcano     0.104       0.0617           1\n 8 Fold1 Stratovolcano     0.105       0.0741           1\n 9 Fold1 Stratovolcano     0.105       0.0864           1\n10 Fold1 Stratovolcano     0.107       0.0988           1\n# … with 2,165 more rows"
  },
  {
    "objectID": "slides/lec-25.html#your-turn-1",
    "href": "slides/lec-25.html#your-turn-1",
    "title": "MultiLR: Predictive models",
    "section": "Your turn",
    "text": "Your turn\n\n📋 github.com/sta210-s22/ae-11-volcanoes - Exercise 2"
  },
  {
    "objectID": "slides/lec-25.html#acknowledgements",
    "href": "slides/lec-25.html#acknowledgements",
    "title": "MultiLR: Predictive models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nInspired by\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttps://juliasilge.com/blog/nber-papers/"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-professor",
    "href": "slides/lec-1.html#meet-the-professor",
    "title": "Welcome to STA 210!",
    "section": "Meet the professor",
    "text": "Meet the professor\n\n\n\n\n\nDr. Mine Çetinkaya-Rundel (she/her)\n\n\n\n\nProfessor of the Practice & Director of Undergraduate Studies, Department of Statistical Science\nAffiliated Faculty, Computational Media, Arts & Cultures\nFind out more at mine-cr.com"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-tas",
    "href": "slides/lec-1.html#meet-the-tas",
    "title": "Welcome to STA 210!",
    "section": "Meet the TAs",
    "text": "Meet the TAs\n\nMartha Aboagye (she/her, UG)\nRich Fremgen (he/him, MS)\nEmily Gentles (she/her, MS)\nSara Mehta (she/her, UG)\nRick Presman (he/him, PhD)\nShari Tian (she/her, UG)\nAaditya Warrier (he/him, UG)"
  },
  {
    "objectID": "slides/lec-1.html#check-out-conversations",
    "href": "slides/lec-1.html#check-out-conversations",
    "title": "Welcome to STA 210!",
    "section": "Check out Conversations",
    "text": "Check out Conversations\n\nGo to Conversations 💬\nAnswer the discussion question: How are you doing?"
  },
  {
    "objectID": "slides/lec-1.html#what-is-regression-analysis",
    "href": "slides/lec-1.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis",
    "text": "What is regression analysis\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or predictors). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lec-1.html#course-faq",
    "href": "slides/lec-1.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\n\nWhat background is assumed for the course? Introductory statistics or probability course.\nWill we be doing computing? Yes. We will use R.\nWill we learn the mathematical theory of regression? Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. The 1-credit course STA 211: Mathematics of Regression you can take simultaneously / after dives into more of the mathematics."
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nAssess whether a proposed model is appropriate and describe its limitations.\nUse Quarto to write reproducible reports and GitHub for version control and collaboration.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/lec-1.html#examples-of-regression-in-practice",
    "href": "slides/lec-1.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/lec-1.html#homepage",
    "href": "slides/lec-1.html#homepage",
    "title": "Welcome to STA 210!",
    "section": "Homepage",
    "text": "Homepage\nsta210-s22.github.io/website\n\nAll course materials\nLinks to Sakai, GitHub, RStudio containers, etc.\nLet’s take a tour!"
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nGitHub organization: github.com/sta210-s22\nRStudio containers: cmgr.oit.duke.edu/containers\nDiscussion forum: Conversations\nAssignment submission and feedback: Gradescope\n\n\n\n\n\n\n\nImportant\n\n\nReserve an RStudio Container (titled STA 210) before lab on Monday!"
  },
  {
    "objectID": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nLab assignments x 7 (first individual, later team-based)\nHomework assignments x 5 (individual)\nThree take-home exams\nTerm project presented during the final exam period"
  },
  {
    "objectID": "slides/lec-1.html#cadence",
    "href": "slides/lec-1.html#cadence",
    "title": "Welcome to STA 210!",
    "section": "Cadence",
    "text": "Cadence\n\nLabs: Start and make large progress on Monday in lab section, finish up by Friday 5pm of that week\nHWs: Posted Friday morning, due following Friday 5pm\nExams: Exam review Thursday in class, exam posted Friday morning, no lab on Monday of following week, due Monday 11:59pm\nProject: Deadlines throughout the semester, with some lab and lecture time dedicated to working on them, and most work done in teams outside of class"
  },
  {
    "objectID": "slides/lec-1.html#teams",
    "href": "slides/lec-1.html#teams",
    "title": "Welcome to STA 210!",
    "section": "Teams",
    "text": "Teams\n\nTeam assignments\n\nAssigned by me\nApplication exercises, labs, and project\nPeer evaluation during teamwork and after completion\n\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nApplication exercises\n3%\n\n\nHomework\n35% (7% x 5)\n\n\nProject\n15%\n\n\nLab\n14% (2% x 7)\n\n\nExam 01\n10%\n\n\nExam 02\n10%\n\n\nExam 03\n10%\n\n\nTeamwork\n3%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-1.html#support",
    "href": "slides/lec-1.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours\nAsk and answer questions on the discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/lec-1.html#announcements",
    "href": "slides/lec-1.html#announcements",
    "title": "Welcome to STA 210!",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Sakai (Announcements tool) and sent via email, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day\nI’ll (try my best to) send a weekly update announcement each Friday, outlining the plan for the following week and reminding you what you need to do to prepare, practice, and perform"
  },
  {
    "objectID": "slides/lec-1.html#diversity-inclusion",
    "href": "slides/lec-1.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know!\nPlease let me know your preferred pronouns. You’ll also be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-1.html#accessibility",
    "href": "slides/lec-1.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/lec-1.html#covid-policies",
    "href": "slides/lec-1.html#covid-policies",
    "title": "Welcome to STA 210!",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask at all times!\nRead and follow university guidance"
  },
  {
    "objectID": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it"
  },
  {
    "objectID": "slides/lec-1.html#collaboration-policy",
    "href": "slides/lec-1.html#collaboration-policy",
    "title": "Welcome to STA 210!",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/lec-1.html#sharing-reusing-code-policy",
    "href": "slides/lec-1.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 210!",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-1.html#academic-integrity",
    "href": "slides/lec-1.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/lec-1.html#most-importantly",
    "href": "slides/lec-1.html#most-importantly",
    "title": "Welcome to STA 210!",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class.\nAsk questions.\nDo the readings.\nDo the homework and lab.\nDon’t procrastinate and don’t let a week pass by with lingering questions."
  },
  {
    "objectID": "slides/lec-1.html#learning-during-a-pandemic",
    "href": "slides/lec-1.html#learning-during-a-pandemic",
    "title": "Welcome to STA 210!",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "slides/lec-1.html#this-weeks-tasks",
    "href": "slides/lec-1.html#this-weeks-tasks",
    "title": "Welcome to STA 210!",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nGet a GitHub account if you don’t have one (some advice for choosing a username here)\nComplete the Getting to know you survey if you haven’t yet done so!\nRead the syllabus\nWatch out for next week’s announcement email, in your inbox sometime tomorrow"
  },
  {
    "objectID": "slides/lec-1.html#midori-says",
    "href": "slides/lec-1.html#midori-says",
    "title": "Welcome to STA 210!",
    "section": "Midori says…",
    "text": "Midori says…"
  },
  {
    "objectID": "slides/lec-26.html#topics",
    "href": "slides/lec-26.html#topics",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Topics",
    "text": "Topics\n\nUnbalanced data\nChoosing the “final” model"
  },
  {
    "objectID": "slides/lec-26.html#computational-setup",
    "href": "slides/lec-26.html#computational-setup",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)\nlibrary(themis)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-26.html#volcanoes",
    "href": "slides/lec-26.html#volcanoes",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Volcanoes",
    "text": "Volcanoes\nThe data come from The Smithsonian Institution, via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"slides\", \"data/volcano.csv\"))\nnames(volcano)\n\n [1] \"volcano_number\"           \"volcano_name\"            \n [3] \"primary_volcano_type\"     \"last_eruption_year\"      \n [5] \"country\"                  \"region\"                  \n [7] \"subregion\"                \"latitude\"                \n [9] \"longitude\"                \"elevation\"               \n[11] \"tectonic_settings\"        \"evidence_category\"       \n[13] \"major_rock_1\"             \"major_rock_2\"            \n[15] \"major_rock_3\"             \"major_rock_4\"            \n[17] \"major_rock_5\"             \"minor_rock_1\"            \n[19] \"minor_rock_2\"             \"minor_rock_3\"            \n[21] \"minor_rock_4\"             \"minor_rock_5\"            \n[23] \"population_within_5_km\"   \"population_within_10_km\" \n[25] \"population_within_30_km\"  \"population_within_100_km\""
  },
  {
    "objectID": "slides/lec-26.html#data-prep",
    "href": "slides/lec-26.html#data-prep",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Data prep",
    "text": "Data prep\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "slides/lec-26.html#split-into-testingtraining",
    "href": "slides/lec-26.html#split-into-testingtraining",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Split into testing/training",
    "text": "Split into testing/training\n\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)"
  },
  {
    "objectID": "slides/lec-26.html#specify-a-model",
    "href": "slides/lec-26.html#specify-a-model",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Specify a model",
    "text": "Specify a model\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_spec\n\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "slides/lec-26.html#create-cross-validation-folds",
    "href": "slides/lec-26.html#create-cross-validation-folds",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Create cross validation folds",
    "text": "Create cross validation folds\n\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5"
  },
  {
    "objectID": "slides/lec-26.html#unbalanced-data-1",
    "href": "slides/lec-26.html#unbalanced-data-1",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Unbalanced data",
    "text": "Unbalanced data\nRemember that the observed volcano types are unbalanced:\n\nvolcano %&gt;% \n  count(volcano_type)\n\n# A tibble: 3 × 2\n  volcano_type      n\n  &lt;fct&gt;         &lt;int&gt;\n1 Stratovolcano   461\n2 Shield          118\n3 Other           379"
  },
  {
    "objectID": "slides/lec-26.html#addressing-unbalance",
    "href": "slides/lec-26.html#addressing-unbalance",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Addressing unbalance",
    "text": "Addressing unbalance\nTo address class unbalance, we generally use\n\n\noversampling data from levels that are less prevalent in the data\n\ne.g., step_smote(): Uses a technique called “Synthetic Minority Over-sampling Technique” to generate new examples of the minority class using nearest neighbors of these cases.\n\n\n\ndownsampling data from levels that are more prevalent in the data\n\ne.g., step_downsample(): Removes rows of a data set to make the occurrence of levels in a specific factor level equal."
  },
  {
    "objectID": "slides/lec-26.html#new-recipe---oversample",
    "href": "slides/lec-26.html#new-recipe---oversample",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "New recipe - oversample",
    "text": "New recipe - oversample\n\nvolcano_rec3 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors()) %&gt;%\n  step_smote(volcano_type)"
  },
  {
    "objectID": "slides/lec-26.html#new-recipe---downsample",
    "href": "slides/lec-26.html#new-recipe---downsample",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "New recipe - downsample",
    "text": "New recipe - downsample\n\nvolcano_rec4 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors()) %&gt;%\n  step_downsample(volcano_type)"
  },
  {
    "objectID": "slides/lec-26.html#new-workflows",
    "href": "slides/lec-26.html#new-workflows",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "New workflows",
    "text": "New workflows\n\nvolcano_wflow3 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec3) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow4 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec4) %&gt;%\n  add_model(volcano_spec)"
  },
  {
    "objectID": "slides/lec-26.html#fit-resamples",
    "href": "slides/lec-26.html#fit-resamples",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nvolcano_fit_rs3 &lt;- volcano_wflow3 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs4 &lt;- volcano_wflow4 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )"
  },
  {
    "objectID": "slides/lec-26.html#collect-metrics",
    "href": "slides/lec-26.html#collect-metrics",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Collect metrics",
    "text": "Collect metrics\n\ncollect_metrics(volcano_fit_rs3)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.517     5  0.0154 Preprocessor1_Model1\n2 roc_auc  hand_till  0.693     5  0.0270 Preprocessor1_Model1\n\ncollect_metrics(volcano_fit_rs4)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.485     5  0.0123 Preprocessor1_Model1\n2 roc_auc  hand_till  0.675     5  0.0219 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-26.html#roc-curves---oversampling",
    "href": "slides/lec-26.html#roc-curves---oversampling",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curves - oversampling",
    "text": "ROC curves - oversampling\n\nvolcano_fit_rs3 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#roc-curves---downsampling",
    "href": "slides/lec-26.html#roc-curves---downsampling",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curves - downsampling",
    "text": "ROC curves - downsampling\n\nvolcano_fit_rs4 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#addressing-unbalance-1",
    "href": "slides/lec-26.html#addressing-unbalance-1",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Addressing unbalance",
    "text": "Addressing unbalance\n\nCan you think of any issues resulting from over/down sampling?"
  },
  {
    "objectID": "slides/lec-26.html#the-chosen-model",
    "href": "slides/lec-26.html#the-chosen-model",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "The “chosen” model",
    "text": "The “chosen” model\nLet’s stick to the models without over/down sampling.\nFrom the application exercise:\n\nvolcano_rec2 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nvolcano_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec2) %&gt;%\n  add_model(volcano_spec)"
  },
  {
    "objectID": "slides/lec-26.html#fitting-the-final-model",
    "href": "slides/lec-26.html#fitting-the-final-model",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Fitting the final model",
    "text": "Fitting the final model\n\nfinal_fit &lt;- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass     0.629 Preprocessor1_Model1\n2 roc_auc  hand_till      0.734 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-26.html#confusion-matrix",
    "href": "slides/lec-26.html#confusion-matrix",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\ncollect_predictions(final_fit) %&gt;%\n  conf_mat(volcano_type, .pred_class)\n\n               Truth\nPrediction      Stratovolcano Shield Other\n  Stratovolcano            96     13    38\n  Shield                    1      0     0\n  Other                    21     16    55"
  },
  {
    "objectID": "slides/lec-26.html#confusion-matrix---visualized",
    "href": "slides/lec-26.html#confusion-matrix---visualized",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Confusion matrix - visualized",
    "text": "Confusion matrix - visualized\n\ncollect_predictions(final_fit) %&gt;%\n  conf_mat(volcano_type, .pred_class) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#roc-curve",
    "href": "slides/lec-26.html#roc-curve",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curve",
    "text": "ROC curve\n\ncollect_predictions(final_fit) %&gt;%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "slides/lec-26.html#roc-curve---altogether",
    "href": "slides/lec-26.html#roc-curve---altogether",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "ROC curve - altogether",
    "text": "ROC curve - altogether\n\n📋 github.com/sta210-s22/ae-11-volcanoes - Exercise 3"
  },
  {
    "objectID": "slides/lec-26.html#prediction",
    "href": "slides/lec-26.html#prediction",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Prediction",
    "text": "Prediction\n\nfinal_fitted &lt;- extract_workflow(final_fit)\n\nnew_volcano &lt;- tibble(\n  latitude = 35.9940,\n  longitude = -78.8986,\n  elevation = 404,\n  tectonic_settings = \"Subduction zone / Continental crust (&gt;25 km)\",\n  major_rock_1 = \"Andesite / Basaltic Andesite\"\n)\n\npredict(\n  final_fitted, \n  new_volcano, \n  type = \"prob\"\n  )\n\n# A tibble: 3 × 1\n  .pred_value\n        &lt;dbl&gt;\n1      0.381 \n2      0.0379\n3      0.581"
  },
  {
    "objectID": "slides/lec-26.html#acknowledgements",
    "href": "slides/lec-26.html#acknowledgements",
    "title": "MultiLR: Predictive models (cont.)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nInspired by\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttps://juliasilge.com/blog/nber-papers/"
  },
  {
    "objectID": "slides/lec-4.html#announcements",
    "href": "slides/lec-4.html#announcements",
    "title": "SLR: Prediction + model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nNew on the course website: FAQ\nNew communication tool: Slack\n\nFind the invite link in your inbox / on Sakai announcements\nUse #general for questions, #random for random 🤪\nUse code formatting for for questions involving code (see Course FAQ for a demo video)\n\nMy office hours: All virtual for now, hope to move 1 hour / week to in person later in the semester"
  },
  {
    "objectID": "slides/lec-4.html#hybrid-teaching",
    "href": "slides/lec-4.html#hybrid-teaching",
    "title": "SLR: Prediction + model evaluation",
    "section": "Hybrid teaching",
    "text": "Hybrid teaching\n\nLectures:\n\nIn person as long as university says so (and I don’t have COVID)\nIf you can’t be in class (and you’re well enough to follow along), watch live (or the recording later) on Panopto\nWatching live and have questions? Post on Slack!\nIn class and see someone ask a question on Slack? Please raise it to me!\n\nLabs:\n\nNot live streamed / recorded\nLab 2 (next Monday) - individual\nLab 3 onwards - in teams, if teammates are in isolation, set up team Zoom calls"
  },
  {
    "objectID": "slides/lec-4.html#computational-setup",
    "href": "slides/lec-4.html#computational-setup",
    "title": "SLR: Prediction + model evaluation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-4.html#data-source",
    "href": "slides/lec-4.html#data-source",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data source",
    "text": "Data source\n\nThe data come from usdata::county_2019\nThese data have been compiled from the 2019 American Community Survey"
  },
  {
    "objectID": "slides/lec-4.html#uninsurance-rate",
    "href": "slides/lec-4.html#uninsurance-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance rate",
    "text": "Uninsurance rate"
  },
  {
    "objectID": "slides/lec-4.html#high-school-graduation-rate",
    "href": "slides/lec-4.html#high-school-graduation-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "High school graduation rate",
    "text": "High school graduation rate"
  },
  {
    "objectID": "slides/lec-4.html#examining-the-relationship",
    "href": "slides/lec-4.html#examining-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Examining the relationship",
    "text": "Examining the relationship\n\nThe NC Labor and Economic Analysis Division (LEAD), which “administers and collects data, conducts research, and publishes information on the state’s economy, labor force, educational, and workforce-related issues”.\nSuppose that an analyst working for LEAD is interested in the relationship between uninsurance and high school graduation rates in NC counties.\n\n\n\nWhat type of visualization should the analyst make to examine the relationship between these two variables?"
  },
  {
    "objectID": "slides/lec-4.html#data-prep",
    "href": "slides/lec-4.html#data-prep",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data prep",
    "text": "Data prep\n\ncounty_2019_nc &lt;- county_2019 %&gt;%\n  as_tibble() %&gt;%\n  filter(state == \"North Carolina\") %&gt;%\n  select(name, hs_grad, uninsured)\n\ncounty_2019_nc\n\n# A tibble: 100 × 3\n   name             hs_grad uninsured\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n 1 Alamance County     86.3      11.2\n 2 Alexander County    82.4       8.9\n 3 Alleghany County    77.5      11.3\n 4 Anson County        80.7      11.1\n 5 Ashe County         85.1      12.6\n 6 Avery County        83.6      15.9\n 7 Beaufort County     87.7      12  \n 8 Bertie County       78.4      11.9\n 9 Bladen County       81.3      12.9\n10 Brunswick County    91.3       9.8\n# … with 90 more rows"
  },
  {
    "objectID": "slides/lec-4.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/lec-4.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance vs. HS graduation rates",
    "text": "Uninsurance vs. HS graduation rates\n\n\nCode\nggplot(county_2019_nc,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  ) +\n  geom_point(data = county_2019_nc %&gt;% filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured), shape = \"circle open\", color = \"#8F2D56\", size = 4, stroke = 2) +\n  geom_text(data = county_2019_nc %&gt;% filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured, label = name), color = \"#8F2D56\", fontface = \"bold\", nudge_y = 3, nudge_x = 2)"
  },
  {
    "objectID": "slides/lec-4.html#modeling-the-relationship",
    "href": "slides/lec-4.html#modeling-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Modeling the relationship",
    "text": "Modeling the relationship\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )"
  },
  {
    "objectID": "slides/lec-4.html#fitting-the-model",
    "href": "slides/lec-4.html#fitting-the-model",
    "title": "SLR: Prediction + model evaluation",
    "section": "Fitting the model",
    "text": "Fitting the model\nWith fit():\n\nnc_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/lec-4.html#augmenting-the-data",
    "href": "slides/lec-4.html#augmenting-the-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug &lt;- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 × 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# … with 90 more rows"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-i",
    "href": "slides/lec-4.html#visualizing-the-model-i",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model I",
    "text": "Visualizing the model I\n\n\n\nBlack circles: Observed values (y = uninsured)"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-ii",
    "href": "slides/lec-4.html#visualizing-the-model-ii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model II",
    "text": "Visualizing the model II\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-iii",
    "href": "slides/lec-4.html#visualizing-the-model-iii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model III",
    "text": "Visualizing the model III\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-model-iv",
    "href": "slides/lec-4.html#visualizing-the-model-iv",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model IV",
    "text": "Visualizing the model IV\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\nGray dashed lines: Residuals"
  },
  {
    "objectID": "slides/lec-4.html#evaluating-the-model-fit",
    "href": "slides/lec-4.html#evaluating-the-model-fit",
    "title": "SLR: Prediction + model evaluation",
    "section": "Evaluating the model fit",
    "text": "Evaluating the model fit\n\nHow can we evaluate whether the model for predicting uninsurance rate from high school graduation rate for NC counties is a good fit?"
  },
  {
    "objectID": "slides/lec-4.html#two-statistics",
    "href": "slides/lec-4.html#two-statistics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/lec-4.html#r-squared",
    "href": "slides/lec-4.html#r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "R-squared",
    "text": "R-squared\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nUnitless\nCalculate with rsq():\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.243"
  },
  {
    "objectID": "slides/lec-4.html#interpreting-r-squared",
    "href": "slides/lec-4.html#interpreting-r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "Interpreting R-squared",
    "text": "Interpreting R-squared\n\n🗳️ Vote on Slack\nThe \\(R^2\\) of the model for predicting uninsurance rate from high school graduation rate for NC counties is 24.3%. Which of the following is the correct interpretation of this value?\n\nHigh school graduation rates correctly predict 24.3% of uninsurance rates in NC counties.\n24.3% of the variability in uninsurance rates in NC counties can be explained by high school graduation rates.\n24.3% of the variability in high school graduation rates in NC counties can be explained by uninsurance rates.\n24.3% of the time uninsurance rates in NC counties can be predicted by high school graduation rates."
  },
  {
    "objectID": "slides/lec-4.html#alternative-approach-for-r-squared",
    "href": "slides/lec-4.html#alternative-approach-for-r-squared",
    "title": "SLR: Prediction + model evaluation",
    "section": "Alternative approach for R-squared",
    "text": "Alternative approach for R-squared\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(nc_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.243         0.235  2.09      31.5 0.000000188     1  -214.  435.  443.\n# … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(nc_fit)$r.squared\n\n[1] 0.2430694"
  },
  {
    "objectID": "slides/lec-4.html#rmse",
    "href": "slides/lec-4.html#rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the outcome variable\nCalculate with rmse():\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        2.07\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/lec-4.html#obtaining-r-squared-and-rmse",
    "href": "slides/lec-4.html#obtaining-r-squared-and-rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "Obtaining R-squared and RMSE",
    "text": "Obtaining R-squared and RMSE\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/lec-4.html#purpose-of-model-evaluation",
    "href": "slides/lec-4.html#purpose-of-model-evaluation",
    "title": "SLR: Prediction + model evaluation",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e. out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/lec-4.html#spending-our-data",
    "href": "slides/lec-4.html#spending-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/lec-4.html#simulation-data-splitting",
    "href": "slides/lec-4.html#simulation-data-splitting",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times"
  },
  {
    "objectID": "slides/lec-4.html#predictive-performance",
    "href": "slides/lec-4.html#predictive-performance",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?"
  },
  {
    "objectID": "slides/lec-4.html#bootstrapping-our-data",
    "href": "slides/lec-4.html#bootstrapping-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/lec-4.html#simulation-bootstrapping",
    "href": "slides/lec-4.html#simulation-bootstrapping",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\nTake a bootstrap sample – sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times"
  },
  {
    "objectID": "slides/lec-4.html#predictive-performance-1",
    "href": "slides/lec-4.html#predictive-performance-1",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?"
  },
  {
    "objectID": "slides/lec-23.html#topics",
    "href": "slides/lec-23.html#topics",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Topics",
    "text": "Topics\n\nIntroduce multinomial logistic regression\nInterpret model coefficients\nInference for a coefficient \\(\\beta_{jk}\\)"
  },
  {
    "objectID": "slides/lec-23.html#computational-setup",
    "href": "slides/lec-23.html#computational-setup",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES)\nlibrary(knitr)\nlibrary(patchwork)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-23.html#generalized-linear-models-glms",
    "href": "slides/lec-23.html#generalized-linear-models-glms",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Generalized Linear Models (GLMs)",
    "text": "Generalized Linear Models (GLMs)\n\nIn practice, there are many different types of outcome variables:\n\nBinary: Win or Lose\nNominal: Democrat, Republican or Third Party candidate\nOrdered: Movie rating (1 - 5 stars)\nand others…\n\nPredicting each of these outcomes requires a generalized linear model, a broader class of models that generalize the multiple linear regression model\n\n\n\n\n\n\n\nNote\n\n\nRecommended reading for more details about GLMs: Generalized Linear Models: A Unifying Theory."
  },
  {
    "objectID": "slides/lec-23.html#binary-outcome-logistic",
    "href": "slides/lec-23.html#binary-outcome-logistic",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Binary outcome (Logistic)",
    "text": "Binary outcome (Logistic)\n\nGiven \\(P(y_i=1|x_i)= \\hat{\\pi}_i\\hspace{5mm} \\text{ and } \\hspace{5mm}P(y_i=0|x_i) = 1-\\hat{\\pi}_i\\)\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_i}{1-\\hat{\\pi}_i}\\Big) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i}\n\\]\nWe can calculate \\(\\hat{\\pi}_i\\) by solving the logit equation:\n\\[\n\\hat{\\pi}_i = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i}}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i}}}\n\\]"
  },
  {
    "objectID": "slides/lec-23.html#binary-outcome-logistic-1",
    "href": "slides/lec-23.html#binary-outcome-logistic-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Binary outcome (Logistic)",
    "text": "Binary outcome (Logistic)\n\nSuppose we consider \\(y=0\\) the baseline category such that\n\\[\nP(y_i=1|x_i) = \\hat{\\pi}_{i1} \\hspace{2mm}  \\text{ and } \\hspace{2mm} P(y_i=0|x_i) = \\hat{\\pi}_{i0}\n\\]\nThen the logistic regression model is\n\\[\n\\log\\bigg(\\frac{\\hat{\\pi}_{i1}}{1- \\hat{\\pi}_{i1}}\\bigg) = \\log\\bigg(\\frac{\\hat{\\pi}_{i1}}{\\hat{\\pi}_{i0}}\\bigg) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\nSlope, \\(\\hat{\\beta}_1\\): When \\(x\\) increases by one unit, the odds of \\(y=1\\) versus the baseline \\(y=0\\) are expected to multiply by a factor of \\(e^{\\hat{\\beta}_1}\\)\nIntercept, \\(\\hat{\\beta}_0\\): When \\(x=0\\), the predicted odds of \\(y=1\\) versus the baseline \\(y=0\\) are \\(\\exp\\{\\hat{\\beta}_0\\}\\)"
  },
  {
    "objectID": "slides/lec-23.html#multinomial-outcome-variable",
    "href": "slides/lec-23.html#multinomial-outcome-variable",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Multinomial outcome variable",
    "text": "Multinomial outcome variable\n\nSuppose the outcome variable \\(y\\) is categorical and can take values \\(1, 2, \\ldots, K\\) such that \\((K &gt; 2)\\)\nMultinomial Distribution:\n\\[\nP(y=1) = \\pi_1, P(y=2) = \\pi_2, \\ldots, P(y=K) = \\pi_K\n\\]\nsuch that \\(\\sum\\limits_{k=1}^{K} \\pi_k = 1\\)"
  },
  {
    "objectID": "slides/lec-23.html#multinomial-logistic-regression",
    "href": "slides/lec-23.html#multinomial-logistic-regression",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\n\nIf we have an explanatory variable \\(x\\), then we want to fit a model such that \\(P(y = k) = \\pi_k\\) is a function of \\(x\\)\nChoose a baseline category. Let’s choose \\(y=1\\). Then,\n\\[\n\\log\\bigg(\\frac{\\pi_{ik}}{\\pi_{i1}}\\bigg) = \\beta_{0k} + \\beta_{1k} x_i\n\\]\nIn the multinomial logistic model, we have a separate equation for each category of the outcome relative to the baseline category\n\nIf the outcome has \\(K\\) possible categories, there will be \\(K-1\\) equations as part of the multinomial logistic model"
  },
  {
    "objectID": "slides/lec-23.html#multinomial-logistic-regression-1",
    "href": "slides/lec-23.html#multinomial-logistic-regression-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\n\nSuppose we have a outcome variable \\(y\\) that can take three possible outcomes that are coded as “A”, “B”, “C”\nLet “A” be the baseline category. Then\n\\[\n\\log\\bigg(\\frac{\\pi_{iB}}{\\pi_{iA}}\\bigg) = \\beta_{0B} + \\beta_{1B}x_i \\\\[10pt]\n\\log\\bigg(\\frac{\\pi_{iC}}{\\pi_{iA}}\\bigg) = \\beta_{0C} + \\beta_{1C} x_i\n\\]"
  },
  {
    "objectID": "slides/lec-23.html#nhanes-data",
    "href": "slides/lec-23.html#nhanes-data",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "NHANES Data",
    "text": "NHANES Data\n\nNational Health and Nutrition Examination Survey is conducted by the National Center for Health Statistics (NCHS)\nThe goal is to “assess the health and nutritional status of adults and children in the United States”\nThis survey includes an interview and a physical examination"
  },
  {
    "objectID": "slides/lec-23.html#nhanes-data-1",
    "href": "slides/lec-23.html#nhanes-data-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "NHANES Data",
    "text": "NHANES Data\n\nWe will use the data from the NHANES R package\nContains 75 variables for the 2009 - 2010 and 2011 - 2012 sample years\nThe data in this package is modified for educational purposes and should not be used for research\nOriginal data can be obtained from the NCHS website for research purposes\nType ?NHANES in console to see list of variables and definitions"
  },
  {
    "objectID": "slides/lec-23.html#variables",
    "href": "slides/lec-23.html#variables",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Variables",
    "text": "Variables\nGoal: Use a person’s age and whether they do regular physical activity to predict their self-reported health rating.\n\nOutcome: HealthGen: Self-reported rating of participant’s health in general. Excellent, Vgood, Good, Fair, or Poor.\nPredictors:\n\nAge:Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities."
  },
  {
    "objectID": "slides/lec-23.html#the-data",
    "href": "slides/lec-23.html#the-data",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "The data",
    "text": "The data\n\nnhanes_adult &lt;- NHANES %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  select(HealthGen, Age, PhysActive) %&gt;%\n  drop_na() %&gt;%\n  mutate(obs_num = 1:n())\n\n\n\nglimpse(nhanes_adult)\n\nRows: 6,710\nColumns: 4\n$ HealthGen  &lt;fct&gt; Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, …\n$ Age        &lt;int&gt; 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,…\n$ PhysActive &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, …\n$ obs_num    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …"
  },
  {
    "objectID": "slides/lec-23.html#exploratory-data-analysis",
    "href": "slides/lec-23.html#exploratory-data-analysis",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis"
  },
  {
    "objectID": "slides/lec-23.html#exploratory-data-analysis-1",
    "href": "slides/lec-23.html#exploratory-data-analysis-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis"
  },
  {
    "objectID": "slides/lec-23.html#model-in-r",
    "href": "slides/lec-23.html#model-in-r",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Model in R",
    "text": "Model in R\nUse the multinom_reg() function with the \"nnet\" engine:\n\nhealth_fit &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-23.html#model-result",
    "href": "slides/lec-23.html#model-result",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Model result",
    "text": "Model result\n\nhealth_fit\n\nparsnip model object\n\nFit time:  113ms \nCall:\nnnet::multinom(formula = HealthGen ~ Age + PhysActive, data = data, \n    trace = FALSE)\n\nCoefficients:\n      (Intercept)           Age PhysActiveYes\nVgood   1.2053460  0.0009101848    -0.3209047\nGood    1.9476261 -0.0023686122    -1.0014925\nFair    0.9145492  0.0030462534    -1.6454297\nPoor   -1.5211414  0.0221905681    -2.6556343\n\nResidual Deviance: 17588.88 \nAIC: 17612.88"
  },
  {
    "objectID": "slides/lec-23.html#next-steps",
    "href": "slides/lec-23.html#next-steps",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Next steps",
    "text": "Next steps\n\nWhat function do we use to get the model summary, i.e., coefficient estimates.\n\n\n\ntidy(health_fit)\n\nError in model.frame.default(formula = HealthGen ~ Age + PhysActive, data = data): 'data' must be a data.frame, environment, or list"
  },
  {
    "objectID": "slides/lec-23.html#looking-inside-the-result-of-fit",
    "href": "slides/lec-23.html#looking-inside-the-result-of-fit",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Looking inside the result of fit()",
    "text": "Looking inside the result of fit()\n\nWhat is the name of the dataset in the call? Is it right?\n\n\nhealth_fit$fit$call\n\nnnet::multinom(formula = HealthGen ~ Age + PhysActive, data = data, \n    trace = FALSE)"
  },
  {
    "objectID": "slides/lec-23.html#repair-and-get-back-on-track",
    "href": "slides/lec-23.html#repair-and-get-back-on-track",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Repair, and get back on track",
    "text": "Repair, and get back on track\n\nhealth_fit &lt;- repair_call(health_fit, data = nhanes_adult)\nhealth_fit$fit$call\n\nnnet::multinom(formula = HealthGen ~ Age + PhysActive, data = nhanes_adult, \n    trace = FALSE)\n\ntidy(health_fit)\n\n# A tibble: 12 × 6\n   y.level term           estimate std.error statistic  p.value\n   &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Vgood   (Intercept)    1.21       0.145       8.33  8.42e-17\n 2 Vgood   Age            0.000910   0.00246     0.369 7.12e- 1\n 3 Vgood   PhysActiveYes -0.321      0.0929     -3.45  5.51e- 4\n 4 Good    (Intercept)    1.95       0.141      13.8   1.39e-43\n 5 Good    Age           -0.00237    0.00242    -0.977 3.29e- 1\n 6 Good    PhysActiveYes -1.00       0.0901    -11.1   1.00e-28\n 7 Fair    (Intercept)    0.915      0.164       5.57  2.61e- 8\n 8 Fair    Age            0.00305    0.00288     1.06  2.90e- 1\n 9 Fair    PhysActiveYes -1.65       0.107     -15.3   5.69e-53\n10 Poor    (Intercept)   -1.52       0.290      -5.24  1.62e- 7\n11 Poor    Age            0.0222     0.00491     4.52  6.11e- 6\n12 Poor    PhysActiveYes -2.66       0.236     -11.3   1.75e-29"
  },
  {
    "objectID": "slides/lec-23.html#model-output",
    "href": "slides/lec-23.html#model-output",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Model output",
    "text": "Model output\n\ntidy(health_fit)\n\n# A tibble: 12 × 6\n   y.level term           estimate std.error statistic  p.value\n   &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Vgood   (Intercept)    1.21       0.145       8.33  8.42e-17\n 2 Vgood   Age            0.000910   0.00246     0.369 7.12e- 1\n 3 Vgood   PhysActiveYes -0.321      0.0929     -3.45  5.51e- 4\n 4 Good    (Intercept)    1.95       0.141      13.8   1.39e-43\n 5 Good    Age           -0.00237    0.00242    -0.977 3.29e- 1\n 6 Good    PhysActiveYes -1.00       0.0901    -11.1   1.00e-28\n 7 Fair    (Intercept)    0.915      0.164       5.57  2.61e- 8\n 8 Fair    Age            0.00305    0.00288     1.06  2.90e- 1\n 9 Fair    PhysActiveYes -1.65       0.107     -15.3   5.69e-53\n10 Poor    (Intercept)   -1.52       0.290      -5.24  1.62e- 7\n11 Poor    Age            0.0222     0.00491     4.52  6.11e- 6\n12 Poor    PhysActiveYes -2.66       0.236     -11.3   1.75e-29"
  },
  {
    "objectID": "slides/lec-23.html#model-output-with-ci",
    "href": "slides/lec-23.html#model-output-with-ci",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Model output, with CI",
    "text": "Model output, with CI\n\ntidy(health_fit, conf.int = TRUE)\n\n# A tibble: 12 × 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Vgood   (Intercept)   1.21e+0   0.145       8.33  8.42e-17  0.922     1.49   \n 2 Vgood   Age           9.10e-4   0.00246     0.369 7.12e- 1 -0.00392   0.00574\n 3 Vgood   PhysActiveY… -3.21e-1   0.0929     -3.45  5.51e- 4 -0.503    -0.139  \n 4 Good    (Intercept)   1.95e+0   0.141      13.8   1.39e-43  1.67      2.22   \n 5 Good    Age          -2.37e-3   0.00242    -0.977 3.29e- 1 -0.00712   0.00238\n 6 Good    PhysActiveY… -1.00e+0   0.0901    -11.1   1.00e-28 -1.18     -0.825  \n 7 Fair    (Intercept)   9.15e-1   0.164       5.57  2.61e- 8  0.592     1.24   \n 8 Fair    Age           3.05e-3   0.00288     1.06  2.90e- 1 -0.00260   0.00869\n 9 Fair    PhysActiveY… -1.65e+0   0.107     -15.3   5.69e-53 -1.86     -1.43   \n10 Poor    (Intercept)  -1.52e+0   0.290      -5.24  1.62e- 7 -2.09     -0.952  \n11 Poor    Age           2.22e-2   0.00491     4.52  6.11e- 6  0.0126    0.0318 \n12 Poor    PhysActiveY… -2.66e+0   0.236     -11.3   1.75e-29 -3.12     -2.19"
  },
  {
    "objectID": "slides/lec-23.html#model-output-with-ci-1",
    "href": "slides/lec-23.html#model-output-with-ci-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Model output, with CI",
    "text": "Model output, with CI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nVgood\n(Intercept)\n1.205\n0.145\n8.325\n0.000\n0.922\n1.489\n\n\nVgood\nAge\n0.001\n0.002\n0.369\n0.712\n-0.004\n0.006\n\n\nVgood\nPhysActiveYes\n-0.321\n0.093\n-3.454\n0.001\n-0.503\n-0.139\n\n\nGood\n(Intercept)\n1.948\n0.141\n13.844\n0.000\n1.672\n2.223\n\n\nGood\nAge\n-0.002\n0.002\n-0.977\n0.329\n-0.007\n0.002\n\n\nGood\nPhysActiveYes\n-1.001\n0.090\n-11.120\n0.000\n-1.178\n-0.825\n\n\nFair\n(Intercept)\n0.915\n0.164\n5.566\n0.000\n0.592\n1.237\n\n\nFair\nAge\n0.003\n0.003\n1.058\n0.290\n-0.003\n0.009\n\n\nFair\nPhysActiveYes\n-1.645\n0.107\n-15.319\n0.000\n-1.856\n-1.435\n\n\nPoor\n(Intercept)\n-1.521\n0.290\n-5.238\n0.000\n-2.090\n-0.952\n\n\nPoor\nAge\n0.022\n0.005\n4.522\n0.000\n0.013\n0.032\n\n\nPoor\nPhysActiveYes\n-2.656\n0.236\n-11.275\n0.000\n-3.117\n-2.194"
  },
  {
    "objectID": "slides/lec-23.html#fair-vs.-excellent-health",
    "href": "slides/lec-23.html#fair-vs.-excellent-health",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Fair vs. Excellent Health",
    "text": "Fair vs. Excellent Health\nThe baseline category for the model is Excellent.\n\nThe model equation for the log-odds a person rates themselves as having “Fair” health vs. “Excellent” is\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_{Fair}}{\\hat{\\pi}_{Excellent}}\\Big) = 0.915  + 0.003 ~ \\text{age} - 1.645 ~ \\text{PhysActive}\n\\]"
  },
  {
    "objectID": "slides/lec-23.html#interpretations",
    "href": "slides/lec-23.html#interpretations",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Interpretations",
    "text": "Interpretations\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_{Fair}}{\\hat{\\pi}_{Excellent}}\\Big) = 0.915  + 0.003 ~ \\text{age} - 1.645 ~ \\text{PhysActive}\n\\]\nFor each additional year in age, the odds a person rates themselves as having fair health versus excellent health are expected to multiply by 1.003 (exp(0.003)), holding physical activity constant.\n\nThe odds a person who does physical activity will rate themselves as having fair health versus excellent health are expected to be 0.193 (exp(-1.645)) times the odds for a person who doesn’t do physical activity, holding age constant."
  },
  {
    "objectID": "slides/lec-23.html#interpretations-1",
    "href": "slides/lec-23.html#interpretations-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Interpretations",
    "text": "Interpretations\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_{Fair}}{\\hat{\\pi}_{Excellent}}\\Big) = 0.915  + 0.003 ~ \\text{age} - 1.645 ~ \\text{PhysActive}\n\\]\nThe odds a 0 year old person who doesn’t do physical activity rates themselves as having fair health vs. excellent health are 2.497 (exp(0.915)).\n\n⚠️ Need to mean-center age for the intercept to have a meaningful interpretation!"
  },
  {
    "objectID": "slides/lec-23.html#hypothesis-test-for-beta_jk",
    "href": "slides/lec-23.html#hypothesis-test-for-beta_jk",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Hypothesis test for \\(\\beta_{jk}\\)",
    "text": "Hypothesis test for \\(\\beta_{jk}\\)\nThe test of significance for the coefficient \\(\\beta_{jk}\\) is\nHypotheses: \\(H_0: \\beta_{jk} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{jk} \\neq 0\\)\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_{jk} - 0}{SE(\\hat{\\beta_{jk}})}\\]\nP-value: \\(P(|Z| &gt; |z|)\\),\nwhere \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/lec-23.html#confidence-interval-for-beta_jk",
    "href": "slides/lec-23.html#confidence-interval-for-beta_jk",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Confidence interval for \\(\\beta_{jk}\\)",
    "text": "Confidence interval for \\(\\beta_{jk}\\)\n\nWe can calculate the C% confidence interval for \\(\\beta_{jk}\\) using \\(\\hat{\\beta}_{jk} \\pm z^* SE(\\hat{\\beta}_{jk})\\), where \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution.\nWe are \\(C\\%\\) confident that for every one unit change in \\(x_{j}\\), the odds of \\(y = k\\) versus the baseline will multiply by a factor of \\(\\exp\\{\\hat{\\beta}_{jk} - z^* SE(\\hat{\\beta}_{jk})\\}\\) to \\(\\exp\\{\\hat{\\beta}_{jk} + z^* SE(\\hat{\\beta}_{jk})\\}\\), holding all else constant."
  },
  {
    "objectID": "slides/lec-23.html#interpreting-cis-for-beta_jk",
    "href": "slides/lec-23.html#interpreting-cis-for-beta_jk",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Interpreting CIs for \\(\\beta_{jk}\\)",
    "text": "Interpreting CIs for \\(\\beta_{jk}\\)\n\ntidy(health_fit, conf.int = TRUE) %&gt;%\n  filter(y.level == \"Fair\") %&gt;%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nFair\n(Intercept)\n0.915\n0.164\n5.566\n0.00\n0.592\n1.237\n\n\nFair\nAge\n0.003\n0.003\n1.058\n0.29\n-0.003\n0.009\n\n\nFair\nPhysActiveYes\n-1.645\n0.107\n-15.319\n0.00\n-1.856\n-1.435\n\n\n\n\n\n\n\nWe are 95% confident, that for each additional year in age, the odds a person rates themselves as having fair health versus excellent health will multiply by 0.997 (exp(-0.003)) to 1.009 (exp(0.009)) , holding physical activity constant."
  },
  {
    "objectID": "slides/lec-23.html#interpreting-cis-for-beta_jk-1",
    "href": "slides/lec-23.html#interpreting-cis-for-beta_jk-1",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Interpreting CIs for \\(\\beta_{jk}\\)",
    "text": "Interpreting CIs for \\(\\beta_{jk}\\)\n\ntidy(health_fit, conf.int = TRUE) %&gt;%\n  filter(y.level == \"Fair\") %&gt;%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nFair\n(Intercept)\n0.915\n0.164\n5.566\n0.00\n0.592\n1.237\n\n\nFair\nAge\n0.003\n0.003\n1.058\n0.29\n-0.003\n0.009\n\n\nFair\nPhysActiveYes\n-1.645\n0.107\n-15.319\n0.00\n-1.856\n-1.435\n\n\n\n\n\n\nWe are 95% confident that the odds a person who does physical activity will rate themselves as having fair health versus excellent health are 0.156 (exp(-1.856 )) to 0.238 (exp(-1.435)) times the odds for a person who doesn’t do physical activity, holding age constant."
  },
  {
    "objectID": "slides/lec-23.html#recap",
    "href": "slides/lec-23.html#recap",
    "title": "Multinomial Logistic Regression (MultiLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduce multinomial logistic regression\nInterpret model coefficients\nInference for a coefficient \\(\\beta_{jk}\\)"
  },
  {
    "objectID": "slides/lab-1.html#what-to-expect-in-lab",
    "href": "slides/lab-1.html#what-to-expect-in-lab",
    "title": "Lab 1 - Meet the toolkit",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nWork on the lab assignment (individual at first, but in teams for the rest of the semester)\nLab instructions posted on the course website.\nStart each lab by finding your assignment repo in the course GitHub organization\n\nThis is where you will find the Quarto document and data to get started"
  },
  {
    "objectID": "slides/lab-1.html#tips",
    "href": "slides/lab-1.html#tips",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Tips",
    "text": "Tips\n\nYou do not have to finish the lab in class, they will always be due the following Friday at 5:00 pm. One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nWhen working in teams (later in the semester) do not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-1.html#check-do-you-have-the-lab-1-repo",
    "href": "slides/lab-1.html#check-do-you-have-the-lab-1-repo",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Check: Do you have the lab-1 repo?",
    "text": "Check: Do you have the lab-1 repo?\n\nGo to the GitHub course organization: https://github.com/sta210-s22\nYou should see a repo with the prefix lab-1- followed by your GitHub username\nIf you do not have this repo, please let your TAs know!"
  },
  {
    "objectID": "slides/lab-1.html#demo",
    "href": "slides/lab-1.html#demo",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Demo",
    "text": "Demo\nFollow along as your TA demonstrates the following:\n\nConfigure Git using SSH\nClone RStudio repo and start new project\nRender document and produce PDF\nUpdate name in YAML\n\nRender, commit, push changes to GitHub\nSee updates in your GitHub repo"
  },
  {
    "objectID": "slides/lab-1.html#when-youre-done-with-lab",
    "href": "slides/lab-1.html#when-youre-done-with-lab",
    "title": "Lab 1 - Meet the toolkit",
    "section": "When you’re done with lab",
    "text": "When you’re done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\nSubmit the PDF of your responses to Gradescope\n\nYou can access Gradescope through Sakai or the course website\nLogin using your Duke NetID credentials"
  },
  {
    "objectID": "slides/lab-1.html#youre-now-ready-to-complete-the-rest-of-lab",
    "href": "slides/lab-1.html#youre-now-ready-to-complete-the-rest-of-lab",
    "title": "Lab 1 - Meet the toolkit",
    "section": "You’re now ready to complete the rest of lab!",
    "text": "You’re now ready to complete the rest of lab!\nPlease “raise your hand” if you need help as you work on the lab"
  },
  {
    "objectID": "slides/lec-7.html#computational-setup",
    "href": "slides/lec-7.html#computational-setup",
    "title": "SLR: Model diagnostics",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-7.html#the-regression-model-revisited",
    "href": "slides/lec-7.html#the-regression-model-revisited",
    "title": "SLR: Model diagnostics",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/lec-7.html#ht-for-the-slope",
    "href": "slides/lec-7.html#ht-for-the-slope",
    "title": "SLR: Model diagnostics",
    "section": "HT for the slope",
    "text": "HT for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/lec-7.html#ht-test-statistic",
    "href": "slides/lec-7.html#ht-test-statistic",
    "title": "SLR: Model diagnostics",
    "section": "HT: Test statistic",
    "text": "HT: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\\[\nt = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{159.48 - 0}{18.17} = 8.78\n\\]"
  },
  {
    "objectID": "slides/lec-7.html#ht-p-value",
    "href": "slides/lec-7.html#ht-p-value",
    "title": "SLR: Model diagnostics",
    "section": "HT: p-value",
    "text": "HT: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/lec-7.html#understanding-the-p-value",
    "href": "slides/lec-7.html#understanding-the-p-value",
    "title": "SLR: Model diagnostics",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/lec-7.html#ht-conclusion-in-context",
    "href": "slides/lec-7.html#ht-conclusion-in-context",
    "title": "SLR: Model diagnostics",
    "section": "HT: Conclusion, in context",
    "text": "HT: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest."
  },
  {
    "objectID": "slides/lec-7.html#ci-for-the-slope",
    "href": "slides/lec-7.html#ci-for-the-slope",
    "title": "SLR: Model diagnostics",
    "section": "CI for the slope",
    "text": "CI for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/lec-7.html#ci-critical-value",
    "href": "slides/lec-7.html#ci-critical-value",
    "title": "SLR: Model diagnostics",
    "section": "CI: Critical value",
    "text": "CI: Critical value\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(duke_forest) - 2)\n\n[1] 1.984984\n\n# confidence level: 90%\nqt(0.95, df = nrow(duke_forest) - 2)\n\n[1] 1.660881\n\n# confidence level: 99%\nqt(0.995, df = nrow(duke_forest) - 2)\n\n[1] 2.628016"
  },
  {
    "objectID": "slides/lec-7.html#ci-for-the-slope-calculation",
    "href": "slides/lec-7.html#ci-for-the-slope-calculation",
    "title": "SLR: Model diagnostics",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 159.48 \\hspace{15mm} t^* = 1.98 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 18.17\\]\n\n\\[\n159.48 \\pm 1.98 \\times 18.17 = (123.50, 195.46)\n\\]"
  },
  {
    "objectID": "slides/lec-7.html#ci-for-the-slope-computation",
    "href": "slides/lec-7.html#ci-for-the-slope-computation",
    "title": "SLR: Model diagnostics",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(df_fit, conf.int = TRUE, conf.level = 0.95) %&gt;% \n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n10847.77\n222456.88\n\n\narea\n159.48\n18.17\n8.78\n0.00\n123.41\n195.55"
  },
  {
    "objectID": "slides/lec-7.html#confidence-interval-for-predictions",
    "href": "slides/lec-7.html#confidence-interval-for-predictions",
    "title": "SLR: Model diagnostics",
    "section": "Confidence interval for predictions",
    "text": "Confidence interval for predictions\n\nSuppose we want to answer the question “What is the predicted sale price of a Duke Forest house that is 2,800 square feet?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/lec-7.html#two-types-of-predictions",
    "href": "slides/lec-7.html#two-types-of-predictions",
    "title": "SLR: Model diagnostics",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: ““What is the average predicted sale price of Duke Forest houses that are 2,800 square feet?”\nPrediction for an individual observation: “What is the predicted sale price of a Duke Forest house that is 2,800 square feet?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/lec-7.html#uncertainty-in-predictions",
    "href": "slides/lec-7.html#uncertainty-in-predictions",
    "title": "SLR: Model diagnostics",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}\\]\n\nPrediction interval for an individual observation: \\[\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}\\]"
  },
  {
    "objectID": "slides/lec-7.html#standard-errors",
    "href": "slides/lec-7.html#standard-errors",
    "title": "SLR: Model diagnostics",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/lec-7.html#standard-errors-1",
    "href": "slides/lec-7.html#standard-errors-1",
    "title": "SLR: Model diagnostics",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/lec-7.html#confidence-interval",
    "href": "slides/lec-7.html#confidence-interval",
    "title": "SLR: Model diagnostics",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcpme:\n\nnew_house &lt;- tibble(area = 2800)\n\npredict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     529351.     597060.\n\n\n\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between $529,351 and $597,060."
  },
  {
    "objectID": "slides/lec-7.html#prediction-interval",
    "href": "slides/lec-7.html#prediction-interval",
    "title": "SLR: Model diagnostics",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction intervalfor the individual outcome:\n\npredict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     226438.     899973.\n\n\n\nWe are 95% confident that predicted sale price of a Duke Forest house that is 2,800 square feet is between $226,438 and $899,973."
  },
  {
    "objectID": "slides/lec-7.html#comparing-intervals",
    "href": "slides/lec-7.html#comparing-intervals",
    "title": "SLR: Model diagnostics",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/lec-7.html#extrapolation",
    "href": "slides/lec-7.html#extrapolation",
    "title": "SLR: Model diagnostics",
    "section": "Extrapolation",
    "text": "Extrapolation\n\n\n\nCalculate the prediction interval for the sale price of a “tiny house” in Duke Forest that is 225 square feet.\n\n\n\n\n\n\n\n\n\nNo, thanks!"
  },
  {
    "objectID": "slides/lec-7.html#model-conditions-1",
    "href": "slides/lec-7.html#model-conditions-1",
    "title": "SLR: Model diagnostics",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable, i.e. the errors are homeoscedastic\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/lec-7.html#linearity",
    "href": "slides/lec-7.html#linearity",
    "title": "SLR: Model diagnostics",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should not show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/lec-7.html#residuals-vs.-fitted-values",
    "href": "slides/lec-7.html#residuals-vs.-fitted-values",
    "title": "SLR: Model diagnostics",
    "section": "Residuals vs. fitted values",
    "text": "Residuals vs. fitted values\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/lec-7.html#application-exercise",
    "href": "slides/lec-7.html#application-exercise",
    "title": "SLR: Model diagnostics",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-3-duke-forest\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-7.html#non-linear-relationships",
    "href": "slides/lec-7.html#non-linear-relationships",
    "title": "SLR: Model diagnostics",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/lec-7.html#constant-variance",
    "href": "slides/lec-7.html#constant-variance",
    "title": "SLR: Model diagnostics",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals should be relatively constant across the plot"
  },
  {
    "objectID": "slides/lec-7.html#non-constant-variance",
    "href": "slides/lec-7.html#non-constant-variance",
    "title": "SLR: Model diagnostics",
    "section": "Non-constant variance",
    "text": "Non-constant variance"
  },
  {
    "objectID": "slides/lec-7.html#normality",
    "href": "slides/lec-7.html#normality",
    "title": "SLR: Model diagnostics",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/lec-7.html#independence",
    "href": "slides/lec-7.html#independence",
    "title": "SLR: Model diagnostics",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected\n\n\n✅ If this is a random sample of Duke Houses, the error for one house does not tell us anything about the error for another use"
  },
  {
    "objectID": "slides/lec-7.html#recap",
    "href": "slides/lec-7.html#recap",
    "title": "SLR: Model diagnostics",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\nLinearity\nConstant variance\n\n\n\nNormality\nIndependence\n\n\n\n\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-20.html#topics",
    "href": "slides/lec-20.html#topics",
    "title": "LR: Prediction / classification",
    "section": "Topics",
    "text": "Topics\n\nBulding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/lec-20.html#computational-setup",
    "href": "slides/lec-20.html#computational-setup",
    "title": "LR: Prediction / classification",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-20.html#openintroemail",
    "href": "slides/lec-20.html#openintroemail",
    "title": "LR: Prediction / classification",
    "section": "openintro::email",
    "text": "openintro::email\nThese data represent incoming emails for the first three months of 2012 for an email account.\n\nOutcome: spam - Indicator for whether the email was spam.\nPredictors: spam, `to_multiple, from, cc, sent_email, time, image, attach, dollar, winner, inherit, viagra, password, num_char, line_breaks, format, re_subj, exclaim_subj, urgent_subj, exclaim_mess, number.\n\nSee here for more detailed information on the variables."
  },
  {
    "objectID": "slides/lec-20.html#training-and-testing-split",
    "href": "slides/lec-20.html#training-and-testing-split",
    "title": "LR: Prediction / classification",
    "section": "Training and testing split",
    "text": "Training and testing split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1116)\n\n# Put 75% of the data into the training set \nemail_split &lt;- initial_split(email)\n\n# Create data frames for the two sets\nemail_train &lt;- training(email_split)\nemail_test  &lt;- testing(email_split)"
  },
  {
    "objectID": "slides/lec-20.html#exploratory-analysis",
    "href": "slides/lec-20.html#exploratory-analysis",
    "title": "LR: Prediction / classification",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nThe sample is unbalanced with respect to spam."
  },
  {
    "objectID": "slides/lec-20.html#reminder-modeling-workflow",
    "href": "slides/lec-20.html#reminder-modeling-workflow",
    "title": "LR: Prediction / classification",
    "section": "Reminder: Modeling workflow",
    "text": "Reminder: Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/lec-20.html#initiate-a-recipe",
    "href": "slides/lec-20.html#initiate-a-recipe",
    "title": "LR: Prediction / classification",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec &lt;- recipe(\n  spam ~ .,          # formula\n  data = email_train  # data to use for cataloging names and types of variables\n  )\nsummary(email_rec)\n\n\n\n# A tibble: 21 × 4\n   variable     type    role      source  \n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   \n 1 to_multiple  nominal predictor original\n 2 from         nominal predictor original\n 3 cc           numeric predictor original\n 4 sent_email   nominal predictor original\n 5 time         date    predictor original\n 6 image        numeric predictor original\n 7 attach       numeric predictor original\n 8 dollar       numeric predictor original\n 9 winner       nominal predictor original\n10 inherit      numeric predictor original\n11 viagra       numeric predictor original\n12 password     numeric predictor original\n13 num_char     numeric predictor original\n14 line_breaks  numeric predictor original\n15 format       nominal predictor original\n16 re_subj      nominal predictor original\n17 exclaim_subj numeric predictor original\n18 urgent_subj  nominal predictor original\n19 exclaim_mess numeric predictor original\n20 number       nominal predictor original\n21 spam         nominal outcome   original"
  },
  {
    "objectID": "slides/lec-20.html#remove-certain-variables",
    "href": "slides/lec-20.html#remove-certain-variables",
    "title": "LR: Prediction / classification",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec &lt;- email_rec %&gt;%\n  step_rm(from, sent_email)\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nDelete terms from, sent_email"
  },
  {
    "objectID": "slides/lec-20.html#feature-engineer-date",
    "href": "slides/lec-20.html#feature-engineer-date",
    "title": "LR: Prediction / classification",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nemail_rec &lt;- email_rec %&gt;%\n  step_date(time, features = c(\"dow\", \"month\")) %&gt;%\n  step_rm(time)\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nDelete terms from, sent_email\nDate features from time\nDelete terms time"
  },
  {
    "objectID": "slides/lec-20.html#discretize-numeric-variables",
    "href": "slides/lec-20.html#discretize-numeric-variables",
    "title": "LR: Prediction / classification",
    "section": "Discretize numeric variables",
    "text": "Discretize numeric variables\n\nemail_rec &lt;- email_rec %&gt;%\n  step_cut(cc, attach, dollar, breaks = c(0, 1))\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nDelete terms from, sent_email\nDate features from time\nDelete terms time\nCut numeric for cc, attach, dollar"
  },
  {
    "objectID": "slides/lec-20.html#create-dummy-variables",
    "href": "slides/lec-20.html#create-dummy-variables",
    "title": "LR: Prediction / classification",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nemail_rec &lt;- email_rec %&gt;%\n  step_dummy(all_nominal(), -all_outcomes())\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nDelete terms from, sent_email\nDate features from time\nDelete terms time\nCut numeric for cc, attach, dollar\nDummy variables from all_nominal(), -all_outcomes()"
  },
  {
    "objectID": "slides/lec-20.html#remove-zero-variance-variables",
    "href": "slides/lec-20.html#remove-zero-variance-variables",
    "title": "LR: Prediction / classification",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value\n\nemail_rec &lt;- email_rec %&gt;%\n  step_zv(all_predictors())\n\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nDelete terms from, sent_email\nDate features from time\nDelete terms time\nCut numeric for cc, attach, dollar\nDummy variables from all_nominal(), -all_outcomes()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-20.html#all-in-one-place",
    "href": "slides/lec-20.html#all-in-one-place",
    "title": "LR: Prediction / classification",
    "section": "All in one place",
    "text": "All in one place\n\nemail_rec &lt;- recipe(spam ~ ., data = email_train) %&gt;%\n  step_rm(from, sent_email) %&gt;%\n  step_date(time, features = c(\"dow\", \"month\")) %&gt;%               \n  step_rm(time) %&gt;%\n  step_cut(cc, attach, dollar, breaks = c(0, 1)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/lec-20.html#define-model",
    "href": "slides/lec-20.html#define-model",
    "title": "LR: Prediction / classification",
    "section": "Define model",
    "text": "Define model\n\nemail_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\nemail_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/lec-20.html#define-workflow",
    "href": "slides/lec-20.html#define-workflow",
    "title": "LR: Prediction / classification",
    "section": "Define workflow",
    "text": "Define workflow\nRemember: Workflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow &lt;- workflow() %&gt;% \n  add_model(email_spec) %&gt;% \n  add_recipe(email_rec)\n\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_date()\n• step_rm()\n• step_cut()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/lec-20.html#fit-model-to-training-data",
    "href": "slides/lec-20.html#fit-model-to-training-data",
    "title": "LR: Prediction / classification",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit &lt;- email_wflow %&gt;% \n  fit(data = email_train)\n\ntidy(email_fit) %&gt;% print(n = 31)\n\n# A tibble: 27 × 5\n   term           estimate std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)    -0.867     0.259     -3.34   8.32e- 4\n 2 image          -1.72      0.941     -1.83   6.78e- 2\n 3 inherit         0.359     0.179      2.01   4.48e- 2\n 4 viagra          1.90     40.6        0.0469 9.63e- 1\n 5 password       -0.951     0.405     -2.35   1.88e- 2\n 6 num_char        0.0475    0.0246     1.93   5.35e- 2\n 7 line_breaks    -0.00499   0.00140   -3.55   3.78e- 4\n 8 exclaim_subj   -0.196     0.287     -0.682  4.95e- 1\n 9 exclaim_mess    0.00845   0.00188    4.49   6.99e- 6\n10 to_multiple_X1 -2.65      0.370     -7.17   7.78e-13\n11 cc_X.1.68.     -0.350     0.518     -0.676  4.99e- 1\n12 attach_X.1.21.  2.17      0.399      5.44   5.19e- 8\n13 dollar_X.1.64.  0.122     0.230      0.529  5.97e- 1\n14 winner_yes      2.25      0.438      5.14   2.79e- 7\n15 format_X1      -0.945     0.165     -5.71   1.10e- 8\n16 re_subj_X1     -2.96      0.463     -6.39   1.61e-10\n17 urgent_subj_X1  4.77      1.26       3.79   1.51e- 4\n18 number_small   -0.928     0.173     -5.36   8.48e- 8\n19 number_big     -0.190     0.256     -0.740  4.59e- 1\n20 time_dow_Mon    0.116     0.307      0.379  7.05e- 1\n21 time_dow_Tue    0.394     0.279      1.41   1.58e- 1\n22 time_dow_Wed   -0.175     0.285     -0.613  5.40e- 1\n23 time_dow_Thu    0.134     0.288      0.467  6.41e- 1\n24 time_dow_Fri    0.101     0.288      0.352  7.25e- 1\n25 time_dow_Sat    0.308     0.310      0.995  3.20e- 1\n26 time_month_Feb  0.767     0.187      4.11   3.99e- 5\n27 time_month_Mar  0.524     0.186      2.82   4.81e- 3"
  },
  {
    "objectID": "slides/lec-20.html#make-predictions-for-test-data",
    "href": "slides/lec-20.html#make-predictions-for-test-data",
    "title": "LR: Prediction / classification",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred &lt;- predict(email_fit, email_test, type = \"prob\") %&gt;% \n  bind_cols(email_test) \nemail_pred\n\n# A tibble: 981 × 23\n   .pred_0  .pred_1 spam  to_multiple from     cc sent_email time               \n     &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;             \n 1   0.962 0.0376   0     0           1         0 0          2012-01-01 02:03:59\n 2   0.995 0.00461  0     1           1         0 1          2012-01-01 12:55:06\n 3   0.999 0.00127  0     0           1         1 1          2012-01-01 14:38:32\n 4   0.997 0.00281  0     0           1         2 0          2012-01-01 18:32:53\n 5   0.987 0.0128   0     0           1         0 0          2012-01-02 00:42:16\n 6   0.999 0.000886 0     0           1         1 0          2012-01-02 10:12:51\n 7   0.994 0.00633  0     0           1         4 0          2012-01-02 11:45:36\n 8   0.851 0.149    0     0           1         0 0          2012-01-02 16:55:03\n 9   0.968 0.0318   0     0           1         0 0          2012-01-02 20:07:17\n10   0.997 0.00277  0     0           1         0 1          2012-01-02 23:34:50\n# … with 971 more rows, and 15 more variables: image &lt;dbl&gt;, attach &lt;dbl&gt;,\n#   dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, password &lt;dbl&gt;,\n#   num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;fct&gt;, re_subj &lt;fct&gt;,\n#   exclaim_subj &lt;dbl&gt;, urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;, number &lt;fct&gt;"
  },
  {
    "objectID": "slides/lec-20.html#a-closer-look-at-predictions",
    "href": "slides/lec-20.html#a-closer-look-at-predictions",
    "title": "LR: Prediction / classification",
    "section": "A closer look at predictions",
    "text": "A closer look at predictions\n\nWhich of the following 10 emails will be misclassified?\n\n\nemail_pred %&gt;%\n  arrange(desc(.pred_1)) %&gt;%\n  select(contains(\"pred\"), spam)\n\n# A tibble: 981 × 3\n   .pred_0 .pred_1 spam \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;\n 1  0.0223   0.978 1    \n 2  0.107    0.893 0    \n 3  0.109    0.891 1    \n 4  0.140    0.860 1    \n 5  0.149    0.851 0    \n 6  0.163    0.837 1    \n 7  0.197    0.803 0    \n 8  0.207    0.793 0    \n 9  0.235    0.765 1    \n10  0.240    0.760 1    \n# … with 971 more rows"
  },
  {
    "objectID": "slides/lec-20.html#false-positive-and-negative",
    "href": "slides/lec-20.html#false-positive-and-negative",
    "title": "LR: Prediction / classification",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\nFalse negative rate = P(classified as not spam | Email spam) = FN / (TP + FN)\nFalse positive rate = P(classified as spam | Email not spam) = FP / (FP + TN)"
  },
  {
    "objectID": "slides/lec-20.html#sensitivity-and-specificity-1",
    "href": "slides/lec-20.html#sensitivity-and-specificity-1",
    "title": "LR: Prediction / classification",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\nSensitivity = P(classified as spam | Email spam) = TP / (TP + FN)\n\nSensitivity = 1 − False negative rate\n\nSpecificity = P(classified as not spam | Email not spam) = TN / (FP + TN)\n\nSpecificity = 1 − False positive rate\n\n\n\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "slides/lec-20.html#evaluate-the-performance",
    "href": "slides/lec-20.html#evaluate-the-performance",
    "title": "LR: Prediction / classification",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve+ which plot true positive rate vs. false positive rate (1 - specificity).\n\n\n\n\nemail_pred %&gt;%\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n+ Originally developed for operators of military radar receivers, hence the name."
  },
  {
    "objectID": "slides/lec-20.html#roc-curve-under-the-hood",
    "href": "slides/lec-20.html#roc-curve-under-the-hood",
    "title": "LR: Prediction / classification",
    "section": "ROC curve, under the hood",
    "text": "ROC curve, under the hood\n\nemail_pred %&gt;%\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 981 × 3\n    .threshold specificity sensitivity\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf            0                 1\n 2    1.31e-13     0                 1\n 3    2.63e- 8     0.00111           1\n 4    5.75e- 6     0.00223           1\n 5    1.36e- 5     0.00334           1\n 6    2.33e- 5     0.00446           1\n 7    2.74e- 5     0.00557           1\n 8    3.28e- 5     0.00669           1\n 9    4.59e- 5     0.00780           1\n10    4.78e- 5     0.00892           1\n# … with 971 more rows"
  },
  {
    "objectID": "slides/lec-20.html#roc-curve",
    "href": "slides/lec-20.html#roc-curve",
    "title": "LR: Prediction / classification",
    "section": "ROC curve",
    "text": "ROC curve"
  },
  {
    "objectID": "slides/lec-20.html#evaluate-the-performance-1",
    "href": "slides/lec-20.html#evaluate-the-performance-1",
    "title": "LR: Prediction / classification",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred %&gt;%\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.850"
  },
  {
    "objectID": "slides/lec-20.html#cutoff-probability-0.5",
    "href": "slides/lec-20.html#cutoff-probability-0.5",
    "title": "LR: Prediction / classification",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n883\n73\n\n\nEmail classified as spam\n14\n11\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.5\nemail_pred %&gt;%\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) %&gt;%\n  count(spam_pred, spam) %&gt;%\n  pivot_wider(names_from = spam, values_from = n) %&gt;%\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/lec-20.html#confusion-matrix",
    "href": "slides/lec-20.html#confusion-matrix",
    "title": "LR: Prediction / classification",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nCross-tabulation of observed and predicted classes:\n\nemail_pred %&gt;%\n  mutate(spam_predicted = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0))) %&gt;%\n  conf_mat(truth = spam, estimate = spam_predicted)\n\n          Truth\nPrediction   0   1\n         0 883  73\n         1  14  11"
  },
  {
    "objectID": "slides/lec-20.html#classification",
    "href": "slides/lec-20.html#classification",
    "title": "LR: Prediction / classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/lec-20.html#cutoff-probability-0.25",
    "href": "slides/lec-20.html#cutoff-probability-0.25",
    "title": "LR: Prediction / classification",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n826\n42\n\n\nEmail classified as spam\n71\n42\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.25\nemail_pred %&gt;%\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) %&gt;%\n  count(spam_pred, spam) %&gt;%\n  pivot_wider(names_from = spam, values_from = n) %&gt;%\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/lec-20.html#classification-1",
    "href": "slides/lec-20.html#classification-1",
    "title": "LR: Prediction / classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/lec-20.html#cutoff-probability-0.75",
    "href": "slides/lec-20.html#cutoff-probability-0.75",
    "title": "LR: Prediction / classification",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n893\n78\n\n\nEmail classified as spam\n4\n6\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.75\nemail_pred %&gt;%\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) %&gt;%\n  count(spam_pred, spam) %&gt;%\n  pivot_wider(names_from = spam, values_from = n) %&gt;%\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/lec-20.html#classification-2",
    "href": "slides/lec-20.html#classification-2",
    "title": "LR: Prediction / classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/lab-2.html#what-to-expect-in-lab",
    "href": "slides/lab-2.html#what-to-expect-in-lab",
    "title": "Lab 1 - Meet the toolkit",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nWork on the lab assignment (individual at first, but in teams for the rest of the semester)\nLab instructions posted on the course website.\nStart each lab by finding your assignment repo in the course GitHub organization\n\nThis is where you will find the Quarto document and data to get started"
  },
  {
    "objectID": "slides/lab-2.html#tips",
    "href": "slides/lab-2.html#tips",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Tips",
    "text": "Tips\n\nYou do not have to finish the lab in class, they will always be due the following Friday at 5:00 pm. One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nWhen working in teams (later in the semester) do not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-2.html#check-do-you-have-the-lab-1-repo",
    "href": "slides/lab-2.html#check-do-you-have-the-lab-1-repo",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Check: Do you have the lab-1 repo?",
    "text": "Check: Do you have the lab-1 repo?\n\nGo to the GitHub course organization: https://github.com/sta210-s22\nYou should see a repo with the prefix lab-1- followed by your GitHub username\nIf you do not have this repo, please let your TAs know!"
  },
  {
    "objectID": "slides/lab-2.html#demo",
    "href": "slides/lab-2.html#demo",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Demo",
    "text": "Demo\nFollow along as your TA demonstrates the following:\n\nConfigure Git using SSH\nClone RStudio repo and start new project\nRender document and produce PDF\nUpdate name in YAML\n\nRender, commit, push changes to GitHub\nSee updates in your GitHub repo"
  },
  {
    "objectID": "slides/lab-2.html#when-youre-done-with-lab",
    "href": "slides/lab-2.html#when-youre-done-with-lab",
    "title": "Lab 1 - Meet the toolkit",
    "section": "When you’re done with lab",
    "text": "When you’re done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\nSubmit the PDF of your responses to Gradescope\n\nYou can access Gradescope through Sakai or the course website\nLogin using your Duke NetID credentials"
  },
  {
    "objectID": "slides/lab-2.html#youre-now-ready-to-complete-the-rest-of-lab",
    "href": "slides/lab-2.html#youre-now-ready-to-complete-the-rest-of-lab",
    "title": "Lab 1 - Meet the toolkit",
    "section": "You’re now ready to complete the rest of lab!",
    "text": "You’re now ready to complete the rest of lab!\nPlease “raise your hand” if you need help as you work on the lab"
  },
  {
    "objectID": "slides/lec-10.html#announcements",
    "href": "slides/lec-10.html#announcements",
    "title": "Types of predictors",
    "section": "Announcements",
    "text": "Announcements\n\nCongratulations on finishing Exam 1!\nGrading of AEs\nQuestions on feedback vs. regrades"
  },
  {
    "objectID": "slides/lec-10.html#topics",
    "href": "slides/lec-10.html#topics",
    "title": "Types of predictors",
    "section": "Topics",
    "text": "Topics\n\nMean-centering quantitative predictors\nUsing indicator variables for categorical predictors\nUsing interaction terms"
  },
  {
    "objectID": "slides/lec-10.html#computational-setup",
    "href": "slides/lec-10.html#computational-setup",
    "title": "Types of predictors",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-10.html#data-peer-to-peer-lender",
    "href": "slides/lec-10.html#data-peer-to-peer-lender",
    "title": "Types of predictors",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# … with 40 more rows"
  },
  {
    "objectID": "slides/lec-10.html#variables",
    "href": "slides/lec-10.html#variables",
    "title": "Types of predictors",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/lec-10.html#outcome-interest_rate",
    "href": "slides/lec-10.html#outcome-interest_rate",
    "title": "Types of predictors",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\n\n\n\n\n5.31\n9.93\n26.3"
  },
  {
    "objectID": "slides/lec-10.html#predictors",
    "href": "slides/lec-10.html#predictors",
    "title": "Types of predictors",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-1-rescale-income",
    "href": "slides/lec-10.html#data-manipulation-1-rescale-income",
    "title": "Types of predictors",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 %&gt;%\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\")"
  },
  {
    "objectID": "slides/lec-10.html#outcome-vs.-predictors",
    "href": "slides/lec-10.html#outcome-vs.-predictors",
    "title": "Types of predictors",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/lec-10.html#fit-regression-model",
    "href": "slides/lec-10.html#fit-regression-model",
    "title": "Types of predictors",
    "section": "Fit regression model",
    "text": "Fit regression model\n\nint_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(interest_rate ~ debt_to_income + verified_income  + annual_income_th,\n      data = loan50)"
  },
  {
    "objectID": "slides/lec-10.html#summarize-model-results",
    "href": "slides/lec-10.html#summarize-model-results",
    "title": "Types of predictors",
    "section": "Summarize model results",
    "text": "Summarize model results\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/lec-10.html#mean-centering",
    "href": "slides/lec-10.html#mean-centering",
    "title": "Types of predictors",
    "section": "Mean-centering",
    "text": "Mean-centering\nIf we are interested in interpreting the intercept, we can mean-center the quantitative predictors in the model.\nWe can mean-center a quantitative predictor \\(X_j\\) using the following:\n\\[X_{j_{Cent}} = X_{j}- \\bar{X}_{j}\\]\n\nIf we mean-center all quantitative variables, then the intercept is interpreted as the expected value of the response variable when all quantitative variables are at their mean value."
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/lec-10.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Types of predictors",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 %&gt;%\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/lec-10.html#visualize-mean-centered-predictors",
    "href": "slides/lec-10.html#visualize-mean-centered-predictors",
    "title": "Types of predictors",
    "section": "Visualize mean-centered predictors",
    "text": "Visualize mean-centered predictors"
  },
  {
    "objectID": "slides/lec-10.html#using-mean-centered-variables-in-the-model",
    "href": "slides/lec-10.html#using-mean-centered-variables-in-the-model",
    "title": "Types of predictors",
    "section": "Using mean-centered variables in the model",
    "text": "Using mean-centered variables in the model\n\nHow do you expect the model to change if we use the debt_inc_cent and annual_income_cent in the model?\n\n\n\n\n# A tibble: 5 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             9.44      0.977      9.66  1.50e-12   7.48    11.4    \n2 debt_inc_cent           0.671     0.676      0.993 3.26e- 1  -0.690    2.03   \n3 verified_incomeSourc…   2.21      1.40       1.58  1.21e- 1  -0.606    5.03   \n4 verified_incomeVerif…   6.88      1.80       3.82  4.06e- 4   3.25    10.5    \n5 annual_income_th_cent  -0.0205    0.0114    -1.80  7.79e- 2  -0.0434   0.00238"
  },
  {
    "objectID": "slides/lec-10.html#original-vs.-mean-centered-model",
    "href": "slides/lec-10.html#original-vs.-mean-centered-model",
    "title": "Types of predictors",
    "section": "Original vs. mean-centered model",
    "text": "Original vs. mean-centered model\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n10.726\n\n\ndebt_to_income\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th\n-0.021\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n9.444\n\n\ndebt_inc_cent\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th_cent\n-0.021"
  },
  {
    "objectID": "slides/lec-10.html#indicator-variables-1",
    "href": "slides/lec-10.html#indicator-variables-1",
    "title": "Types of predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/lec-10.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Types of predictors",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 %&gt;%\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 × 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/lec-10.html#indicators-in-the-model",
    "href": "slides/lec-10.html#indicators-in-the-model",
    "title": "Types of predictors",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\n\n\n\n\n# A tibble: 3 × 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0"
  },
  {
    "objectID": "slides/lec-10.html#interpreting-verified_income",
    "href": "slides/lec-10.html#interpreting-verified_income",
    "title": "Types of predictors",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\nThe baseline category is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\nPeople with verified income are expected to take a loan with an interest rate that is 6.880% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant."
  },
  {
    "objectID": "slides/lec-10.html#interaction-terms-1",
    "href": "slides/lec-10.html#interaction-terms-1",
    "title": "Types of predictors",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/lec-10.html#interest-rate-vs.-annual-income",
    "href": "slides/lec-10.html#interest-rate-vs.-annual-income",
    "title": "Types of predictors",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/lec-10.html#interaction-term-in-model",
    "href": "slides/lec-10.html#interaction-term-in-model",
    "title": "Types of predictors",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_cent_int_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(interest_rate ~ debt_inc_cent  +  debt_inc_cent + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent:verified_incomeSource Verified\n-0.016\n0.026\n-0.643\n0.523\n\n\nannual_income_th_cent:verified_incomeVerified\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/lec-10.html#interpreting-interaction-terms",
    "href": "slides/lec-10.html#interpreting-interaction-terms",
    "title": "Types of predictors",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/lec-10.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/lec-10.html#data-manipulation-4-create-interaction-variables",
    "title": "Types of predictors",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, …\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.…\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,…\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, …\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, …\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, …\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,…\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0…\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11…"
  },
  {
    "objectID": "slides/lec-10.html#recap",
    "href": "slides/lec-10.html#recap",
    "title": "Types of predictors",
    "section": "Recap",
    "text": "Recap\n\nMean-centering quantitative predictors\nUsing indicator variables for categorical predictors\nUsing interaction terms"
  },
  {
    "objectID": "slides/lec-10.html#looking-backward",
    "href": "slides/lec-10.html#looking-backward",
    "title": "Types of predictors",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):\n\nloan50 %&gt;%\n  select(interest_rate, annual_income, debt_to_income, verified_income) %&gt;%\n  mutate(\n    # 1. rescale income\n    annual_income_th = annual_income / 1000,\n    # 2. mean-center quantitative predictors\n    debt_inc_cent = debt_to_income - mean(debt_to_income),\n    annual_income_th_cent = annual_income_th - mean(annual_income_th),\n    # 3. create dummy variables for verified_income\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0),\n    # 4. create interaction variables\n    `annual_income_th_cent:verified_incomeSource Verified` = annual_income_th_cent * source_verified,\n    `annual_income_th_cent:verified_incomeVerified` = annual_income_th_cent * verified\n  )"
  },
  {
    "objectID": "slides/lec-10.html#looking-forward",
    "href": "slides/lec-10.html#looking-forward",
    "title": "Types of predictors",
    "section": "Looking forward",
    "text": "Looking forward\nFeature engineering, with recipes (from tidymodels):\n\nloan_rec &lt;- recipe( ~ ., data = loan50) %&gt;%\n  # 1. rescale income\n  step_mutate(annual_income_th = annual_income / 1000) %&gt;%\n  # 2. mean-center quantitative predictors\n  step_center(all_numeric_predictors()) %&gt;%\n  # 3. create dummy variables for verified_income\n  step_dummy(verified_income) %&gt;%\n  # 4. create interaction variables\n  step_interact(terms = ~ annual_income_th:verified_income)"
  },
  {
    "objectID": "slides/lec-10.html#recipe",
    "href": "slides/lec-10.html#recipe",
    "title": "Types of predictors",
    "section": "Recipe",
    "text": "Recipe\n\nloan_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n predictor         24\n\nOperations:\n\nVariable mutation\nCentering for all_numeric_predictors()\nDummy variables from verified_income\nInteractions with annual_income_th:verified_income"
  },
  {
    "objectID": "slides/lec-12.html#announcements",
    "href": "slides/lec-12.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nCheck Sakai Gradebook to make sure all scores so far are accurate\nAny questions on topic selection for projects?\nAny feedback on time of my office hours?"
  },
  {
    "objectID": "slides/lec-12.html#midterm-evaluation-summary",
    "href": "slides/lec-12.html#midterm-evaluation-summary",
    "title": "Feature engineering",
    "section": "Midterm evaluation summary",
    "text": "Midterm evaluation summary\nLive analysis…"
  },
  {
    "objectID": "slides/lec-12.html#topics",
    "href": "slides/lec-12.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nReview: Training and testing splits\nFeature engineering with recipes"
  },
  {
    "objectID": "slides/lec-12.html#computational-setup",
    "href": "slides/lec-12.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-12.html#the-office",
    "href": "slides/lec-12.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/lec-12.html#data",
    "href": "slides/lec-12.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings &lt;- read_csv(here::here(\"slides\", \"data/office_ratings.csv\"))\noffice_ratings\n\n# A tibble: 188 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# … with 178 more rows"
  },
  {
    "objectID": "slides/lec-12.html#imdb-ratings",
    "href": "slides/lec-12.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/lec-12.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/lec-12.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. number of votes",
    "text": "IMDB ratings vs. number of votes"
  },
  {
    "objectID": "slides/lec-12.html#outliers",
    "href": "slides/lec-12.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/lec-12.html#aside",
    "href": "slides/lec-12.html#aside",
    "title": "Feature engineering",
    "section": "Aside…",
    "text": "Aside…\nIf you like the Dinner Party episode, I highly recommend this “oral history” of the episode published on Rolling Stone magazine."
  },
  {
    "objectID": "slides/lec-12.html#rating-vs.-air-date",
    "href": "slides/lec-12.html#rating-vs.-air-date",
    "title": "Feature engineering",
    "section": "Rating vs. air date",
    "text": "Rating vs. air date"
  },
  {
    "objectID": "slides/lec-12.html#imdb-ratings-vs.-seasons",
    "href": "slides/lec-12.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. seasons",
    "text": "IMDB ratings vs. seasons"
  },
  {
    "objectID": "slides/lec-12.html#train-test",
    "href": "slides/lec-12.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\n\n\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/lec-12.html#training-data",
    "href": "slides/lec-12.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 × 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# … with 131 more rows"
  },
  {
    "objectID": "slides/lec-12.html#feature-engineering",
    "href": "slides/lec-12.html#feature-engineering",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "slides/lec-12.html#feature-engineering-with-dplyr",
    "href": "slides/lec-12.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\noffice_train %&gt;%\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 × 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 8           18 Last Day in Flo…         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev…         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb…         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# … with 135 more rows\n\n\n\n\nCan you identify any potential problems with this approach?"
  },
  {
    "objectID": "slides/lec-12.html#modeling-workflow-revisited",
    "href": "slides/lec-12.html#modeling-workflow-revisited",
    "title": "Feature engineering",
    "section": "Modeling workflow, revisited",
    "text": "Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/lec-12.html#initiate-a-recipe",
    "href": "slides/lec-12.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5"
  },
  {
    "objectID": "slides/lec-12.html#step-1-alter-roles",
    "href": "slides/lec-12.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn’t a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec %&gt;%\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4"
  },
  {
    "objectID": "slides/lec-12.html#step-2-add-features",
    "href": "slides/lec-12.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec %&gt;%\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date"
  },
  {
    "objectID": "slides/lec-12.html#step-3-add-more-features",
    "href": "slides/lec-12.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec %&gt;%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date"
  },
  {
    "objectID": "slides/lec-12.html#step-4-convert-numbers-to-factors",
    "href": "slides/lec-12.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec %&gt;%\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season"
  },
  {
    "objectID": "slides/lec-12.html#step-5-make-dummy-variables",
    "href": "slides/lec-12.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec %&gt;%\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()"
  },
  {
    "objectID": "slides/lec-12.html#step-6-remove-zero-variance-pred.s",
    "href": "slides/lec-12.html#step-6-remove-zero-variance-pred.s",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance pred.s",
    "text": "Step 6: Remove zero variance pred.s\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec %&gt;%\n  step_zv(all_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-12.html#putting-it-altogether",
    "href": "slides/lec-12.html#putting-it-altogether",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  # make title's role ID\n  update_role(title, new_role = \"ID\") %&gt;%\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) %&gt;%\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %&gt;%\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) %&gt;%\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/lec-12.html#putting-it-altogether-1",
    "href": "slides/lec-12.html#putting-it-altogether-1",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-12.html#recap",
    "href": "slides/lec-12.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\nReview: Training and testing splits\nFeature engineering with recipes"
  },
  {
    "objectID": "slides/lec-9.html#announcements",
    "href": "slides/lec-9.html#announcements",
    "title": "Exam 1 Review",
    "section": "Announcements",
    "text": "Announcements\n\nLab 3 due tomorrow (Friday) at 5pm on Gradescope, one submission per team\nExam 1 open tomorrow (Friday) at 9am and ends on Monday, Feb 7 at 11:59pm\nNo lab on Monday, take the time to work on your exam"
  },
  {
    "objectID": "slides/lec-9.html#feedback-from-submissions-so-far",
    "href": "slides/lec-9.html#feedback-from-submissions-so-far",
    "title": "Exam 1 Review",
    "section": "Feedback from submissions so far",
    "text": "Feedback from submissions so far\n\nYou must submit a PDF (not HTML) to Gradescope\nYou must tag your pages when you upload to Gradescope – if you don’t know how to do this, please ask well before the deadline!\nYou must not refer to keys distributed in previous semesters of the course – much of what we’re doing is different and some of it is the same. If you need help, please ask!"
  },
  {
    "objectID": "slides/lec-9.html#exam-1",
    "href": "slides/lec-9.html#exam-1",
    "title": "Exam 1 Review",
    "section": "Exam 1",
    "text": "Exam 1\n\nInstructions can be found at\nCovers everything we’ve done so far\nOffice hours:\n\nTomorrow’s TA office hours are only for Lab 3 questions\nMonday: I have office hours 10:30-11:30am\n\nAny clarification questions for the exam?\n\nDirect message me or Rick on Slack\nPost on Sakai Conversations, post to “Instructors in this site”"
  },
  {
    "objectID": "slides/lec-9.html#application-exercise",
    "href": "slides/lec-9.html#application-exercise",
    "title": "Exam 1 Review",
    "section": "Application Exercise",
    "text": "Application Exercise\n\n📋 github.com/sta210-s22/ae-4-exam-1-review"
  },
  {
    "objectID": "slides/lec-16.html#topics",
    "href": "slides/lec-16.html#topics",
    "title": "MLR: Inference",
    "section": "Topics",
    "text": "Topics\n\nConduct a hypothesis test for \\(\\beta_j\\)\nCalculate a confidence interval for \\(\\beta_j\\)\nInference pitfalls"
  },
  {
    "objectID": "slides/lec-16.html#computational-setup",
    "href": "slides/lec-16.html#computational-setup",
    "title": "MLR: Inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)      # for tables\nlibrary(patchwork)  # for laying out plots\nlibrary(rms)        # for vif\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-16.html#data-rail_trail",
    "href": "slides/lec-16.html#data-rail_trail",
    "title": "MLR: Inference",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\nrail_trail &lt;- read_csv(here::here(\"slides\", \"data/rail_trail.csv\"))\nrail_trail\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# … with 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/lec-16.html#variables",
    "href": "slides/lec-16.html#variables",
    "title": "MLR: Inference",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of “Fall”, “Spring”, or “Summer”\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of “weekday” or “weekend”"
  },
  {
    "objectID": "slides/lec-16.html#review-simple-linear-regression-slr",
    "href": "slides/lec-16.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\nggplot(rail_trail, aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"High temp (F)\", y = \"Number of riders\")"
  },
  {
    "objectID": "slides/lec-16.html#slr-model-summary",
    "href": "slides/lec-16.html#slr-model-summary",
    "title": "MLR: Inference",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -17.1     59.4      -0.288 0.774        \n2 hightemp        5.70     0.848     6.72  0.00000000171"
  },
  {
    "objectID": "slides/lec-16.html#slr-hypothesis-test",
    "href": "slides/lec-16.html#slr-hypothesis-test",
    "title": "MLR: Inference",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -17.1     59.4      -0.288 0.774        \n2 hightemp        5.70     0.848     6.72  0.00000000171\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) and \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.72\\) with a degrees of freedom of 88, and a p-value &lt; 0.0001.\n\n\n\n\nState the conclusion: With a small p-value, we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders."
  },
  {
    "objectID": "slides/lec-16.html#multiple-linear-regression",
    "href": "slides/lec-16.html#multiple-linear-regression",
    "title": "MLR: Inference",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111"
  },
  {
    "objectID": "slides/lec-16.html#mlr-hypothesis-test-hightemp",
    "href": "slides/lec-16.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) and \\(H_A: \\beta_{hightemp} \\ne 0\\), given weekday is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\) with a degrees of freedom of 86, and a p-value &lt; 0.0001.\n\n\n\n\nState the conclusion: With such a small p-value, the data provides strong evidence against \\(H_0\\), i.e., the data provide strong evidence that high temperature for the day is a helpful predictor in a model that already contains whether the given day is a weekday as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/lec-16.html#the-model-for-season-spring",
    "href": "slides/lec-16.html#the-model-for-season-spring",
    "title": "MLR: Inference",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-16.html#the-model-for-season-summer",
    "href": "slides/lec-16.html#the-model-for-season-summer",
    "title": "MLR: Inference",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-16.html#the-model-for-season-fall",
    "href": "slides/lec-16.html#the-model-for-season-fall",
    "title": "MLR: Inference",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-16.html#the-models",
    "href": "slides/lec-16.html#the-models",
    "title": "MLR: Inference",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/lec-16.html#application-exercise",
    "href": "slides/lec-16.html#application-exercise",
    "title": "MLR: Inference",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-8-rail-trail\n\n\n\n\nEx 1. Recreate the following visualization in R based on the results of the model."
  },
  {
    "objectID": "slides/lec-16.html#application-exercise-1",
    "href": "slides/lec-16.html#application-exercise-1",
    "title": "MLR: Inference",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-8-rail-trail\n\n\nEx 2. Add an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n\n\n\n10:00"
  },
  {
    "objectID": "slides/lec-16.html#confidence-interval-for-beta_j-1",
    "href": "slides/lec-16.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically, we are \\(C%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context, we are \\(C%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant."
  },
  {
    "objectID": "slides/lec-16.html#confidence-interval-for-beta_j-2",
    "href": "slides/lec-16.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term         estimate std.error statistic       p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841         -268.       17.2 \n2 hightemp         7.54      1.17     6.43  0.00000000692     5.21      9.87\n3 seasonSpring     5.13     34.3      0.150 0.881           -63.1      73.4 \n4 seasonSummer   -76.8      47.7     -1.61  0.111          -172.       18.0"
  },
  {
    "objectID": "slides/lec-16.html#ci-for-hightemp",
    "href": "slides/lec-16.html#ci-for-hightemp",
    "title": "MLR: Inference",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n# A tibble: 4 × 7\n  term         estimate std.error statistic       p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841         -268.       17.2 \n2 hightemp         7.54      1.17     6.43  0.00000000692     5.21      9.87\n3 seasonSpring     5.13     34.3      0.150 0.881           -63.1      73.4 \n4 seasonSummer   -76.8      47.7     -1.61  0.111          -172.       18.0 \n\n\n\nWe are 95% confident that for every degrees Fahrenheit the day is warmer, we expect the number of riders to increase by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/lec-16.html#ci-for-seasonspring",
    "href": "slides/lec-16.html#ci-for-seasonspring",
    "title": "MLR: Inference",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n# A tibble: 4 × 7\n  term         estimate std.error statistic       p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841         -268.       17.2 \n2 hightemp         7.54      1.17     6.43  0.00000000692     5.21      9.87\n3 seasonSpring     5.13     34.3      0.150 0.881           -63.1      73.4 \n4 seasonSummer   -76.8      47.7     -1.61  0.111          -172.       18.0 \n\n\n\nWe are 95% confident that the number of riders on a Spring day is, on average, lower by 63.1 to higher by 73.4 compared to a Fall day, holding high temperature for the day constant."
  },
  {
    "objectID": "slides/lec-16.html#large-sample-sizes",
    "href": "slides/lec-16.html#large-sample-sizes",
    "title": "MLR: Inference",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/lec-16.html#small-sample-sizes",
    "href": "slides/lec-16.html#small-sample-sizes",
    "title": "MLR: Inference",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/lec-14.html#topics",
    "href": "slides/lec-14.html#topics",
    "title": "Cross validation",
    "section": "Topics",
    "text": "Topics\n\nCross validation for model evaluation\nCross validation for model comparison"
  },
  {
    "objectID": "slides/lec-14.html#computational-setup",
    "href": "slides/lec-14.html#computational-setup",
    "title": "Cross validation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(schrute)"
  },
  {
    "objectID": "slides/lec-14.html#data-goal",
    "href": "slides/lec-14.html#data-goal",
    "title": "Cross validation",
    "section": "Data & goal",
    "text": "Data & goal\n\nData: The data come from the shrute package, and has been transformed using instructions from Lab 4\nGoal: Predict imdb_rating from other variables in the dataset\n\n\noffice_episodes &lt;- read_csv(here::here(\"slides\", \"data/office_episodes.csv\"))\noffice_episodes\n\n# A tibble: 186 × 14\n   season episode episode_name      imdb_rating total_votes air_date   lines_jim\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;\n 1      1       1 Pilot                     7.6        3706 2005-03-24    0.157 \n 2      1       2 Diversity Day             8.3        3566 2005-03-29    0.123 \n 3      1       3 Health Care               7.9        2983 2005-04-05    0.172 \n 4      1       4 The Alliance              8.1        2886 2005-04-12    0.202 \n 5      1       5 Basketball                8.4        3179 2005-04-19    0.0913\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26    0.159 \n 7      2       1 The Dundies               8.7        3213 2005-09-20    0.125 \n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27    0.0565\n 9      2       3 Office Olympics           8.4        2742 2005-10-04    0.196 \n10      2       4 The Fire                  8.4        2713 2005-10-11    0.160 \n# … with 176 more rows, and 7 more variables: lines_pam &lt;dbl&gt;,\n#   lines_michael &lt;dbl&gt;, lines_dwight &lt;dbl&gt;, halloween &lt;chr&gt;, valentine &lt;chr&gt;,\n#   christmas &lt;chr&gt;, michael &lt;chr&gt;"
  },
  {
    "objectID": "slides/lec-14.html#split-data-into-training-and-testing",
    "href": "slides/lec-14.html#split-data-into-training-and-testing",
    "title": "Cross validation",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\n\nset.seed(123)\noffice_split &lt;- initial_split(office_episodes)\noffice_train &lt;- training(office_split)\noffice_test &lt;- testing(office_split)"
  },
  {
    "objectID": "slides/lec-14.html#specify-model",
    "href": "slides/lec-14.html#specify-model",
    "title": "Cross validation",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-14.html#from-yesterdays-lab",
    "href": "slides/lec-14.html#from-yesterdays-lab",
    "title": "Cross validation",
    "section": "From yesterday’s lab",
    "text": "From yesterday’s lab\n\nCreate a recipe that uses the new variables we generated\nDenotes episode_name as an ID variable and doesn’t use air_date as a predictor\nCreate dummy variables for all nominal predictors\nRemove all zero variance predictors"
  },
  {
    "objectID": "slides/lec-14.html#create-recipe",
    "href": "slides/lec-14.html#create-recipe",
    "title": "Cross validation",
    "section": "Create recipe",
    "text": "Create recipe\n\noffice_rec1 &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  update_role(episode_name, new_role = \"id\") %&gt;%\n  step_rm(air_date) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\noffice_rec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor         12\n\nOperations:\n\nDelete terms air_date\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-14.html#create-workflow",
    "href": "slides/lec-14.html#create-workflow",
    "title": "Cross validation",
    "section": "Create workflow",
    "text": "Create workflow\n\noffice_wflow1 &lt;- workflow() %&gt;%\n  add_model(office_spec) %&gt;%\n  add_recipe(office_rec1)\n\noffice_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_rm()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-14.html#fit-model-to-training-data",
    "href": "slides/lec-14.html#fit-model-to-training-data",
    "title": "Cross validation",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nActually, not so fast!"
  },
  {
    "objectID": "slides/lec-14.html#spending-our-data",
    "href": "slides/lec-14.html#spending-our-data",
    "title": "Cross validation",
    "section": "Spending our data",
    "text": "Spending our data\n\nWe have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\nHowever, we usually need to understand the effectiveness of the model before using the test set.\nTypically we can’t decide on which final model to take to the test set without making model assessments.\nRemedy: Resampling to make model assessments on training data in a way that can generalize to new data."
  },
  {
    "objectID": "slides/lec-14.html#resampling-for-model-assessment",
    "href": "slides/lec-14.html#resampling-for-model-assessment",
    "title": "Cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set.\nThe model is evaluated with the assessment set."
  },
  {
    "objectID": "slides/lec-14.html#resampling-for-model-assessment-1",
    "href": "slides/lec-14.html#resampling-for-model-assessment-1",
    "title": "Cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\nSource: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/lec-14.html#analysis-and-assessment-sets",
    "href": "slides/lec-14.html#analysis-and-assessment-sets",
    "title": "Cross validation",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-1",
    "href": "slides/lec-14.html#cross-validation-1",
    "title": "Cross validation",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation – commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse 1 partition for assessment, and the remaining v-1 partitions for analysis\nRepeat v times, updating which partition is used for assessment each time\n\n\nLet’s give an example where v = 3…"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-step-1",
    "href": "slides/lec-14.html#cross-validation-step-1",
    "title": "Cross validation",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/lec-14.html#split-data",
    "href": "slides/lec-14.html#split-data",
    "title": "Cross validation",
    "section": "Split data",
    "text": "Split data\n\nset.seed(345)\nfolds &lt;- vfold_cv(office_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits          id   \n  &lt;list&gt;          &lt;chr&gt;\n1 &lt;split [92/47]&gt; Fold1\n2 &lt;split [93/46]&gt; Fold2\n3 &lt;split [93/46]&gt; Fold3"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-steps-2-and-3",
    "href": "slides/lec-14.html#cross-validation-steps-2-and-3",
    "title": "Cross validation",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\nUse 1 partition for assessment, and the remaining v-1 partitions for analysis\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/lec-14.html#fit-resamples",
    "href": "slides/lec-14.html#fit-resamples",
    "title": "Cross validation",
    "section": "Fit resamples",
    "text": "Fit resamples\n\nset.seed(456)\n\noffice_fit_rs1 &lt;- office_wflow1 %&gt;%\n  fit_resamples(folds)\n\noffice_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 4\n  splits          id    .metrics         .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [92/47]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt;\n2 &lt;split [93/46]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt;\n3 &lt;split [93/46]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt;"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-now-what",
    "href": "slides/lec-14.html#cross-validation-now-what",
    "title": "Cross validation",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe’ve fit a bunch of models\nNow it’s time to use them to collect metrics (e.g., R-squared, RMSE) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/lec-14.html#collect-cv-metrics",
    "href": "slides/lec-14.html#collect-cv-metrics",
    "title": "Cross validation",
    "section": "Collect CV metrics",
    "text": "Collect CV metrics\n\ncollect_metrics(office_fit_rs1)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.351     3  0.0111 Preprocessor1_Model1\n2 rsq     standard   0.546     3  0.0378 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-14.html#deeper-look-into-cv-metrics",
    "href": "slides/lec-14.html#deeper-look-into-cv-metrics",
    "title": "Cross validation",
    "section": "Deeper look into CV metrics",
    "text": "Deeper look into CV metrics\n\ncv_metrics1 &lt;- collect_metrics(office_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 × 5\n  id    .metric .estimator .estimate .config             \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Fold1 rmse    standard       0.356 Preprocessor1_Model1\n2 Fold1 rsq     standard       0.520 Preprocessor1_Model1\n3 Fold2 rmse    standard       0.367 Preprocessor1_Model1\n4 Fold2 rsq     standard       0.498 Preprocessor1_Model1\n5 Fold3 rmse    standard       0.330 Preprocessor1_Model1\n6 Fold3 rsq     standard       0.621 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/lec-14.html#better-tabulation-of-cv-metrics",
    "href": "slides/lec-14.html#better-tabulation-of-cv-metrics",
    "title": "Cross validation",
    "section": "Better tabulation of CV metrics",
    "text": "Better tabulation of CV metrics\n\ncv_metrics1 %&gt;%\n  mutate(.estimate = round(.estimate, 3)) %&gt;%\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) %&gt;%\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n0.356\n0.520\n\n\nFold2\n0.367\n0.498\n\n\nFold3\n0.330\n0.621"
  },
  {
    "objectID": "slides/lec-14.html#how-does-rmse-compare-to-y",
    "href": "slides/lec-14.html#how-does-rmse-compare-to-y",
    "title": "Cross validation",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\nCross validation RMSE stats:\n\ncv_metrics1 %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean     sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 0.330 0.367 0.351 0.0192\n\n\nTraining data IMDB score stats:\n\noffice_episodes %&gt;%\n  summarise(\n    min = min(imdb_rating),\n    max = max(imdb_rating),\n    mean = mean(imdb_rating),\n    sd = sd(imdb_rating)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7  8.25 0.535"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-jargon",
    "href": "slides/lec-14.html#cross-validation-jargon",
    "title": "Cross validation",
    "section": "Cross validation jargon",
    "text": "Cross validation jargon\n\nReferred to as v-fold or k-fold cross validation\nAlso commonly abbreviated as CV"
  },
  {
    "objectID": "slides/lec-14.html#cross-validation-for-reals",
    "href": "slides/lec-14.html#cross-validation-for-reals",
    "title": "Cross validation",
    "section": "Cross validation, for reals",
    "text": "Cross validation, for reals\n\nTo illustrate how CV works, we used v = 3:\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/lec-14.html#recap",
    "href": "slides/lec-14.html#recap",
    "title": "Cross validation",
    "section": "Recap",
    "text": "Recap\n\nCross validation for model evaluation\nCross validation for model comparison"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Instructor\n\nFlorian Shkurti is an assistant professor in computer science at the University of Toronto, where he leads the Robot Vision and Learning lab. He is a faculty member of the University of Toronto Robotics Institute, the Acceleration Consortium, and a faculty affiliate at Vector Institute. His research group develops methods that enable robots to learn to perceive, reason, plan, and act effectively and safely, particularly in dynamic environments and alongside humans. Application areas of his research include field robotics for environmental monitoring, visual navigation for autonomous vehicles, and mobile manipulation.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMondays 4:30-5:30pm ET\n(Hybrid) Zoom + Sanford Fleming 3328, 3rd floor\n\n\n\n\n\nTeaching Assistants\n\nVahid Balazadeh I’m a Ph.D. student in Computer Science at the University of Toronto, supervised by Rahul G. Krishnan. I’m interested in understanding the concepts and mechanisms that help humans in optimal decision-making, especially in healthcare. To this end, I’m working on causality and its relationship with machine learning, as well as reinforcement learning from offline observations. In particular, my research involves causal estimation and partial identification of causal effects in high-dimensional data using machine learning. I also like to explore ideas from causality, such as causal representation learning, to design interpretable machine learning methods that are robust to out-of-distribution data.\n\n\n\nOffice hours\nLocation\n\n\n\n\nWednesdays 3-4pm ET\n(Online) Zoom\n\n\n\n\nWei-Cheng Tseng I am a PhD student at the University of Toronto, advised by Prof. Florian Shkurti. My research interests include Robot Learning, 3D Vision, and Deep Reinforcement Learning, with the goal of enabling robots to interact efficiently and effectively with real-world environments. Currently, I am working on a perception module for manipulation tasks. Prior to joining UofT, I worked on multi-agent systems, offline reinforcement learning, and novel-view synthesis.",
    "crumbs": [
      "Teaching Staff"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Fall 2024 - CSC 2626: Imitation Learning for Robotics",
    "section": "",
    "text": "Course Overview\nIn the next few decades we are going to witness millions of people, from various backgrounds and levels of technical expertise, needing to effectively interact with robotic technologies on a daily basis. As such, people will need to modify the behavior of their robots without explicitly writing code, but by providing only a small number of kinesthetic or visual demonstrations, or even natural language commands. At the same time, robots should try to infer and predict the human’s intentions and internal objectives from past interactions, in order to provide assistance before it is explicitly asked. This graduate-level course will examine some of the most important papers in imitation learning for robot control, placing more emphasis on developments in the last 10 years. Its purpose is to familiarize students with the frontiers of this research area, to help them identify open problems, and to enable them to make a research contribution.\nThis course will broadly cover the following areas:\n\nImitating the policies of demonstrators (people, expensive algorithms, optimal controllers)\nConnections between imitation learning, optimal control, and reinforcement learning\nLearning the cost functions that best explain a set of demonstrations\nShared autonomy between humans and robots for real-time control\n\n\n\nPrerequisites\nYou need to be comfortable with: introductory machine learning concepts (such as from CSC411/CSC413/ECE521 or equivalent), linear algebra, basic multivariable calculus, intro to probability. You also need to have strong programming skills in Python. Note: if you don’t meet all the prerequisites above please contact the instructor by email. Optional, but recommended: experience with neural networks, such as from CSC321, introductory-level familiarity with reinforcement learning and control.\n\n\nCourse Delivery Details\n\nLectures: In-person, Mondays @ 1pm-4pm ET, Carr Hall 404\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures\nAnonymous feedback form for suggested improvements"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html",
    "href": "ae/ae-10-flight-delays.html",
    "title": "AE 10: Flight delays",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-10-flight-delays-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#packages",
    "href": "ae/ae-10-flight-delays.html#packages",
    "title": "AE 10: Flight delays",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#data",
    "href": "ae/ae-10-flight-delays.html#data",
    "title": "AE 10: Flight delays",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of 25,000 randomly sampled flights that departed one of three NYC airports (JFK, LGA, EWR) in 2013.\n\nflight_data &lt;- read_csv(\"data/flight-data.csv\")\n\nRows: 25000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): origin, dest, carrier, arr_delay\ndbl  (4): dep_time, flight, air_time, distance\ndttm (1): time_hour\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nConvert arr_delay to factor with levels \"late\" (first level) and \"on_time\" (second level). This variable is our outcome and it indicates whether the flight’s arrival was more than 30 minutes.\n\n\nflight_data &lt;- flight_data %&gt;%\n  mutate(arr_delay = as.factor(arr_delay))\n\nlevels(flight_data$arr_delay)\n\n[1] \"late\"    \"on_time\"\n\n\n\nLet’s get started with some data prep: Convert all variables that are character strings to factors.\n\n\n#flight_data &lt;- flight_data %&gt;%\n#  mutate(\n#    origin = as.factor(origin),\n#    carrier = as.factor(carrier),\n#    dest = as.factor(dest)\n#    )\n\nflight_data &lt;- flight_data %&gt;%\n  #go across all columns and convert that are characters to factors\n  #go across all columns and convert if is.character = TRUE to factors\n  #go across all columns and if is.character apply as.factor\n  mutate(across(where(is.character), as.factor))"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#modeling-prep",
    "href": "ae/ae-10-flight-delays.html#modeling-prep",
    "title": "AE 10: Flight delays",
    "section": "Modeling prep",
    "text": "Modeling prep\n\nSplit the data into testing (75%) and training (25%), and save each subset.\n\n\nset.seed(222)\n\nflight_split &lt;- initial_split(flight_data)\n\nflight_train &lt;- training(flight_split)\nflight_test &lt;- testing(flight_split)\n\n\nSpecify a logistic regression model that uses the \"glm\" engine.\n\n\nflight_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nNext, we’ll create two recipes and workflows and compare them to each other."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "href": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "title": "AE 10: Flight delays",
    "section": "Model 1: Everything and the kitchen sink",
    "text": "Model 1: Everything and the kitchen sink\n\nDefine a recipe that predicts arr_delay using all variables except for flight and time_hour, which, in combination, can be used to identify a flight. Also make sure this recipe handles dummy coding as well as issues that can arise due to having categorical variables with some levels apparent in the training set but not in the testing set. Call this recipe flights_rec1.\n\n\nflights_rec1 &lt;- recipe(arr_delay ~ ., data = flight_train) %&gt;%\n  update_role(flight, time_hour, new_role = \"id\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nCreate a workflow that uses flights_rec1 and the model you specified.\n\n\nflight_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(flights_rec1) %&gt;%\n  add_model(flight_spec)\n\n\nFit the this model to the training data using your workflow and display a tidy summary of the model fit.\n\n\nflight_fit1 &lt;- flight_wflow1 %&gt;%\n  fit(data = flight_train)\n\ntidy(flight_fit1)\n\n# A tibble: 119 × 5\n   term          estimate   std.error statistic   p.value\n   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  13.3      287.          0.0464  9.63e-  1\n 2 dep_time     -0.00164    0.0000504 -32.6     1.04e-233\n 3 air_time     -0.0349     0.00179   -19.5     1.75e- 84\n 4 distance      0.00533    0.00523     1.02    3.08e-  1\n 5 date          0.000227   0.000198    1.15    2.51e-  1\n 6 origin_JFK    0.0830     0.102       0.815   4.15e-  1\n 7 origin_LGA   -0.0360     0.0983     -0.366   7.14e-  1\n 8 dest_ACK    -12.4      287.         -0.0434  9.65e-  1\n 9 dest_ALB    -12.4      287.         -0.0433  9.65e-  1\n10 dest_ANC     -3.75     928.         -0.00404 9.97e-  1\n# … with 109 more rows\n\n\n\nPredict arr_delay for the testing data using this model.\n\n\nflight_aug1 &lt;- augment(flight_fit1, flight_test)\n\n\nPlot the ROC curve and find the area under the curve. Comment on how well you think this model has done for predicting arrival delay.\n\n\nflight_aug1 %&gt;%\n  roc_curve(\n    truth = arr_delay,\n    .pred_late\n  ) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nflight_aug1 %&gt;%\n  roc_auc(\n    truth = arr_delay,\n    .pred_late\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.734"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "href": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "title": "AE 10: Flight delays",
    "section": "Model 2: Let’s be a bit more thoughtful",
    "text": "Model 2: Let’s be a bit more thoughtful\n\nDefine a new recipe, flights_rec2, that, in addition to what was done in flights_rec1, adds features for day of week and month based on date and also adds indicators for all US holidays (also based on date). A list of these holidays can be found in timeDate::listHolidays(\"US\"). Once these features are added, date should be removed from the data. Then, create a new workflow, fit the same model (logistic regression) to the training data, and do predictions on the testing data. Finally, draw another ROC curve and find the area under the curve. Compare the predictive performance of this new model to the previous one. Based on the area under the curve statistic, which model does better?"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "href": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "title": "AE 10: Flight delays",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\nCreate an ROC curve that plots both models, in different colors, and adds a legend indicating which model is which."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#acknowledgement",
    "href": "ae/ae-10-flight-delays.html#acknowledgement",
    "title": "AE 10: Flight delays",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://www.tidymodels.org/start/recipes/."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html",
    "href": "ae/ae-3-duke-forest.html",
    "title": "AE 3: Duke Forest houses",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-3-duke-forest-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#packages",
    "href": "ae/ae-3-duke-forest.html#packages",
    "title": "AE 3: Duke Forest houses",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "href": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "title": "AE 3: Duke Forest houses",
    "section": "Predict sale price from area",
    "text": "Predict sale price from area\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#model-conditions",
    "href": "ae/ae-3-duke-forest.html#model-conditions",
    "title": "AE 3: Duke Forest houses",
    "section": "Model conditions",
    "text": "Model conditions\n\nExercise 1\nThe following code produces the residuals vs. fitted values plot for this model. Comment out the layer that defines the y-axis limits and re-create the plot. How does the plot change? Why might we want to define the limits explicitly?\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n\n\n\n\n\n\n\nExercise 2\nImprove how the values on the axes of the plot are displayed by modifying the code below.\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html",
    "href": "ae/ae-7-exam-2-review.html",
    "title": "AE 7: Exam 2 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-7-exam-2-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#packages",
    "href": "ae/ae-7-exam-2-review.html#packages",
    "title": "AE 7: Exam 2 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\n# fix data!\nloans_full_schema &lt;- droplevels(loans_full_schema)"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#goal",
    "href": "ae/ae-7-exam-2-review.html#goal",
    "title": "AE 7: Exam 2 Review",
    "section": "Goal",
    "text": "Goal\nCreate a model for precicting interest_rate."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#view-data",
    "href": "ae/ae-7-exam-2-review.html#view-data",
    "title": "AE 7: Exam 2 Review",
    "section": "View data",
    "text": "View data\nNote the dimensions of the data and the variable names. Review the data dictionary.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "href": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "title": "AE 7: Exam 2 Review",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#write-the-model",
    "href": "ae/ae-7-exam-2-review.html#write-the-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Write the model",
    "text": "Write the model\nWrite the model for predicting interest rate (interest_rate) from debt to income ratio (debt_to_income), the term of loan (term), the number of inquiries (credit checks) into the applicant’s credit during the last 12 months (inquiries_last_12m), whether there are any bankruptcies listed in the public record for this applicant (bankrupt), and the type of application (application_type). The model should allow for the effect of to income ratio on interest rate to vary by application type.\nAdd model here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#exploration",
    "href": "ae/ae-7-exam-2-review.html#exploration",
    "title": "AE 7: Exam 2 Review",
    "section": "Exploration",
    "text": "Exploration\nExplore characteristics of the variables you’ll use for the model using the training data only.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#specify-model",
    "href": "ae/ae-7-exam-2-review.html#specify-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-recipe",
    "href": "ae/ae-7-exam-2-review.html#create-recipe",
    "title": "AE 7: Exam 2 Review",
    "section": "Create recipe",
    "text": "Create recipe\n\nPredict interest_rate from debt_to_income, term, inquiries_last_12m, public_record_bankrupt, and application_type.\nMean center debt_to_income.\nMake term a factor.\nCreate a new variable: bankrupt that takes on the value “no” if public_record_bankrupt is 0 and the value “yes” if public_record_bankrupt is 1 or higher. Then, remove public_record_bankrupt.\nInteract application_type with debt_to_income.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-workflow",
    "href": "ae/ae-7-exam-2-review.html#create-workflow",
    "title": "AE 7: Exam 2 Review",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#cross-validation",
    "href": "ae/ae-7-exam-2-review.html#cross-validation",
    "title": "AE 7: Exam 2 Review",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "href": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "title": "AE 7: Exam 2 Review",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here\n\nWhy are we focusing on R-squared and RMSE instead of adjusted R-squared, AIC, BIC?\n[Add response here]"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#next-steps",
    "href": "ae/ae-7-exam-2-review.html#next-steps",
    "title": "AE 7: Exam 2 Review",
    "section": "Next steps…",
    "text": "Next steps…\nDepending on time, either\n\nCreate a workflow for another model with a new recipe (omitting the interaction variable), conduct CV, do model selection between these two, and then interpret the coefficients for the selected model.\nOr interpret the coefficients for the one model you fit.\n\nMake sure to interpret the intercept and slope coefficient for at least one numerical, one categorical, and one interaction predictor."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html",
    "href": "ae/ae-12-exam-3-review.html",
    "title": "AE 12: Exam 3 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-12-exam-3-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#packages",
    "href": "ae/ae-12-exam-3-review.html#packages",
    "title": "AE 12: Exam 3 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\nlibrary(rms)\nlibrary(nnet)"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#data",
    "href": "ae/ae-12-exam-3-review.html#data",
    "title": "AE 12: Exam 3 Review",
    "section": "Data",
    "text": "Data\nAs part of a study of the effects of predatory intertidal crab species on snail populations, researchers measured the mean closing forces and the propodus heights of the claws on several crabs of three species.\n\n\nclaws &lt;- read_csv(here::here(\"ae\", \"data/claws.csv\"))\n\nWe will use the following variables:\n\nforce: Closing force of claw (newtons)\nheight: Propodus height (mm)\nspecies: Crab species - Cp(Cancer productus), Hn (Hemigrapsus nudus), Lb(Lophopanopeus bellus)\nlb: 1 if Lophopanopeus bellus species, 0 otherwise\nhn: 1 if Hemigrapsus nudus species, 0 otherwise\ncp: 1 if Cancer productus species, 0 otherwise\nforce_cent: mean centered force\nheight_cent: mean centered height\n\nBefore we get started, let’s make the categorical and indicator variables factors.\n\nclaws &lt;- claws %&gt;%\n  mutate(\n    species = as_factor(species),\n    lb = as_factor(lb),\n    hn = as_factor(hn),\n    cp = as_factor(cp)\n  )"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "href": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "title": "AE 12: Exam 3 Review",
    "section": "Probabilities vs. odds vs. log-odds",
    "text": "Probabilities vs. odds vs. log-odds\nWhy we use log-odds as response variable: https://sta210-s22.github.io/website/slides/lec-18.html#/do-teenagers-get-7-hours-of-sleep"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-1",
    "href": "ae/ae-12-exam-3-review.html#exercise-1",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nFill in the blanks:\n\nUse log-odds to fit the model (outcome)\nUse odds to interpret model results\nUse probabilities to make predictions for individual observations and ultimately to make classification decisions"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-2",
    "href": "ae/ae-12-exam-3-review.html#exercise-2",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose we want to use force to determine whether or not a crab is from the Lophopanopeus bellus (Lb) species. Why should we use a logistic regression model for this analysis?\n\nclaws %&gt;%\n  distinct(lb)\n\n# A tibble: 2 × 1\n  lb   \n  &lt;fct&gt;\n1 0    \n2 1"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-3",
    "href": "ae/ae-12-exam-3-review.html#exercise-3",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe will use the mean-centered variables for force in the model. The model output is below. Write the equation of the model produced by R. Don’t forget to fill in the blanks for ….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.798\n0.358\n-2.233\n0.026\n-1.542\n-0.123\n\n\nforce_cent\n0.043\n0.039\n1.090\n0.276\n-0.034\n0.123\n\n\n\n\n\nLet \\(\\pi\\) be probability that a crab is from Lb species.\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = -0.798 + 0.043 * force\\_cent\n\\]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-4",
    "href": "ae/ae-12-exam-3-review.html#exercise-4",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the intercept in the context of the data.\n\nmean_force &lt;- round(mean(claws$force), 2)\n\nFor crabs with average closing force (12.13 newtons), we expect odds of the crab being Lophopanopeus bellus is 0.45 (exp(-0.798))."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-5",
    "href": "ae/ae-12-exam-3-review.html#exercise-5",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the effect of force in the context of the data.\nWhen x goes up by 1 unit, we expect y to change by (slope) units.\nFor each additional unit increase in closing force, the odds of crab being from lb species multiplies on average by a factor of 1.0439379."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-6",
    "href": "ae/ae-12-exam-3-review.html#exercise-6",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow let’s consider adding height_cent to the model. Fit the model that includes height_cent. Then use AIC to choose the model that best fits the data.\n\nlb_fit_2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(lb ~ force_cent + height_cent, data = claws)\n\ntidy(lb_fit_2, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -1.13     0.463      -2.44  0.0146  -2.17      -0.306\n2 force_cent     0.211    0.0925      2.28  0.0227   0.0563     0.424\n3 height_cent   -0.895    0.398      -2.25  0.0245  -1.82      -0.234\n\nglance(lb_fit_1)$AIC\n\n[1] 50.19535\n\nglance(lb_fit_2)$AIC\n\n[1] 44.11812"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-7",
    "href": "ae/ae-12-exam-3-review.html#exercise-7",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat do the following mean in the context of this data. Explain and calculate them.\n\nSensitivity: P(predict lb | actual lb) = 6 / 12\nSpecificity: P(predict not lb | actual not lb) = 4/ 26\nNegative predictive power: P(actual not lb | predict not lb) = 22 / 28\nPositive predictive power: P(actual lb | predict lb) = 6 / 10\n\n\n\n\nActual\nPredict lb\nPredict not lb\nTOTAL predicted\n\n\n\n\nLb\n6\n6\n12\n\n\nNot lb\n4\n22\n26\n\n\nTOTAL actual\n10\n28\n38"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-8",
    "href": "ae/ae-12-exam-3-review.html#exercise-8",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nWrite the equation of the model.\n\\[\\log\\Big(\\frac{\\hat{\\pi}_{Hn}}{\\hat{\\pi}_{Cp}}\\Big) = \\]\n\\[\\log\\Big(\\frac{\\hat{\\pi}_{Lb}}{\\hat{\\pi}_{Cp}}\\Big) = \\]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-9",
    "href": "ae/ae-12-exam-3-review.html#exercise-9",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nInterpret the intercept for the odds a crab is Hn vs. Cp species.\nInterpret the effect of force on the odds a crab is Lb vs. Cp species."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-10",
    "href": "ae/ae-12-exam-3-review.html#exercise-10",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the effect of force on the odds a crab is in the Hn vs. Lb species.\nCAUTION: We can write an interpretation based on the estimated coefficients; however, we can’t make any inferential conclusions for this question based on the current model. We would need to refit the model with Lb as the baseline category to do so."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-11",
    "href": "ae/ae-12-exam-3-review.html#exercise-11",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 11",
    "text": "Exercise 11\nConditions for multinomial logistic (and logistic models as well):\n\nIndependence:\nRandomness:\nLinearity:\n\nemplogitplot1(lb ~ force, data = claws, ngroups = 10)\nemplogitplot1(lb ~ height, data = claws, ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n# add code here for other species here\n\n\n\n# add code here for other species here"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "href": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "title": "AE 12: Exam 3 Review",
    "section": "Checking for multicollinearity in logistic and multinomial logistic",
    "text": "Checking for multicollinearity in logistic and multinomial logistic\nSimilar to multiple linear regression, we can also check for multicollinearity in logistic and multinomial logistic models.\n\nUse the vif function to check for multicollinearity in logistic regression.\n\n\nThe vif function doesn’t work for the multinomial logistic regression models, so we can look at a correlation matrix of the predictors as a way to assess if the predictors are highly correlated:"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html",
    "href": "ae/ae-8-rail-trail.html",
    "title": "AE 8: Rail Trail",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-8-rail-trail-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#packages-and-data",
    "href": "ae/ae-8-rail-trail.html#packages-and-data",
    "title": "AE 8: Rail Trail",
    "section": "Packages and data",
    "text": "Packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-1",
    "href": "ae/ae-8-rail-trail.html#exercise-1",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model predicting volume from hightemp and season.\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111        \n\n\nRecreate the following visualization which displays the three regression lines we can draw based on the results of this model.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-2",
    "href": "ae/ae-8-rail-trail.html#exercise-2",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 2",
    "text": "Exercise 2\nAdd an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-3",
    "href": "ae/ae-8-rail-trail.html#exercise-3",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a model predicting volume from all available predictors.\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\nRecreate the following visualization which displays a histogram of residuals and a normal density curve overlaid.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet’s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips %&gt;%\n  specify(Tip ~ Party) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you’re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you’re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs. leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLet’s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs. fitted values and a plot of the square root of the absolute value of standardized residuals vs. fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs. fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the \\(\\sqrt{|\\text{standardized residuals}|}\\) vs. fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCook’s distance\n\nMake a plot to check Cook’s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#footnotes",
    "href": "ae/ae-4-exam-1-review.html#footnotes",
    "title": "AE 4: Exam 1 Review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-5-the-office.html",
    "href": "ae/ae-5-the-office.html",
    "title": "AE 5: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-5-the-office-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-5-the-office.html#packages",
    "href": "ae/ae-5-the-office.html#packages",
    "title": "AE 5: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-5-the-office.html#load-data",
    "href": "ae/ae-5-the-office.html#load-data",
    "title": "AE 5: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "href": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "title": "AE 5: The Office",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRecreate at least one of the exploratory visualizations from class."
  },
  {
    "objectID": "ae/ae-5-the-office.html#testtrain-split",
    "href": "ae/ae-5-the-office.html#testtrain-split",
    "title": "AE 5: The Office",
    "section": "Test/train split",
    "text": "Test/train split\nSplit your data into testing and training sets."
  },
  {
    "objectID": "ae/ae-5-the-office.html#build-a-recipe",
    "href": "ae/ae-5-the-office.html#build-a-recipe",
    "title": "AE 5: The Office",
    "section": "Build a recipe",
    "text": "Build a recipe\nBuild the recipe from class.\n\nTime permitting…"
  },
  {
    "objectID": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "href": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "title": "AE 5: The Office",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\nBuild the modeling workflow and fit the model to the training data after feature engineering with the recipe."
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#introduction",
    "href": "hw/hw-2.html#introduction",
    "title": "HW 2 - Multiple linear regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the multiple linear regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multiple linear regression models with main and interaction effects.\nCompare multiple linear regression models.\nReason around multiple linear regression concepts.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-2. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-2.html#part-1---conceptual",
    "href": "hw/hw-2.html#part-1---conceptual",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 1 - Conceptual",
    "text": "Part 1 - Conceptual\n\nDealing with categorical predictors. Two friends, Elliott and Adrian, want to build a model predicting typing speed (average number of words typed per minute) from whether the person wears glasses or not. Before building the model they want to conduct some exploratory analysis to evaluate the strength of the association between these two variables, but they’re in disagreement about how to evaluate how strongly a categorical predictor is associated with a numerical outcome. Elliott claims that it is not possible to calculate a correlation coefficient to summarize the relationship between a categorical predictor and a numerical outcome, however they’re not sure what a better alternative is. Adrian claims that you can recode a binary predictor as a 0/1 variable (assign one level to be 0 and the other to be 1), thus converting it to a numerical variable. According to Adrian, you can then calculate the correlation coefficient between the predictor and the outcome. Who is right: Elliott or Adrian? If you pick Elliott, can you suggest a better alternative for evaluating the association between the categorical predictor and the numerical outcome?\nHigh correlation, good or bad? Two friends, Frances and Annika, are in disagreement about whether high correlation values are always good in the context of regression. Frances claims that it’s desirable for all variables in the dataset to be highly correlated to each other when building linear models. Annika claims that while it’s desirable for each of the predictors to be highly correlated with the outcome, it is not desirable for the predictors to be highly correlated with each other. Who is right: Frances, Annika, both, or neither? Explain your reasoning using appropriate terminology.\nTraining for the 5K. Nico signs up for a 5K (a 5,000 metre running race) 30 days prior to the race. They decide to run a 5K every day to train for it, and each day they record the following information: days_since_start (number of days since starting training), days_till_race (number of days left until the race), mood (poor, good, awesome), tiredness (1-not tired to 10-very tired), and time (time it takes to run 5K, recorded as mm:ss). Top few rows of the data they collect is shown below.\n\n\n\ndays_since_start\ndays_till_race\nmood\ntiredness\ntime\n\n\n\n\n1\n29\ngood\n3\n25:45\n\n\n2\n28\npoor\n5\n27:13\n\n\n3\n27\nawesome\n4\n24:13\n\n\n…\n…\n…\n…\n…\n\n\n\nUsing these data Nico wants to build a model predicting time from the other variables. Should they include all variables shown above in their model? Why or why not?\nMultiple regression fact checking. Determine which of the following statements are true and false. For each statement that is false, explain why it is false.\n\nIf predictors are colinear, then removing one variable will have no influence on the point estimate of another variable’s coefficient.\nSuppose a numerical predictor \\(x\\) has a coefficient of \\(\\hat{\\beta}_1 = 2.5\\) in a multiple regression model. Suppose also that the first observation has \\(x_{1,1} = 7.2\\), the second observation has a value of \\(x_{2,1} = 8.2\\), and these two observations have the same values for all other predictors. Then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model.\nIf a regression model’s first predictor has a coefficient of \\(\\hat{\\beta}_1 = 5.7\\) and if we are able to influence the data so that an observation will have its \\(x_1\\) be 1 larger than it would otherwise, the value \\(\\hat{y}_1\\) for this observation would increase by 5.7."
  },
  {
    "objectID": "hw/hw-2.html#part-2---palmer-penguins",
    "href": "hw/hw-2.html#part-2---palmer-penguins",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 2 - Palmer penguins",
    "text": "Part 2 - Palmer penguins\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\n\n\nArtwork by @allison_horst\n\n\nThese data can be found in the palmerpenguins package. We’re going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\nBody mass. Our first goal is to fit a model predicting body mass (which is more difficult to measure) from bill length, bill depth, flipper length, species, and sex.\n\nFit a model predicting body mass (which is more difficult to measure) from the other variables listed above.\nWrite the equation of the regression model.\nInterpret each one of the slopes in this context.\nCalculate the residual for a male Adelie penguin that weighs 3750 grams with the following body measurements: bill_length_mm = 39.1, bill_depth_mm = 18.7, flipper_length_mm = 181. Does the model overpredict or underpredict this penguin’s weight?\nFind the \\(R^2\\) of this model and interpret this value in context of the data and the model.\n\n\n\n\nBill depth. Next we’ll be focusing on bill depth and bill length and also considering species.\n\nFit a model predicting bill depth from bill length. Find the adjusted R-squared, AIC, and BIC for this model.\nThen, add a new predictor: species. Fit another model predicting bill depth from bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nFinally, add one more predictor: the interaction between bill length and species. Find the adjusted R-squared, AIC, and BIC for this model.\nUsing the three criteria you recorded for these three models, and with the goal of parsimony, which model is the “best” for predicting bill depth from bill length and/or species. Explain your reasoning.\nCreate a visualization representing your model from part a. Hint: Make a scatterplot of bill depth vs. bill length and add the linear model.\nCreate a visualization representing your model from part b. Hint: Same as part (e), but think about how many lines to plot and whether their slopes should be the same or different.\nCreate a visualization representing your model from part c. Hint: Same as part (f), but think about how many lines to plot and whether their slopes should be the same or different.\nBased on your visualizations from parts e - g, and with the goal of parsimony, is your answer for which model is the “best” for predicting bill depth from bill length and/or species the same as your answer in part d? Explain your reasoning."
  },
  {
    "objectID": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "href": "hw/hw-2.html#part-3---perceived-threat-of-covid-19",
    "title": "HW 2 - Multiple linear regression",
    "section": "Part 3 - Perceived threat of Covid-19",
    "text": "Part 3 - Perceived threat of Covid-19\nGarbe, Rau, and Toppe (2020), published in June 2020, aims to examine the relationship between personality traits, perceived threat of Covid-19 and stockpiling toilet paper. For this study titled Influence of perceived threat of Covid-19 and HEXACO personality traits on toilet paper stockpiling, researchers conducted an online survey March 23 - 29, 2020 and used the results to fit multiple linear regression models to draw conclusions about their research questions. From their survey, they collected data on adults across 35 countries. Given the small number of responses from people outside of the United States, Canada, and Europe, only responses from people in these three locations were included in the regression analysis.\nLet’s consider their results for the model looking at the effect on perceived threat of Covid-19. The model can be found on page 6 of the paper. The perceived threat of Covid was quantified using the responses to the following survey question:\n\nHow threatened do you feel by Coronavirus? [Users select on a 10-point visual analogue scale (Not at all threatened to Extremely Threatened)]\n\n\nInterpret the coefficient of Age (0.072) in the context of the analysis.\nInterpret the coefficient of Place of residence in the context of the analysis.\nThe model includes an interaction between Place of residence and Emotionality (capturing differential tendencies in to worry and be anxious).\n\nWhat does the coefficient for the interaction (0.101) mean in the context of the data?\nInterpret the estimated effect of Emotionality for a person who lives in the US/Canada.\nInterpret the estimated effect of Emotionality for a person who lives in Europe."
  },
  {
    "objectID": "hw/hw-2.html#submission",
    "href": "hw/hw-2.html#submission",
    "title": "HW 2 - Multiple linear regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-2.html#grading",
    "href": "hw/hw-2.html#grading",
    "title": "HW 2 - Multiple linear regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-2.html#footnotes",
    "href": "hw/hw-2.html#footnotes",
    "title": "HW 2 - Multiple linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#introduction",
    "href": "hw/hw-4.html#introduction",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate model conditions\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-4. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)"
  },
  {
    "objectID": "hw/hw-4.html#data",
    "href": "hw/hw-4.html#data",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Data",
    "text": "Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the “foil”, i.e. a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or don’t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large “N” sticker on one cheek. (The letter “N” was chosen to represent the first author’s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new &lt;- read_csv(here::here(\"hw\", \"data/eyewitness.csv\"))\new &lt;- ew %&gt;%\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-4.html#exercises",
    "href": "hw/hw-4.html#exercises",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s begin by doing some exploratory data analysis. The univariate (single variable) plots for each of the predictor variables and the response variable are shown below.\n\n\n\n\n\n\n\n\n\n\nComplete the exploratory data analysis by creating the plots and/or summary statistics to examine the relationship between the response variable (id) and each of the explanatory variables (lineup, weapon, and feature).\n\nUsing the plots/tables from Exercise 1:\n\n\nWhat is one thing you learn about the data from the univariate plots?\nBased on the bivariate plots, do any of the predictors appear to have a significant effect on the id? Briefly explain.\n\n\nBriefly explain why you should use a multinomial logistic regression model to predict id using lineup, weapon and feature.\nFit the multinomial logistic model that only includes main effects. Display the model output.\n\n\nWhat is the baseline category for the response variable?\nInterpret the intercepts for each part of the model in terms of the odds.\nInterpret the coefficients of lineup for each part of the model in terms of the odds.\n\n\nYou want to consider all possible first-order interaction effects (interaction effects between two variables) for the model.\n\n\nUse the appropriate test to determine if there is at least one significant interaction effect.\nBased on your test, is there evidence of any significant interaction effects?\n\nRegardless of your answer to Question 5, use the model that includes the interaction terms for the remainder of the assignment.\n\nAccording to the model,\n\n\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the log-odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nWhich group of participants (i.e., which set of experimental conditions) is described by the intercept?\n\n\nAre the conditions inference met? List of the conditions, and, if relevant, create visualizations to check the conditions and evaluate whether each condition is met. Include an assessment about each condition and a brief explanation about your conclusion.\nUse the model to predict the decision made by each participant. Make a table of the predicted vs. the actual decisions.\n\n\nBriefly describe how the predicted decision is determined for each participant.\nWhat is the misclassification rate?"
  },
  {
    "objectID": "hw/hw-4.html#submission",
    "href": "hw/hw-4.html#submission",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-4.html#grading",
    "href": "hw/hw-4.html#grading",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nExercises\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-4.html#footnotes",
    "href": "hw/hw-4.html#footnotes",
    "title": "HW 4 - Multinomial logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Friday, April 15 - Project peer review of drafts"
  },
  {
    "objectID": "weeks/week-14.html#participate",
    "href": "weeks/week-14.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 26 - MultiLR: Predictive models (cont.)\n🖥️ Lecture 27 - Exam 3 Review"
  },
  {
    "objectID": "weeks/week-14.html#practice",
    "href": "weeks/week-14.html#practice",
    "title": "Week 14",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 12 - Exam 3 Review"
  },
  {
    "objectID": "weeks/week-14.html#perform",
    "href": "weeks/week-14.html#perform",
    "title": "Week 14",
    "section": "Perform",
    "text": "Perform\n✍️ Project - Peer review of drafts\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\nDue date: Lab 4 - Friday, Feb 25"
  },
  {
    "objectID": "weeks/week-8.html#prepare",
    "href": "weeks/week-8.html#prepare",
    "title": "Week 8",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Tidy Modeling in R Chp 10: Resampling for evaluating performance"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 14 - MLR: Cross validation\n🖥️ Lecture 15 - Exam 2 review"
  },
  {
    "objectID": "weeks/week-8.html#practice",
    "href": "weeks/week-8.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nApplication Exercise 6 - The office - CV\nApplication Exercise 7 - Exam 2 Review"
  },
  {
    "objectID": "weeks/week-8.html#perform",
    "href": "weeks/week-8.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 4 - The Office, another look\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\nDue dates: None."
  },
  {
    "objectID": "weeks/week-12.html#prepare",
    "href": "weeks/week-12.html#prepare",
    "title": "Week 12",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Chp 26: Inference for logistic regression"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 22 - LR: Inference + conditions\n🖥️ Lecture 23 - Multinomial logistic regression (MultiLR)"
  },
  {
    "objectID": "weeks/week-12.html#practice",
    "href": "weeks/week-12.html#practice",
    "title": "Week 12",
    "section": "Practice",
    "text": "Practice\nNo application exercises this week."
  },
  {
    "objectID": "weeks/week-12.html#perform",
    "href": "weeks/week-12.html#perform",
    "title": "Week 12",
    "section": "Perform",
    "text": "Perform\n✍️ HW 4 - Multinomial logistic regression\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nLab 5 - Friday, Mar 25\nHW 3 - Friday, Mar 25"
  },
  {
    "objectID": "weeks/week-11.html#prepare",
    "href": "weeks/week-11.html#prepare",
    "title": "Week 11",
    "section": "Prepare",
    "text": "Prepare\nNo additional readings this week. Catch up with previously assigned readings if you’ve fallen behind."
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 20 - LR: Prediction / classification\n🖥️ Lecture 21 - LR: Model validation"
  },
  {
    "objectID": "weeks/week-11.html#practice",
    "href": "weeks/week-11.html#practice",
    "title": "Week 11",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 10 - Flight delays"
  },
  {
    "objectID": "weeks/week-11.html#perform",
    "href": "weeks/week-11.html#perform",
    "title": "Week 11",
    "section": "Perform",
    "text": "Perform\n✍️ HW 3 - Logistic regression and log transformation\n⌨️ Lab 5 - General Social Survey\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nClasses are virtual this week. Find Zoom links here.\nDue dates:\n\nLab 1: Fri, Jan 14, 5pm ET\nAE 1: Sun, Jan 16, 11:59pm ET"
  },
  {
    "objectID": "weeks/week-2.html#prepare",
    "href": "weeks/week-2.html#prepare",
    "title": "Week 2",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Chp 7: Linear regression with a single predictor"
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 1 - Meet the toolkit\n🖥️ Lecture 2 - Simple linear regression\n🖥️ Lecture 3 - Model fitting in R with tidymodels"
  },
  {
    "objectID": "weeks/week-2.html#practice",
    "href": "weeks/week-2.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 1 - Bike rentals in DC (Post-class note: complete only Part 1 - Daily counts and temperature)"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 1 - Meet the toolkit\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\nClasses are virtual this week. Find Zoom links here."
  },
  {
    "objectID": "weeks/week-1.html#prepare",
    "href": "weeks/week-1.html#prepare",
    "title": "Week 1",
    "section": "Prepare",
    "text": "Prepare\n📖 Read the syllabus\n📖 Read the support resources"
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 0 -Meet+ greet\n🖥️ Lecture 1 - Welcome to STA 210"
  },
  {
    "objectID": "weeks/week-1.html#practice",
    "href": "weeks/week-1.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\n📋 AE 0 - Movies"
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 0 - Meet + greet\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important\n\n\n\n\nDue date: Lab 2 - Fri, Feb 4, 5pm ET\nExam 1 released on Fri, Feb 4, due Mon, Feb 7 at 11:59pm"
  },
  {
    "objectID": "weeks/week-5.html#prepare",
    "href": "weeks/week-5.html#prepare",
    "title": "Week 5",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 8.1: Indicator and categorical predictors\n📖 Read Introduction to Modern Statistics, Sec 8.2: Many predictors in a model\n📖 Read Introduction to Modern Statistics, Sec 8.3: Adjusted R-squared"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 3 - Coffee ratings\n🖥️ Lecture 8 - Multiple linear regression (MLR)\n🖥️ Lecture 9 - Exam 1 review"
  },
  {
    "objectID": "weeks/week-5.html#practice",
    "href": "weeks/week-5.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 4 - Exam 1 Review"
  },
  {
    "objectID": "weeks/week-5.html#perform",
    "href": "weeks/week-5.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 3 - Coffee ratings\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Exam 1 - Mon, Feb 7 at 11:59pm"
  },
  {
    "objectID": "weeks/week-6.html#prepare",
    "href": "weeks/week-6.html#prepare",
    "title": "Week 6",
    "section": "Prepare",
    "text": "Prepare\nNo reading (take a break after the exam! 😴)"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 10 - MLR: Types of predictors\n🖥️ Lecture 11 - MLR: Model comparison"
  },
  {
    "objectID": "weeks/week-6.html#practice",
    "href": "weeks/week-6.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\nNo application exercises"
  },
  {
    "objectID": "weeks/week-6.html#perform",
    "href": "weeks/week-6.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\nNo lab\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful Resources and Links",
    "section": "",
    "text": "Recommended Simulators\nYou are encouraged to use the simplest possible simulator to accomplish the task you are interested in. You can submit links to simulators not included here by opening a github issue.\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics simulators. Read this first. The rest of this table shows simulators and environments not mentioned by Simulately.\n\n\nIsaac Lab (formerly Isaac Orbit / Isaac Gym)\nLayer of abstraction and tools to make using Isaac Sim easier.\n\n\nDrake Simulator\nA framework of simulation, analysis and control tools for robotics.\n\n\nDeepmind Control Suite\nSet of robotics environments on top of Mujoco.\n\n\nMujoco Menagerie\nHigh-quality description files and assets for robots, built on top of Mujoco.\n\n\nOpenAI Gym\nAtari, Mujoco, classic control, and third-party environments for RL.\n\n\nRoboSuite\nRobotics simulation environments on top of Mujoco. Also a benchmark.\n\n\nKlampt\nModeling, simulating, planning, and optimization for complex robots, particularly for manipulation and locomotion tasks.\n\n\nDART\nPhysics simulator for robotics and animation.\n\n\nCARLA\nSelf-driving environment and benchmarks on top of the Unreal simulation engine.\n\n\nAirSim\nRobotics simulation environments for flying and driving, built on top of Unreal engine.\n\n\ngym-pybullet-drones\nRobotics simulation environments and tools for quadrotors on top of PyBullet.\n\n\nHabitat 3.0\nSimulation of indoor scenes, humans, and robots. Good for visual navigation and social navigation tasks.\n\n\nGPUDrive\nGPU-accelerated multi-agent driving simulator.\n\n\nProcGen\nProcedurally generated simulation environments (not robotics, but useful).\n\n\nRaiSim\nRigid body physics engine. Supports biomechanics of human motion, as well as quadrupeds.\n\n\nFlightmare\nSimulation environment for flying vehicles built on top of the Unity simulation engine.\n\n\nIKEA Furniture Assembly\nIKEA furniture assembly environment.\n\n\nFurnitureBench\nSimulators, datasets, and real environments for furniture assembly\n\n\nRLBench\nSimulation environments for manipulation, built on top of the CoppeliaSim simulator.\n\n\nALFRED\nSimulation environments for visual and language-based navigation and manipulation tasks.\n\n\nMyoSuite\nMuscosceletal simulation environments for biomechanics, based on Mujoco.\n\n\nMetaWorld\nMulti-task RL environments and benchmarks.\n\n\nBimanual Manipulation Gym\nBimanual manipulation environments\n\n\n\n\n\nRecommended datasets\n\n\n\nSimulately\nA detailed wiki comparing various widely-used robotics datasets. Read this first. The rest of this table shows datasets not mentioned by Simulately.\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\nRoboMimic\nManipulation datasets and imitation learning algorithms\n\n\nMimicGen\nAutomatic augmentation of manipulation datasets starting from human demonstrations\n\n\nOptimus\nAutomatically generating long-horizon manipulation dataset from Task and Motion Planners.\n\n\nDROID\nManipulation dataset across various labs and robots\n\n\nD4RL\nManipulation and navigation datasets for offline RL\n\n\n\n\n\nRecommended RL, IL, trajectory optimization, and motion planning libraries\n\n\n\nSimulately\nA detailed wiki comparing various widely-used RL libaries. Read this first. The rest of this table shows libraries not mentioned by Simulately.\n\n\nRSL RL\nRL library used for training quadrupeds at the RSL lab at ETHZ. Used in Isaac Lab.\n\n\nSTORM\nMPC motion planner on the GPU\n\n\nOMPL\nOpen motion planning library\n\n\nMink\nInverse kinematics library, built on top of pink and pinocchio\n\n\nPureJaxRL\nRL library in JAX, with training and environments running fully on GPU\n\n\nCleanRL\nClean implementations of Online RL baselines\n\n\nClean Offline RL\nClean implementations of Offline RL baselines\n\n\nrliable\nMethod and library for reliable evaluation of RL algorithms\n\n\nDiffusion policy\nImplementation of diffusion policy in action space for imitation learning\n\n\nImplicit behavior cloning\nImplementation of behavior cloning with energy based models\n\n\nTheseus\nA library for differentiable nonlinear optimization in Pytorch\n\n\nModel-based RL algorithms\nList of model-based RL algorithms",
    "crumbs": [
      "Robotics Simulators"
    ]
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\n\n\n\n\n\n\nNote\n\n\n\nThis is a different data source than the one we’ve used in class last week.\n\n\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data"
  },
  {
    "objectID": "labs/lab-4.html#introduction",
    "href": "labs/lab-4.html#introduction",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\n\n\n\n\n\n\nNote\n\n\n\nThis is a different data source than the one we’ve used in class last week.\n\n\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data"
  },
  {
    "objectID": "labs/lab-4.html#getting-started",
    "href": "labs/lab-4.html#getting-started",
    "title": "Lab 4 - The Office",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-4. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-4.html#packages",
    "href": "labs/lab-4.html#packages",
    "title": "Lab 4 - The Office",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(schrute)\nlibrary(lubridate)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-4.html#data-the-office",
    "href": "labs/lab-4.html#data-the-office",
    "title": "Lab 4 - The Office",
    "section": "Data: The Office",
    "text": "Data: The Office\nThe dataset for this lab comes from the schrute package and it’s called theoffice. This dataset contains the entire script transcriptions from The Office.\nLet’s start by taking a peek at the data.\n\nglimpse(theoffice)\n\nRows: 55,130\nColumns: 12\n$ index            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     &lt;chr&gt; \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         &lt;chr&gt; \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           &lt;chr&gt; \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        &lt;chr&gt; \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      &lt;dbl&gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      &lt;int&gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         &lt;fct&gt; 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n\n\nThere are 55130 observations and 12 columns in this dataset. The variable names are as follows.\n\nnames(theoffice)\n\n [1] \"index\"            \"season\"           \"episode\"          \"episode_name\"    \n [5] \"director\"         \"writer\"           \"character\"        \"text\"            \n [9] \"text_w_direction\" \"imdb_rating\"      \"total_votes\"      \"air_date\"        \n\n\nEach row in the dataset is a line spoken by a character in a given episode of the show. This means some information at the episode level (e.g., imdb_rating, air_date, etc. are repeated across the rows that belong to a single episode.\nThe air_date variable is coded as a factor, which is undesirable. We’ll want to parse that variable later into its components during feature engineering. So, for now, let’s convert it to date.\n\ntheoffice &lt;- theoffice %&gt;%\n  mutate(air_date = ymd(as.character(air_date)))\n\nLet’s take a look at the data to confirm we’re happy with how each of the variables are encoded.\n\nglimpse(theoffice)\n\nRows: 55,130\nColumns: 12\n$ index            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     &lt;chr&gt; \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         &lt;chr&gt; \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           &lt;chr&gt; \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        &lt;chr&gt; \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      &lt;dbl&gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      &lt;int&gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         &lt;date&gt; 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005…"
  },
  {
    "objectID": "labs/lab-4.html#exercises",
    "href": "labs/lab-4.html#exercises",
    "title": "Lab 4 - The Office",
    "section": "Exercises",
    "text": "Exercises\n\nData prep\n\nExercise 1\nIdentify episodes that touch on Halloween, Valentine’s Day, and Christmas.\n\nFirst, convert all text to lowercase with str_to_lower().\nThen, create three new variables (halloween_mention, valentine_mention, and christmas_mention) where that take on the value 1 if the character string \"halloween\", \"valentine\", or \"christmas\" appears in the text, respectively, and 0 otherwise.\n\nSome code is provided below to help you get started.\n\ntheoffice &lt;- theoffice %&gt;%\n  mutate(\n    text = ___(text),\n    halloween_mention = if_else(str_detect(text, \"___\"), ___, ___),\n    valentine_mention = ___,\n    ___ = ___\n  )\n\n\n\nExercise 2\nIn this exercise we’ll accomplish two separate tasks. And there’s a good reason why we’re doing it all at once; we’re going to drastically change our data frame, from one row per line spoken to one row per episode. We’ll call the resulting data frame office_episodes.\nThe two tasks are as follows:\n\nTask 1. Identify episodes where the word “halloween”, “valentine”, or “christmas” were ever mentioned, using variables you created above.\nTask 2. Calculate the percentage of lines spoken by Jim, Pam, Michael, and Dwight for each episode of The Office.\n\nBelow are some instructions and starter code to get you started with these tasks.\n\nStart by grouping theoffice data by season, episode, episode_name, imdb_rating, total_votes, and air_date. (These variables, except for season have the same value for each given episode, hence grouping by them allows us to make sure they appear in the output of this pipeline.)\nUse summarize() to calculate the desired features at the season-episode level.\nTask 1:\n\nCalculate the number of lines per season per episode, you might name this new variable n_lines.\nThen, calculate the proportion of lines in that episode spoken by each of the four characters Jim, Pam, Michael, and Dwight. Name these new variables lines_jim, lines_pam, lines_michael, and lines_dwight, respectively.\n\nTask 2:\n\nCreate a variable called halloween that sums up the 1s in halloween_mention at the season-episode level and takes on the value \"yes\" if the sum is greater than or equal to 1, or \"no\" otherwise.\nDo something similar for new variables valentine and christmas as well based on values from valentine_mention and christmas_mention.\n\nFinish up your summarize() statement by dropping the groups, so the resulting data frame is no longer grouped and remove n_lines (we won’t use that variable in our analysis, we only calculated it as an intermediary step).\n\n\noffice_episodes &lt;- theoffice %&gt;%\n  group_by(___) %&gt;%\n  summarize(\n    n_lines = n(),\n    lines_jim = sum(character == \"___\") / n_lines,\n    lines_pam = ___,\n    lines_michael = ___,\n    lines_dwight = ___,\n    halloween = if_else(sum(___) &gt;= 1, \"yes\", \"no\"),\n    valentine = if_else(___, \"___\", \"___\"),\n    christmas = if_else(___, \"___\", \"___\"),\n    .groups = \"drop\"\n  ) %&gt;%\n  select(-n_lines)\n\n\n\n\n\n\n\nNote\n\n\n\nWhy summarize() and not mutate()? We use mutate() to add / modify a column of a data frame. The output data frame always has the same number of rows as the input data frame. On the other hand, we use summarize() to reduce the data frame to either a single row (single summary statistic) or one row per each group (summary statistics at the group level).\nAnd what about that .groups argument in summarize? Try running your summarize() step without it first. You’ll see that R print out a message saying “summarize() has grouped output by season, episode. You can override using the .groups argument.” summarize() will only drop the last group. So if you want a data frame that doesn’t have a grouping structure as a result of a summaerize(), you can explicitly ask for that with .groups = \"drop\". Before you proceed, read the documentation for summarize(), and specifically the explanation for the .groups argument to prepare yourself for future instances where you might see this type of message.\n\n\n\n\nExercise 3\nThe Michael Scott character (played by Steve Carrell) left the show at the end of Season 7. Add an indicator variable, michael, that takes on the value \"yes\" if Michael Scott (Steve Carrell) was in the show, and \"no\" if not.\n\noffice_episodes &lt;- office_episodes %&gt;%\n  mutate(michael = if_else(season &gt; ___, \"___\", \"___\"))\n\n\n\nExercise 4\nPrint out the dimensions (dim()) of the new dataset you created as well as the names() of the columns in the dataset.\nYour new dataset, office_episodes, should have 186 rows and 14 columns. The column names should be season, episode, episode_name, imdb_rating, total_votes, air_date, lines_jim, lines_pam, lines_michael, lines_dwight, halloween, valentine, christmas, and michael. If you are not matching these numbers or columns, go back and try to figure out where you went wrong. Or ask your TA for help!\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\nIt’s also a good place to let another team member take over the keyboard! A team member who hasn’t done so yet should pull the changes and make the commits for the next few exercises.\n\n\n\n\nExploratory data analysis\nThis would be a good place to conduct some exploratory data analysis (EDA). For example, plot the proportion of lines spoken by each character over time. Or calculate the percentage of episodes that mention Halloween, or Valentine’s Day, or Christmas. Given we have limited time in the lab we’re not going to ask you to report EDA results as part of this lab, but we’re noting this here to provide suggestions for how you might go about structuring your project.\n\n\nModeling prep\n\nExercise 5\nSplit the data into training (75%) and testing (25%). Save the training and testing data as office_train and office_test respectively.\nNaming suggestion: Call the initial split office_split, the training data office_train, and testing data office_test.\n\nset.seed(123)\noffice_split &lt;- ___(office_episodes)\noffice_train &lt;- ___(office_split)\noffice_test &lt;- ___(___)\n\n\n\nExercise 6\nSpecify a linear regression model with engine \"lm\" and call it office_spec.\nNaming suggestion: Call the model specification office_spec.\n\noffice_spec &lt;- ___\n\n\n\nExercise 7\nCreate a recipe that performs feature engineering using the following steps (in the given order):\n\nupdate_role(): updates the role of episode_name to not be a predictor (be an ID)\nstep_rm(): removes air_date as a predictor\nstep_dummy(): creates dummy variables for all_nominal_predictors()\nstep_zv(): removes all zero variance predictors\n\nNaming suggestion: Call the recipe office_rec.\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  ___\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\nIt’s also a good place to let another team member take over the keyboard! A team member who hasn’t done so yet should pull the changes and make the commits for the next few exercises.\n\n\n\nExercise 8\nBuild a model workflow for fitting the model specified earlier and using the recipe you developed to preprocess the data.\nNaming suggestion: Call the model workflow office_wflow.\n\noffice_wflow &lt;- workflow() %&gt;%\n  add_model(___) %&gt;%\n  add_recipe(___)\n\n\n\n\nModel fit and evaluation\n\nExercise 9\nFit the model to training data, neatly display the model output, and interpret two of the slope coefficients.\nNaming suggestion: Call the model fit office_fit.\n\noffice_fit &lt;- office_wflow %&gt;%\n  fit(data = ___)\n\n___\n\n\n\nExercise 10\nCalculate predicted imdb_rating for the training data using the predict() function. Then, bind two columns from the training data to this result: imdb_rating and episode_name. The resulting data frame should have three columns: .pred, imdb_rating, and episode_name. Then, using this data frame, create a scatterplot of predicted and observed IMDB ratings for the training data.\nNaming suggestion: Call the resulting data frame office_train_pred.\nStretch goal. Add episode names, using geom_text(), for episodes with much higher and much lower observed IMDB ratings compared to others.\n\n\nExercise 11\nCalculate the R-squared and RMSE for this model for predictions on the training data.\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\nIt’s also a good place to let another team member take over the keyboard! A team member who hasn’t done so yet should pull the changes and make the commits for the next few exercises.\n\n\n\nExercise 12\nRepeat Exercise 10, but with testing data.\nNaming suggestion: Call the resulting data frame office_test_pred.\n\n\nExercise 13\nBased on your visualization on Exercise 12, speculate on whether you expect the R-squared and RMSE for this model to be higher or lower for predictions on the testing data compared to those on the training data, or do you expect them to be the same? Explain your reasoning.\n\n\nExercise 14\nCheck your intuition in Exercise 13 by actually calculating the R-squared and RMSE for this model for predictions on the training data. Comment on whether your intuition is confirmed or not."
  },
  {
    "objectID": "labs/lab-4.html#submission",
    "href": "labs/lab-4.html#submission",
    "title": "Lab 4 - The Office",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-4.html#grading",
    "href": "labs/lab-4.html#grading",
    "title": "Lab 4 - The Office",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-4.html#footnotes",
    "href": "labs/lab-4.html#footnotes",
    "title": "Lab 4 - The Office",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Lab 1 - Meet the toolkit",
    "section": "",
    "text": "This lab will go through much of the same workflow we’ve demonstrated in class. The main goal is to reinforce our understanding of R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nAn additional goal is to reinforce git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nTo make versioning simpler, this is a solo lab. In the future, you’ll learn about collaborating on GitHub and producing a single lab report for your lab team, but for now, concentrate on getting the basics down.\n\n\nBy the end of the lab, you will…\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nGain practice writing a reproducible report using RMarkdown\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2\nBe able to describe variable distributions and the relationship between multiple variables"
  },
  {
    "objectID": "labs/lab-1.html#introduction",
    "href": "labs/lab-1.html#introduction",
    "title": "Lab 1 - Meet the toolkit",
    "section": "",
    "text": "This lab will go through much of the same workflow we’ve demonstrated in class. The main goal is to reinforce our understanding of R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nAn additional goal is to reinforce git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nTo make versioning simpler, this is a solo lab. In the future, you’ll learn about collaborating on GitHub and producing a single lab report for your lab team, but for now, concentrate on getting the basics down.\n\n\nBy the end of the lab, you will…\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nGain practice writing a reproducible report using RMarkdown\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2\nBe able to describe variable distributions and the relationship between multiple variables"
  },
  {
    "objectID": "labs/lab-1.html#getting-started",
    "href": "labs/lab-1.html#getting-started",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Getting started",
    "text": "Getting started\n\n\n\n\n\n\nImportant\n\n\n\nYour lab TA will lead you through the Getting Started section.\n\n\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you haven’t yet done so, you will need to reserve a container for STA210 first.\n\n\n\n\nSet up your SSH key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nType credentials::ssh_setup_github() into your console.\nR will ask “No SSH key found. Generate one now?” You should click 1 for yes.\nYou will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” You should click 1 for yes.\nYou may be asked to provide your GitHub username and password to log into GitHub. After entering this information, you should paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta210).\n\nYou can find more detailed instructions here if you’re interested.\n\n\nConfigure Git\nThere is one more thing we need to do before getting started on the assignment. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"GitHub username\", \n  user.email = \"Email associated with your GitHub account\"\n  )\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"mine-cetinkaya-rundel\", \n  user.email = \"cetinkaya.mine@gmail.com\"\n  )\n\nYou are now ready interact with GitHub via RStudio!\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix lab-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-1-ikea.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab.\n\n\n\nR and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\nYAML\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (`.qmd`) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-1.html#packages",
    "href": "labs/lab-1.html#packages",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nThe tidyverse is a meta-package. When you load it you get eight packages loaded for you:\n\nggplot2: for data visualization\ndplyr: for data wrangling\ntidyr: for data tidying and rectangling\nreadr: for reading and writing data\ntibble: for modern, tidy data frames\nstringr: for string manipulation\nforcats: for dealing with factors\npurrr: for iteration with functional programming\n\nThe message that’s printed when you load the package tells you which versions of these packages are loaded as well as any conflicts they may have introduced, e.g., the filter() function from dplyr has now masked (overwritten) the filter() function available in base R (and that’s ok, we’ll use dplyr::filter() anyway).\nWe’ll be using functionality from all of these packages throughout the semester, though we’ll always load them all at once with library(tidyverse). You can find out more about the tidyverse and each of the packages that make it up here."
  },
  {
    "objectID": "labs/lab-1.html#data-ikea-furniture",
    "href": "labs/lab-1.html#data-ikea-furniture",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Data: Ikea furniture",
    "text": "Data: Ikea furniture\nToday’s data is all about Ikea furniture. The data was obtained from the TidyTuesday data collection.\nUse the code below to read in the data.\n\nikea &lt;- read_csv(\"data/ikea.csv\")\n\n\nData dictionary\nThe variable definitions are as follows:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nitem_id\ndouble\nitem id which can be used later to merge with other IKEA data frames\n\n\nname\ncharacter\nthe commercial name of items\n\n\ncategory\ncharacter\nthe furniture category that the item belongs to (Sofas, beds, chairs, Trolleys,…)\n\n\nsellable_online\nlogical\nSellable online TRUE or FALSE\n\n\nlink\ncharacter\nthe web link of the item\n\n\nother_colors\ncharacter\nif other colors are available for the item, or just one color as displayed in the website (Boolean)\n\n\nshort_description\ncharacter\na brief description of the item\n\n\ndesigner\ncharacter\nThe name of the designer who designed the item. this is extracted from the full_description column.\n\n\ndepth\ndouble\nDepth of the item in Centimeter\n\n\nheight\ndouble\nHeight of the item in Centimeter\n\n\nwidth\ndouble\nWidth of the item in Centimeter\n\n\nprice_usd\ndouble\nthe current price in US dollars as it is shown in the website by 4/20/2020\n\n\n\n\n\nView the data\nBefore doing any analysis, you may want to get quick view of the data. This is useful when you’ve imported data to see if your data imported correctly. We can use the view() function to see the entire data set in RStudio. Type the code below in the Console to view the entire dataset.\n\nview(ikea)"
  },
  {
    "objectID": "labs/lab-1.html#exercises",
    "href": "labs/lab-1.html#exercises",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Exercises",
    "text": "Exercises\nWrite all code and narrative in your R Markdown file. Write all narrative in complete sentences. Throughout the assignment, you should periodically Render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all or your code in your PDF document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (%&gt;%) and plus sign (+).\n\n\n\nExercise 1\nThe view() function helped us get a quick view of the dataset, but let’s get more detail about its structure. Viewing a summary of the data is a useful starting point for data analysis, especially if the dataset has a large number of observations (rows) or variables (columns). Run the code below to use the glimpse() function to see a summary of the ikea dataset.\nHow many observations are in the ikea dataset? How many variables?\n\nglimpse(ikea)\n\n\n\n\n\n\n\nNote\n\n\n\nIn your lab-1-ikea.qmd document you’ll see that we already added the code required for the exercise as well as a sentence where you can fill in the blanks to report the answer. Use this format for the remaining exercises.\nAlso note that the code chunk as a label: glimpse-data. It’s not required, but good practice and highly encouraged to label your code chunks in this way.\n\n\n\n\nExercise 2\nWe begin each regression analysis with exploratory data analysis (EDA) to help us “get to know” the data and examine the variable distributions and relationships between variables. We do this by visualizing the data and calculating summary statistics to describe the variables in our dataset. In this lab, we will focus on data visualizations.\nLet’s begin by looking at the price of Ikea furniture. Use the code below to visualize the distribution of price_usd, the price in US dollars.\n\nggplot(data = ikea, aes(x = price_usd)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nUse the visualization to describe the distribution of price. In your narrative, include description of the shape, approximate center, approximate spread, and any presence of outliers. Briefly explain why the median is more representative of the center of this distribution than the mean.\n\n\n\n\n\n\nTip\n\n\n\nWhen using the visual editor you can insert a code chunk using the Insert menu on top or by using the catch-all ⌘ / shortcut to insert just about anything. Just execute the shortcut then type what you want to insert. If you are at the beginning of a line you can also enter plain / to invoke the shortcut.\n\n\n\n\nExercise 3\nWhen we make visualizations, we want them to be clear and suitable for a professional audience. This means that, at a minimum, each visualization should have an informative title and informative axis labels. Let’s modify the plot from the previous question to make it suitable for a professional audience. Complete the code below to include an informative title and informative axis labels.\n\nggplot(data = ikea, aes(x = price_usd)) +\n  geom_histogram() +\n  labs(\n    x = \"_____\",\n    y = \"_____\",\n    title = \"_____\"\n  )\n\n\nThis is a good place to fender, commit, and push changes to your remote lab-1 repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 1 - 3”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nAnother way to visualize numeric data is using density plots. Make a density plot to visualize the distribution of price_usd. Be sure to include an informative title and informative axis labels.\nIn this course, we’ll be most interested in the relationship between two or more variables, so let’s begin by looking at the distribution of price by category. We’ll focus on the five categories in the code below, since these include commonly purchased types of furniture.\nUse the code below to create a new data frame that only includes the furniture categories of interest. We’re assigning this data frame to an object with a new name, so we don’t overwrite the original data.\nHow many observations are in the ikea_sub dataset? How many variables?\n\nikea_sub &lt;- ikea %&gt;%\n  filter(category %in% c(\n    \"Tables & desks\", \"Beds\",\n    \"Bookcases & shelving units\",\n    \"Sofas & armchairs\", \"Children's furniture\"\n  ))\n\n\n\n\n\n\n\nImportant\n\n\n\nYou will use this newly constructed data frame, ikea_sub, for the remainder of the lab.\n\n\n\n\nExercise 5\nLet’s make a new visualization with the density curves colored by category, so we can compare the distribution of price for each category.\n\nggplot(data = ikea_sub, aes(x = price_usd, fill = category)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe overlapping colors make it difficult to tell what’s happening with the distributions for the categories plotted first and hence covered by categories plotted over them. We can change the transparency level of the fill color to help with this. The alpha argument takes values between 0 and 1: 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so it’s best to try a few.\nRecreate the density plot using a more suitable alpha level, so we can more easily see the distribution of all the categories. Include an informative title and informative axis labels.\n\nggplot(data = ikea_sub, aes(x = price_usd, fill = category)) +\n  geom_density(alpha = 0.8)\n\n\n\n\n\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab-1 repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 4 and 5”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 6\nBriefly describe why we defined the fill of the curves by mapping aesthetics of the plot (inside the aes function) but we defined the alpha level as a characteristic of the plotting geom.\n\n\nExercise 7\nOverlapping density plots are not the only way to visualize the relationship between a quantitative and categorical variable.\nUse a different type of plot to visualize the relationship between price_usd and category. Include an informative title and informative axis labels.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the ggplot2 cheatsheet and from Data to Viz for inspiration.\n\n\n\n\nExercise 8\nCompare and contrast your plots from the previous exercise to the overlapping density plots from Exercise 5. What features are apparent in the plot from the previous exercise that aren’t in the overlapping density plots? What features are apparent in the overlapping density plots that aren’t in the plot from the previous exercise? What features are apparent in both?\n\nThis is a good place to render, commit, and push changes to your remote lab-1 repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 6 - 8”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 9\nNext, let’s look at the relationship between the price and width of Ikea furniture. Fill in the code below to visualize the relationship between the two variables using a scatterplot.\nThen, use your visualization to describe the relationship between the width and price of Ikea furniture.\n\nggplot(data = _____, aes(x = width, y = _____)) +\n  geom_point() + \n  labs(\n    x = \"_____\", \n    y = \"_____\", \n    title = \"_____\"\n    )\n\n\n\nExercise 10\nColor the points of the scatterplot by category. Describe how the relationship between price and width of Ikea furniture differs by category, if at all.\n\nYou’re done and ready to submit your work! Render, commit, and push all remaining changes. You can use the commit message “Done with Lab 1!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "labs/lab-1.html#submission",
    "href": "labs/lab-1.html#submission",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Submission",
    "text": "Submission\nIn this class, we’ll be submitting PDF documents to Gradescope.\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-1.html#grading",
    "href": "labs/lab-1.html#grading",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-1.html#resources-for-additional-practice-optional",
    "href": "labs/lab-1.html#resources-for-additional-practice-optional",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Resources for additional practice (optional)",
    "text": "Resources for additional practice (optional)\n\nChapter 2: Get Started Data Visualization by Kieran Healy\nChapter 3: Data visualization in R for Data Science by Hadley Wickham\nRStudio Cloud Primers\n\nVisualization Basics: https://rstudio.cloud/learn/primers/1.1\nWork with Data: https://rstudio.cloud/learn/primers/2\nVisualize Data: https://rstudio.cloud/learn/primers/3"
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2 - College scorecard",
    "section": "",
    "text": "In today’s lab, you’ll use simple linear regression to analyze the relationship between the admissions rate and total cost for colleges and universities in the United States.\n\n\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use statistical inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-2.html#introduction",
    "href": "labs/lab-2.html#introduction",
    "title": "Lab 2 - College scorecard",
    "section": "",
    "text": "In today’s lab, you’ll use simple linear regression to analyze the relationship between the admissions rate and total cost for colleges and universities in the United States.\n\n\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use statistical inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-2.html#getting-started",
    "href": "labs/lab-2.html#getting-started",
    "title": "Lab 2 - College scorecard",
    "section": "Getting started",
    "text": "Getting started\n\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-2. It contains the starter documents you need to complete the lab.\nClone the repo and start a new project in RStudio. See the Lab 1 instructions for details on cloning a repo, starting a new R project and configuring git."
  },
  {
    "objectID": "labs/lab-2.html#packages",
    "href": "labs/lab-2.html#packages",
    "title": "Lab 2 - College scorecard",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\nlibrary(tidyverse)  # for data wrangling + visualization\nlibrary(tidymodels) # for modeling\nlibrary(knitr)      # for pretty printing of tables"
  },
  {
    "objectID": "labs/lab-2.html#data-college-scorecard",
    "href": "labs/lab-2.html#data-college-scorecard",
    "title": "Lab 2 - College scorecard",
    "section": "Data: College scorecard",
    "text": "Data: College scorecard\nThe data for this lab is from the scorecard data set in the rcfss R package. It includes information originally obtained from the U.S. Department of Education’s College Scorecard for 1753 colleges and universities during the 2018 - 2019 academic year.\nThe lab focuses on the following variables:\n\nadmrate: Undergraduate admissions rate (from 0-100%)\ncost: The average annual total cost of attendance, including tuition and fees, books and supplies, and living expenses\ntype: Type of college (Public; Private, nonprofit; Private, for-profit)\n\nClick here to see a full list of variables and definitions.\nUse the code below to load the data set.\n\nscorecard &lt;- read_csv(\"data/scorecard.csv\")"
  },
  {
    "objectID": "labs/lab-2.html#exercises",
    "href": "labs/lab-2.html#exercises",
    "title": "Lab 2 - College scorecard",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\nInclude axis labels and an informative title for all plots. Use the kable() function to neatly print tables and regression output.\n\n\n\nExercise 1\nCreate a histogram to examine the distribution of admrate and calculate summary statistics for the center (mean and median) and the spread (standard deviation and IQR).\n\n\nExercise 2\nUse the results from the previous exercise to describe the distribution of admrate. Include the shape, center, spread, and if there are potential outliers.\n\n\nExercise 3\nPlot the distribution of cost and calculate the appropriate summary statistics. Describe the distribution of cost (shape, center, and spread, and outliers) using the plot and appropriate summary statistics.\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nThe goal of this analysis is to fit a regression model that can be used to understand the variability in the cost of college based on the admission rate. Before fitting the model, let’s look at the relationship between the two variables. Create a scatterplot to display the relationship between cost and admissions rate. Describe the relationship between the two variables based on the plot.\n\n\nExercise 5\nDoes the relationship between cost and admissions rate differ by type of college? Modify the plot from the previous exercise visualize the relationship by type of college.\n\n\nExercise 6\nDescribe two new observations from the scatterplot in Exercise 5 that you didn’t see in the scatterplot from Exercise 4.\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 7\nFit the linear regression model. Use the kable function to neatly display the results with a reasonable number of decimals.\n\n\nExercise 8\nConsider the model from the previous exercise.\n\nInterpret the slope in the context of the problem.\nDoes the intercept have a meaningful interpretation? If so, write the interpretation in the context of the problem. Otherwise, explain why the interpretation is not meaningful.\n\n\n\nExercise 9\nConstruct a 95% confidence interval for the slope using bootstrapping. Follow these steps to accomplish this:\n\nFirst set a seed for simulating reproducibly.\nThen, simulate the bootstrap distribution of the slope using 1,000 bootstrap samples.\nThen, visually estimate the bounds of the bootstrap interval based on a histogram of the distribution of the bootstrapped slopes, using the percentile method.\nAnd then, use the get_confidence_interval() function to explicitly calculate the bounds of the confidence interval using the percentile method.\nFinally, interpret the confidence interval in the context of the data.\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 10\nFinally, we want to answer the question “Do the data provide sufficient evidence of a linear relationship between cost and admissions rate, i.e. \\(\\beta_1\\) is different from 0?”\nTo answer this question we will use a hypothesis test. We can conduct a hypothesis test via simulation (what we’ll do in this lab) or using mathematical models (what we’ll do in the next class).\nBefore we can conduct the hypothesis test, let’s first set our hypotheses. Remember that the null hypothesis represents the status quo (nothing going on, i.e. there is no relationship) and the alternative hypothesis represents our research question (there is something going on, i.e. there is a relationship).\n\n\\(H_0\\): There is no linear relationship between the admissions rate and cost of colleges in the United States, \\(\\beta_1 = 0\\)\n\\(H_A\\): There is a linear relationship between the admissions rate and cost of colleges in the United States, \\(\\beta_1 \\ne 0\\)\n\nTo test these hypotheses, we will use a permutation test, where we\n\nSimulate new samples from the original sample via permutation under the assumption that the null hypothesis is true\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to calculate the p-value for the hypothesis test\n\nThe major difference between constructing a confidence interval and conducting a hypothesis test is that for the hypothesis test we assume that the null hypothesis is true. This requires a simulation scheme that will allow us to measure the natural variability in the data due to sampling but not due to cost and admission rate being correlated by permuting permute one variable to eliminate any existing relationship between the variables. To do so, we randomly assign each admrate value to cost of a given university, i.e. cost and admrate are no longer matched for a given university.\nIn the following code chunk we\n\nFirst set a seed for simulating reproducibly.\nThen, we start with our data frame and specify our model as cost vs. admrate.\nThen, we set our null hypothesis (cost and admrate are independent)\nAnd then we generate 1000 replicates of our data where, for each replicate, we permute values of admrate to randomly assign them to values of cost\nFinally, we fit our model to each of our 1000 permuted datasets\n\n\nset.seed(1234)\n\nperm_fits &lt;- scorecard %&gt;%\n  specify(cost ~ admrate) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  fit()\n\nThe resulting dataset perm_fits has nrow(perm_fits) and ncol(perm_fits) columns. The first column, replicate indicates the replicate number of the dataset the models were fit to; the values in this column range between 1 and 1000. The second column, term, tells us which term (intercept of the model or slope of admrate) the estimate value in the third column is for.\n\nperm_fits\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept  36857. \n 2         1 admrate     -781. \n 3         2 intercept  35901. \n 4         2 admrate      643. \n 5         3 intercept  36608. \n 6         3 admrate     -411. \n 7         4 intercept  35831. \n 8         4 admrate      746. \n 9         5 intercept  36367. \n10         5 admrate      -51.7\n# … with 1,990 more rows\n\n\n\nCreate a histogram of the slope estimates in perm_fits. (Hint: Filter the dataset for just the slope values, term == \"admrate\".)\nEstimate the p-value of the hypothesis test based on this distribution.\nState your conclusion for the test in context.\nIndicate whether or not it is consistent with the results of the hypothesis test from the previous exercise. Briefly explain your response.\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-2.html#submission",
    "href": "labs/lab-2.html#submission",
    "title": "Lab 2 - College scorecard",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-2.html#grading",
    "href": "labs/lab-2.html#grading",
    "title": "Lab 2 - College scorecard",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3 - Coffee ratings",
    "section": "",
    "text": "In today’s lab you will analyze data from over 1,000 different coffees to explore the relationship between a coffee’s aroma and it’s overall quality. You will also begin working with your team and practicing a collaborative data analysis workflow.\n\n\nBy the end of the lab you will…\n\nCreate plots and calculate associated statistics to assess model diagnostics.\nPractice collaborating with others using a single Github repo."
  },
  {
    "objectID": "labs/lab-3.html#introduction",
    "href": "labs/lab-3.html#introduction",
    "title": "Lab 3 - Coffee ratings",
    "section": "",
    "text": "In today’s lab you will analyze data from over 1,000 different coffees to explore the relationship between a coffee’s aroma and it’s overall quality. You will also begin working with your team and practicing a collaborative data analysis workflow.\n\n\nBy the end of the lab you will…\n\nCreate plots and calculate associated statistics to assess model diagnostics.\nPractice collaborating with others using a single Github repo."
  },
  {
    "objectID": "labs/lab-3.html#meet-your-team",
    "href": "labs/lab-3.html#meet-your-team",
    "title": "Lab 3 - Coffee ratings",
    "section": "Meet your team!",
    "text": "Meet your team!\nClick here to see the team assignments for STA 210. This will be your team for labs and the final project.\nBefore you get started on the lab, your TA will walk you through the following:\n\nIcebreaker activity to get to know your teammates.\nCome up with a team name. You can’t use the same name as another team, so I encourage you to be creative! Your TA will get your team name by the end of lab.\nFill out the team agreement. This will help you figure out a plan for communication and working together during labs and outside of lab times. You can find the team agreement in the GitHub repo team-agreement-[github_team_name].\nHave one person from the team clone the repo and start a new RStudio project. This person will type the team’s responses as you discuss the sections of the agreement. No one else in the team should type at this point but should be contributing to the discussion.\nBe sure to push the completed agreement to GitHub. Each team member can refer to the document in this repo or download the PDF of the agreement for future reference. You do not need to submit the agreement on Gradescope."
  },
  {
    "objectID": "labs/lab-3.html#getting-started",
    "href": "labs/lab-3.html#getting-started",
    "title": "Lab 3 - Coffee ratings",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-3. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Do not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-3.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-3.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 3 - Coffee ratings",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nAssign each person on your team a number 1 through 4. For teams of three, Team Member 1 can take on the role of Team Member 4.\n\n\nThe following exercises must be done in order. Only one person should type in the .qmd file, commit, and push updates at a time. When it is not your turn to type, you should still share ideas and contribute to the team’s discussion.\n\n\n\n\n\n\n⌨️ Team Member 1: Hands on the keyboard.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!1\n\n\n\nChange the author to your team name and include each team member’s name in the author field of the YAML in the following format: Team Name: Member 1, Member 2, Member 3, Member 4.\n\nTeam Member 1: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub.\n\n\nTeam Members 2, 3, 4: Once Team Member 1 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the updated name in your .qmd file."
  },
  {
    "objectID": "labs/lab-3.html#packages",
    "href": "labs/lab-3.html#packages",
    "title": "Lab 3 - Coffee ratings",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(ggfortify)"
  },
  {
    "objectID": "labs/lab-3.html#data-coffee-ratings",
    "href": "labs/lab-3.html#data-coffee-ratings",
    "title": "Lab 3 - Coffee ratings",
    "section": "Data: Coffee ratings",
    "text": "Data: Coffee ratings\nThe dataset for this lab comes from the Coffee Quality Database and was obtained from the #TidyTuesday GitHub repo. It includes information about the origin, producer, measures of various characteristics, and the quality measure for over 1000 coffees.\nThis lab will focus on the following variables:\n\naroma: Aroma grade, 0 - 10 scale\ntotal_cup_points: Measure of quality, 0 - 100 scale\n\nYou can find the definitions for all variables in the data set here. Click here for more details about how these measures are obtained.\n\ncoffee_ratings &lt;- read_csv(\"data/coffee_ratings.csv\")"
  },
  {
    "objectID": "labs/lab-3.html#exercises",
    "href": "labs/lab-3.html#exercises",
    "title": "Lab 3 - Coffee ratings",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\n\nInclude axis labels and an informative title for all plots.\nUse the kable function to neatly print tables and regression output. Write all interpretations in the context of the data.\nDo the following exercises in order, following each step carefully.\nOnly one person at a time should type in the .qmd file and push updates.\nIf you are working on any portion of the lab virtually, the person working should share their screen and the others should follow along.\n\n\n\n\n\n\n\n\n\n⌨️ Team Member 1: Hands still on the keyboard. Write the answers to Exercises 1 and 2.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\nExercise 1\nVisualize the relationship between aroma and the total cup points. What do you observe from the plot? Use the plot the describe the relationship between the two variables.\n\n\nExercise 2\nFit the linear model and neatly display the results using 3 digits. Interpret the slope in context of the data.\n\nTeam Member 1: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 2, 3, 4: Once Team Member 1 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 1 and 2 in your .qmd file.\n\n\nNow it’s time for a hand off…\n\n\n\n\n\n\n⌨️ Team Member 2: Hands on the keyboard. Write the answers to Exercises 3 and 4.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 3\nWould the members of your group drink a coffee represented by the intercept? Why or why not? Discuss as a group and write the group’s consensus.\n\n\nExercise 4\nLeverage is the measure of the distance between an observation’s values of the predictor variables and the average values of the predictor variables for the entire data set. An observation s set if have high leverage if its combination of values for the predictor variables is very far from the typical combination of values in the data.An observation has high leverage if its combination of values for the predictor variables is very far from the typical combination of values in the data. Observations with high leverage should be considered as potential influential points.\nWe will proceed assuming the model conditions hold, so let’s focus on the model diagnostics. We’ll start by examining if there are any points with high leverage in the data.\nTheoretically, the leverage of the \\(i^{th}\\) observation as follows:\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j = 1}^n (x_j - \\bar{x})^2}\n\\]\nNote that leverage only depends on values of the predictor variable(s).\nThe sum of the leverages for all points is \\(p + 1\\), where\n\n\\(p\\) is the number of predictors\nIn the case of SLR, \\(\\sum_{i = 1}^n h_i = 2\\)\nThe “typical” leverage is \\(\\frac{(p + 1)}{n}\\)\n\nTherefore, an observation is said to have high leverage if\n\\[\nh_i &gt; \\frac{2(p + 1)}{n}\n\\]\nIn addition to comparing the leverage of points to a threshold, we also generally visualize standard residuals vs. leverage values our data. The autoplot() function from the ggfortify package is very useful for drawing these standard plots easily.\n\nautoplot(coffee_fit$fit, which = 5)\n\n\nWhat threshold will you use to determine if there are points with high leverage for this dataset?\nAre there any observations with high leverage? If so, how many? Briefly explain, including any output, graphs, etc. you used to determine the response. Improve your plot by adding a new year to draw a vertical line (with geom_vline()) at the value of the threshold you’re using to determine which points have high leverage.\n\n\nTeam Member 2: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 3, 4: Once Team Member 2 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 3 and 4 in your .qmd file.\n\n\nNow it’s time for another hand off…\n\n\n\n\n\n\n⌨️ Team Member 3: Hands on the keyboard. Write the answers to Exercises 5.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 5\nAnother standard model diagnostic involves identifying points that don’t fit the pattern from the regression line. We do this by determining which points have large standardized residuals (residual divided by the standard error of residuals).\n\\[\nStd.~res_i = \\frac{y_i - \\hat{y}_1}{\\hat{\\sigma}_\\epsilon ~ \\sqrt{1 - h_i}},\n\\]\nwhere \\(\\hat{\\sigma}_\\epsilon\\) is the regression standard error.\n\n\n\n\n\n\nNote\n\n\n\nThese values are already calculated in the output of augment().\n\n\nObservations that have standardized residuals of large magnitude (usually beyond \\(\\pm\\) 3) are potential outliers, since they don’t fit the pattern determined by the regression model. Therefore, a common practice is to plot standardized residuals vs. fitted values, to make it easier to identify outliers.\nWe can obtain this plot with the following:\n\nautoplot(coffee_fit$fit, which = 3)\n\nCreate this visualization and horizontal lines (with geom_hline()) at the cutoff values for “large” standardized residuals (\\(\\pm\\) 3). Are there any such points in the data? If so, how many? Briefly explain, including any output, graphs, etc. you used to determine the response.\n\nTeam Member 3: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 4: Once Team Member 3 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercise 5 and 4 in your .qmd file.\n\n\nNow it’s time for another hand off…\n\n\n\n\n\n\n⌨️ Team Member 4: Hands on the keyboard. Write the answers to Exercises 6.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 6\nFinally, we’ll examine Cook’s Distance. An observation’s influence on the regression line depends on how close it lies to the general trend of the data (i.e., its standardized residual) and it’s leverage (\\(h_i\\)). Cook’s Distance is a statistic that includes both of these components to measure an observation’s overall impact on the model. Cook’s Distance for the \\(i^{th}\\) observation is defined as the follows:\n\\[\nD_i = \\frac{(std.~res)^2}{p + 1} (\\frac{h_i}{1-\\frac{h_i})\n\\]\nAn observation with large \\(D_i\\) is said to have a strong influence on the predicted values. On that scale,\n\n\\(D_i\\) &gt; 0.5 is moderately influential\n\\(D_i\\) &gt; 1 is very influential\n\nWe can plot of Cook’s distances vs. the observation number with the following:\n\nautoplot(coffee_fit$fit, which = 4, ncol = 1)\n\n\n\n\n\n\n\n\nStandardized residuals, leverage, and Cook’s Distance should all be examined together. So what do we do with observations identified as outliers or leverage points?\nIt is OK to drop an observation based on the predictor variables if…\n\nIt is meaningful to drop the observation given the context of the problem\nYou intended to build a model on a smaller range of the predictor variables. You should mention this in the write up of the results and be careful to avoid extrapolation when making predictions.\n\nIt is not OK to drop an observation based on the response variable if…\n\nThese are legitimate observations and should be in the model.\nYou can try transformations or increasing the sample size by collecting more data.\n\nSo lastly, let’s analyze Cook’s D to determine if there are influential points in the data.\n\nBased on Cook’s D, are there any influential points in our data? Briefly explain, including any output, graphs, etc. you used to determine the response.\nIf there are influential points, briefly explain why they are outliers, i.e., not in the trend of the rest of the data.\nIf there are influential points, remove those points from the data and refit the model. How do the model coefficients change, if at all?\nIf there are influential points, would you recommend using the model fit with or without these points for inferential conclusions and predictions? Briefly explain why or why not. Additionally, briefly explain potential impacts your choice has on inferential conclusions and/or predictions.\n\n\nTeam Member 4: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 3: Once Team Member 4 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercise 6 and 4 in your .qmd file.\n\n\nNow it’s time for one last hand off…"
  },
  {
    "objectID": "labs/lab-3.html#wrapping-up",
    "href": "labs/lab-3.html#wrapping-up",
    "title": "Lab 3 - Coffee ratings",
    "section": "Wrapping up",
    "text": "Wrapping up\n\n\n\n\n\n\nImportant\n\n\n\n⌨️ Team Member 2: Hands on the keyboard. Make any edits as needed.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\nTeam Member 2: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 3, 4: Once Team Member 2 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the final version of your .qmd file."
  },
  {
    "objectID": "labs/lab-3.html#submission",
    "href": "labs/lab-3.html#submission",
    "title": "Lab 3 - Coffee ratings",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nSelect one team member to upload the team’s PDF submission to Gradescope.\nBe sure to include every team member’s name in the Gradescope submission.\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”). If any answer spans multiple pages, then mark all pages.\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section.\n\n\n\n\n\n\n\nImportant\n\n\n\nThere should only be one submission per team on Gradescope."
  },
  {
    "objectID": "labs/lab-3.html#grading",
    "href": "labs/lab-3.html#grading",
    "title": "Lab 3 - Coffee ratings",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 6\n42\n\n\nWorkflow & formatting\n52\n\n\nComplete team contract\n3"
  },
  {
    "objectID": "labs/lab-3.html#footnotes",
    "href": "labs/lab-3.html#footnotes",
    "title": "Lab 3 - Coffee ratings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon’t trust yourself to keep your hands off the keyboard? Put them in your picket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-0.html",
    "href": "labs/lab-0.html",
    "title": "Lab 0 - Meet + greet",
    "section": "",
    "text": "Today’s lab is short and sweet! We just need you to fill out the “Getting to know you” survey. Please go here to take it. You will need to log on to Sakai to access to survey.\nYour answers can be brief. Some of your answers will be used to guide what application examples might be of interest to a majority of students in the course and some of your answers will be used to help guide team formation. We expect this will take you ~10 minutes."
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 5 - General Social Survey",
    "section": "",
    "text": "In today’s lab you will analyze data from the General Social Survey.\n\n\nBy the end of the lab you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model"
  },
  {
    "objectID": "labs/lab-5.html#introduction",
    "href": "labs/lab-5.html#introduction",
    "title": "Lab 5 - General Social Survey",
    "section": "",
    "text": "In today’s lab you will analyze data from the General Social Survey.\n\n\nBy the end of the lab you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model"
  },
  {
    "objectID": "labs/lab-5.html#getting-started",
    "href": "labs/lab-5.html#getting-started",
    "title": "Lab 5 - General Social Survey",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-5. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-5.html#packages",
    "href": "labs/lab-5.html#packages",
    "title": "Lab 5 - General Social Survey",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-5.html#data-general-social-survey",
    "href": "labs/lab-5.html#data-general-social-survey",
    "title": "Lab 5 - General Social Survey",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\n\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\n\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "labs/lab-5.html#exercises",
    "href": "labs/lab-5.html#exercises",
    "title": "Lab 5 - General Social Survey",
    "section": "Exercises",
    "text": "Exercises\nThe goal of today’s lab is to use the GSS to examine the relationship between US adults’ political views and attitudes towards government spending on mass transportation projects.\n\nPart I: Exploratory data analysis\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then make a plot of the new variable, using informative labels for each category.\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\nMake a plot of the distribution of polviews.\nWhich political view occurs most frequently in this data set?\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\nWe’d like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as \"89 or older\".\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values \"89 or older\" with a single value.\nThen plot the distribution of age.\n\n\n\n\nPart II: Logistic regression model\n\nBriefly explain why we should use a logistic regression model to predict the odds a randomly selected person is satisfied with spending on mass transportation.\nLet’s start by fitting a model using the demographic factors - age, sex, sei10, and region - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model.\nInterpret the intercept in the context of the data.\nConsider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the odds of being satisfied with spending on mass transportation."
  },
  {
    "objectID": "labs/lab-5.html#submission",
    "href": "labs/lab-5.html#submission",
    "title": "Lab 5 - General Social Survey",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-5.html#grading",
    "href": "labs/lab-5.html#grading",
    "title": "Lab 5 - General Social Survey",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-5.html#footnotes",
    "href": "labs/lab-5.html#footnotes",
    "title": "Lab 5 - General Social Survey",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "",
    "text": "In today’s lab you will analyze data from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”.\n\n\nBy the end of the lab you will be able to…\n\nConduct exploratory data analysis for multinomial logistic regression.\nFit and interpret coefficients of the multinomial logistic regression model.\nUse the multinomial logistic regression model for prediction."
  },
  {
    "objectID": "labs/lab-6.html#introduction",
    "href": "labs/lab-6.html#introduction",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "",
    "text": "In today’s lab you will analyze data from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”.\n\n\nBy the end of the lab you will be able to…\n\nConduct exploratory data analysis for multinomial logistic regression.\nFit and interpret coefficients of the multinomial logistic regression model.\nUse the multinomial logistic regression model for prediction."
  },
  {
    "objectID": "labs/lab-6.html#getting-started",
    "href": "labs/lab-6.html#getting-started",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-6. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-6.html#packages",
    "href": "labs/lab-6.html#packages",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-6.html#data-five-thirty-eight",
    "href": "labs/lab-6.html#data-five-thirty-eight",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Data: Five Thirty Eight",
    "text": "Data: Five Thirty Eight\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on are (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\nrace: Race of respondent, census categories. Note: all categories except Hispanic are non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\nvoters &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")\n\nNote that the authors use weighting to make the final sample more representative on the US population for their article. We will not use weighting in this assignment, so we should treat the sample as a convenience sample rather than a random sample of the population."
  },
  {
    "objectID": "labs/lab-6.html#exercises",
    "href": "labs/lab-6.html#exercises",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Exercises",
    "text": "Exercises\n\nWhy do you think the authors chose to only include data from people who were eligible to vote for at least four election cycles?\nLet’s prepare the data for analysis and modeling.\n\nThe variable Q30 contains the respondent’s political party identification. Make a new variable that simplifies Q30 into four categories: “Democrat”, “Republican”, “Independent”, “Other” (“Other” also includes respondents who did not answer the question).\nThe variable voter_category identifies the respondent’s past voter behavior. Relevel the variable to make rarely/never the baseline level, followed by sporadic, then always.\n\nIn the FiveThirtyEight article, the authors include visualizations of the relationship between the voter category and demographic variables such as race, age, education, etc. Select two demographic variables. For each variable, interpret the plot to describe its relationship with voter category.\nFit a model using mean-centered age, race, gender, income, and education to predict voter category. Show the code used to fit the model, but do not display the model output.\n\nNext, we want to determine whether party identification be added to the model. In order to do this we need to compare two nested models.\n\nThe reduced model is the one we fit so far, including the predictors mean-centered age, race, gender, income, and education.\nThe full model is the one that includes, in addition to these predictors, party identification.\n\n\nShould party identification be added to the model? Use a drop-in-deviance test to determine if party identification should be added to the model. Include the hypotheses in mathematical notation, the output from the test, and the conclusion in the context of the data. Then, neatly display the model you selected.\n\nUse the model you select for the remainder of the assignment.\n\nInterpret the following coefficients in the context of the data in terms of the odds of voting sporadically versus rarely/never.\n\nInterpret the intercept in the context of the data. Use actual values in the interpretation.\nInterpret the effect of age in the context of the data.\nInterpret the effect of party ID in the context of the data. Include discussion about which level(s) differ from the baseline.\n\nIn the article, the authors write\n\n“Nonvoters were more likely to have lower incomes; to be young; to have lower levels of education; and to say they don’t belong to either political party, which are all traits that square with what we know about people less likely to engage with the political system.”\n\nDoes your model support this statement? Briefly explain why or why not.\nLet’s use the model to predict the voting categories. Obtain the predicted voter category for each observation.\n\nCreate a table of the actual versus predicted voter categories and a visualization of the association between the two.\nHow well did the model perform? Briefly assess the model performance using 2 - 3 observations from the table and/or visualization to support your response."
  },
  {
    "objectID": "labs/lab-6.html#submission",
    "href": "labs/lab-6.html#submission",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-6.html#grading",
    "href": "labs/lab-6.html#grading",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-6.html#footnotes",
    "href": "labs/lab-6.html#footnotes",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nHW 2 - Friday, Feb 18\nProject ideas - Friday, Feb 18"
  },
  {
    "objectID": "weeks/week-7.html#prepare",
    "href": "weeks/week-7.html#prepare",
    "title": "Week 7",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Tidy Modeling in R Chp 8: Feature engineering with recipes"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 12 - MLR: Feature engineering\n🖥️ Lecture 13 - MLR: Feature engineering (cont.)"
  },
  {
    "objectID": "weeks/week-7.html#practice",
    "href": "weeks/week-7.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 5 - The Office"
  },
  {
    "objectID": "weeks/week-7.html#perform",
    "href": "weeks/week-7.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n✍️ HW 2 - Multiple linear regression\n📂 Project - Topic ideas\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\n\nIf you can’t be in class for the lectures, you can watch the live stream or watch the recording later on Panopto.\nDue dates:\n\nHW 1: Fri, Jan 28, 5pm ET\nLab 1: Fri, Jan 28, 5pm ET"
  },
  {
    "objectID": "weeks/week-4.html#prepare",
    "href": "weeks/week-4.html#prepare",
    "title": "Week 4",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 24.4: Mathematical model for testing the slope\n📖 Read Introduction to Modern Statistics, Sec 24.5: Mathematical model, interval for the slope\n📖 Read Introduction to Modern Statistics, Sec 24.6: Checking model conditions\n📖 Read Introduction to Modern Statistics, Sec 24.7: Chapter review"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 2 - College scorecard\n🖥️ Lecture 6 - SLR: Mathematical models for inference\n🖥️ Lecture 7 - SLR: Model diagnostics"
  },
  {
    "objectID": "weeks/week-4.html#practice",
    "href": "weeks/week-4.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 3 - Checking model conditions"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 2 - College scorecard\n✍️ HW 1 - In-person voting trends\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nWe’re back to in person classes this week! See here for class locations. And don’t forget to wear your mask! 😷\nDue dates:\n\nAE 2: Fri, Jan 21, 11:59pm ET"
  },
  {
    "objectID": "weeks/week-3.html#prepare",
    "href": "weeks/week-3.html#prepare",
    "title": "Week 3",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Sec 24.1: Case study: Sandwich store\n📖 Read Introduction to Modern Statistics, Sec 24.2: Randomization test for the slope\n📖 Read Introduction to Modern Statistics, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 4 - SLR: Prediction + model evaluation\n🖥️ Lecture 5 - SLR: Simulation-based inference"
  },
  {
    "objectID": "weeks/week-3.html#practice",
    "href": "weeks/week-3.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 2 - Bike rentals in DC"
  },
  {
    "objectID": "weeks/week-3.html#perform",
    "href": "weeks/week-3.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\nNone.\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\nDue date: Project proposal due Fri, Mar 18 at 5:00 pm."
  },
  {
    "objectID": "weeks/week-10.html#prepare",
    "href": "weeks/week-10.html#prepare",
    "title": "Week 10",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Introduction to Modern Statistics, Chp 9: Logistic regression"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 18 - Logistic regression\n🖥️ Lecture 19 - Probabilities, odds, odds ratios"
  },
  {
    "objectID": "weeks/week-10.html#practice",
    "href": "weeks/week-10.html#practice",
    "title": "Week 10",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 9 - Odds"
  },
  {
    "objectID": "weeks/week-10.html#perform",
    "href": "weeks/week-10.html#perform",
    "title": "Week 10",
    "section": "Perform",
    "text": "Perform\n✍️ HW 3 - Logistic regression and log transformation\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important\n\n\n\nDue dates:\n\nLab 6 - Friday, April 8\nProject drafts - Sunday, April 10"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 24 - MultiLR: Prediction + inferential models\n🖥️ Lecture 25 - MultiLR: Predictive models"
  },
  {
    "objectID": "weeks/week-13.html#practice",
    "href": "weeks/week-13.html#practice",
    "title": "Week 13",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 11 - Volcanoes"
  },
  {
    "objectID": "weeks/week-13.html#perform",
    "href": "weeks/week-13.html#perform",
    "title": "Week 13",
    "section": "Perform",
    "text": "Perform\n✍️ HW 4 - Multinomial logistic regression\n💻 Lab 6 - Why Many Americans Don’t Vote\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\nDue date: Exam 1 released on Fri, Feb 35, due Mon, Feb 28 at 11:59pm"
  },
  {
    "objectID": "weeks/week-9.html#prepare",
    "href": "weeks/week-9.html#prepare",
    "title": "Week 9",
    "section": "Prepare",
    "text": "Prepare\nNo readings this week."
  },
  {
    "objectID": "weeks/week-9.html#participate",
    "href": "weeks/week-9.html#participate",
    "title": "Week 9",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 16 - MLR: Inference\n🖥️ Lecture 17 - MLR: Inference conditions + multicollinearity"
  },
  {
    "objectID": "weeks/week-9.html#practice",
    "href": "weeks/week-9.html#practice",
    "title": "Week 9",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 8 - Rail Trail"
  },
  {
    "objectID": "weeks/week-9.html#perform",
    "href": "weeks/week-9.html#perform",
    "title": "Week 9",
    "section": "Perform",
    "text": "Perform\n✅ Exam 2\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important\n\n\n\nDue dates: Tue, Apr 19 - HW 5"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 15",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 28 - Wrap up"
  },
  {
    "objectID": "weeks/week-15.html#practice",
    "href": "weeks/week-15.html#practice",
    "title": "Week 15",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 13 - A Tale of Two Creeks"
  },
  {
    "objectID": "weeks/week-15.html#perform",
    "href": "weeks/week-15.html#perform",
    "title": "Week 15",
    "section": "Perform",
    "text": "Perform\n✍️ HW 5 - Statistics experience\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5 - Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience\n2️⃣ Make a slide summarizing on your experience\nYou must complete both parts to receive credit."
  },
  {
    "objectID": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "hw/hw-5.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::global 2021 talks\nrstudio::conf 2020 talks\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask your professor if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team. One option is DataFest, which will take place over the April 1-3, 2022 weekend. More information to follow here.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask your professor to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nHow Charts Lie by Alberto Cairo\nList of books about data science ethics\n\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: Coding out loud\nWatch an episode of Coding out loud (either live or pre-recorded) and work through the project.\nA few guidelines:\n✅ Create a GitHub repo for your Coding out loud submission. Your repo should include\n\nThe Quarto file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The final product (visualuzation, table, etc.) should include features or customization that are beyond what was achieved in the Coding out loud episode.\n✅ Include the link to your GitHub repo in the slide summarizing your experience."
  },
  {
    "objectID": "hw/hw-5.html#part-2-summarize-your-experience",
    "href": "hw/hw-5.html#part-2-summarize-your-experience",
    "title": "HW 5 - Statistics Experience",
    "section": "Part 2: Summarize your experience",
    "text": "Part 2: Summarize your experience\nMake one slide summarizing your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "hw/hw-5.html#submission",
    "href": "hw/hw-5.html#submission",
    "title": "HW 5 - Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the HW 5 - Statistics Experience assignment on Gradescope by Fri, Apr 15 at 5 pm ET. It must be submitted by the deadline on Gradescope to be considered for grading."
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#introduction",
    "href": "hw/hw-3.html#introduction",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "",
    "text": "In this assignment, you’ll get to put into practice the logistic regression skills you’ve developed.\n\n\nIn this assignment, you will…\n\nFit and interpret logistic regression models.\nFit and interpret multiple linear regression models with log transformed outcomes.\nReason around log transformations of various types.\nContinue developing a workflow for reproducible data analysis.\n\n\n\n\nYour repo for this assignment is at github.com/sta210-s22 and starts with the prefix hw-3. For more detailed instructions on getting started, see HW 1.\n\n\n\nThe following packages will be used in this assignment. You can add other packages as needed.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "hw/hw-3.html#part-1---palmer-penguins",
    "href": "hw/hw-3.html#part-1---palmer-penguins",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 1 - Palmer penguins",
    "text": "Part 1 - Palmer penguins\nIn this part we’ll go back to the Palmer penguins dataset from HW 2.\nWe will use the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nflipper_length_mm\ninteger\nFlipper length in mm\n\n\n\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is from the Adelie species. First, we need to create a new response variable to identify whether a penguin is from the Adelie species.\n\npenguins &lt;- penguins %&gt;%\n  mutate(adelie = factor(if_else(species == \"Adelie\", 1, 0)))\n\nAnd let’s check to make sure the new variable looks right before we continue with the analysis.\n\npenguins %&gt;%\n  count(adelie, species)\n\n# A tibble: 3 × 3\n  adelie species       n\n  &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n1 0      Chinstrap    68\n2 0      Gentoo      124\n3 1      Adelie      152\n\n\nLet’s start by looking at the relationship between island and whether a penguin is from the Adelie species.\n\nWhat does the values_fill argument do in the following chunk? The documentation for the function will be helpful in answering this question.\n\npenguins %&gt;%\n  count(island, adelie) %&gt;%\n  pivot_wider(names_from = adelie, values_from = n, values_fill = 0)\n\n# A tibble: 3 × 3\n  island      `0`   `1`\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 Biscoe      124    44\n2 Dream        68    56\n3 Torgersen     0    52\n\n\nCalculate the odds ratio of a penguin being from the Adelie species for those recorded on Dream compared to those recorded on Biscoe.\nYou want to fit a model using island to predict the odds of being from the Adelie species. Let \\(\\pi\\) be the probability a penguin is from the Adelie species. The model has the following form. What do you expect the value of \\(\\hat{\\beta}_1\\), the estimated coefficient for Dream, to be? Explain your reasoning.\n\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1 ~ Dream + \\beta_2 ~ Torgersen\n\\]\n\nFit a model predicting adelie from island and display the model output. For the following exercise, use this model.\nBased on this model, what are the odds of a penguin being from the Adelie species if it was recorded on Biscoe island? on Dream island?\nNext, add flipper length to the model so that there are two predictors. Display the model output. For the following exercises, use this model.\nWrite the regression equation for the model.\nInterpret the coefficient of flipper_length_mm in terms of the log-odds of being from the Adelie species.\nInterpret the coefficient of flipper_length_mm in terms of the odds of being from the Adelie species.\nInterpret the coefficient of Dream in terms of the odds of being from the Adelie species.\nHow do you expect the log-odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island.\nHow do you expect the odds of being from the Adelie species to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins were recorded on the Dream island."
  },
  {
    "objectID": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "href": "hw/hw-3.html#part-2---gdp-and-urban-population",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Part 2 - GDP and Urban population",
    "text": "Part 2 - GDP and Urban population\nData on countries’ Gross Domestic Product (GDP) and percentage of urban population was collected and made available by The World Bank in 2020. A description of the variables as defined by The World Bank are provided below.\n\nGDP: “GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.”\nUrban Population (% of total): “Urban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects.”\n\nThe data can be found in the data folder of your repository. Read the data and name it gdp_2020.\n\nFit a model predicting GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the linear model seem appropriate for modeling this relationship? Explain your reasoning.\nAdd a new column to the gdp_2020 dataset called gdp_log which is the (natural) log of gdp.\nFit a new model, predicting the log of GDP from urban population. Then make a plot of residuals vs. fitted for this model. Does the model predicting logged GDP or original GDP appear to be a better fit? Explain your reasoning.\n\nThe model output for predicting logged GDP.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.107\n0.202\n30.291\n0\n\n\nurban\n0.042\n0.003\n13.769\n0\n\n\n\n\n\nThe linear model for predicting log of GDP can be expressed as follows:\n\\[\n\\widehat{\\log(GDP)} = 6.11 + 0.042 \\times urban\n\\]\nTherefore, the coefficient of urban (0.042) can be interpreted as the change in logged GDP associated with 1 percentage point increase in urban population. The problem is, logged GDP is not a very informative value to talk about. So we need to undo the transformation we’ve done.\nTo do so, let’s do a quick review of some properties of logs.\n\nSubtraction and logs: \\(log(a) − log(b) = log(\\frac{a}{b})\\)\nNatural logarithm: \\(e^{log(x)} = x\\)\n\nBased on the interpretation of the slope above, the difference between the predicted values of logged GDP for a given value of urban and a value that is 1 percentage point higher is 0.0425. Let’s write this out mathematically, and then use the properties we’ve listed above to work through the equation.\n\\[\n\\begin{aligned}\nlog(\\text{GDP for urban } x + 1) - log(\\text{GDP for urban } x) &= 0.042 \\\\\nlog\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big) &= 0.042 \\\\\ne^{log\\Big( \\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} \\Big)} &= e^{0.042}\\\\\n\\frac{\\text{GDP for urban } x + 1}{\\text{GDP for urban } x} &= e^{0.042}\n\\end{aligned}\n\\]\n\nBased on the derivation above, fill in the blanks in the following sentence for an alternative (and more useful interpretation) of the slope of urban.\n\nFor each additional percentage point the urban population is higher, the GDP of a country is expected to be ___, on average, by a factor of ___."
  },
  {
    "objectID": "hw/hw-3.html#submission",
    "href": "hw/hw-3.html#submission",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-3.html#grading",
    "href": "hw/hw-3.html#grading",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 9\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-3.html#footnotes",
    "href": "hw/hw-3.html#footnotes",
    "title": "HW 3 - Logistic regression and log transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#introduction",
    "href": "hw/hw-1.html#introduction",
    "title": "HW 1 - In-person voting trends",
    "section": "",
    "text": "In this assignment, you’ll use simple linear regression to explore the percent of votes cast in-person in the 2020 U.S. election based on the county’s political leanings.\n\n\nIn this assignment, you will…\n\nFit and interpret simple linear regression models\nAssess the conditions for simple linear regression.\nCreate and interpret spatial data visualizations using R.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-1.html#getting-started",
    "href": "hw/hw-1.html#getting-started",
    "title": "HW 1 - In-person voting trends",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix hw-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick hw-1-voting.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/hw-1.html#packages",
    "href": "hw/hw-1.html#packages",
    "title": "HW 1 - In-person voting trends",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used in this assignment:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw-1.html#data-2020-election",
    "href": "hw/hw-1.html#data-2020-election",
    "title": "HW 1 - In-person voting trends",
    "section": "Data: 2020 Election",
    "text": "Data: 2020 Election\nThere are multiple data sets for this assignment. Use the code below to load the data.\n\nelection_nc &lt;- read_csv(\"data/nc-election-2020.csv\") %&gt;%\n  mutate(fips = as.integer(FIPS))\ncounty_map_data &lt;-  read_csv(\"data/nc-county-map-data.csv\")\nelection_sample &lt;- read_csv(\"data/us-election-2020-sample.csv\")\n\nThe county-level election data in election_nc and election_sample are from The Economist GitHub repo. The data were originally analyzed in the July 2021 article In-person voting really did accelerate covid-19’s spread in America. For this analysis, we will focus on the following variables:\n\ninperson_pct: The proportion of a county’s votes cast in-person in the 2020 election\npctTrump_2016: The proportion of a county’s votes cast for Donald Trump in the 2016 election\n\nThe data in county_map_data were obtained from the maps package in R. We will not analyze any of the variables in this data set but will use it to help create maps in the assignment. Click here to see the documentation for the maps package. Click here for code examples."
  },
  {
    "objectID": "hw/hw-1.html#exercises",
    "href": "hw/hw-1.html#exercises",
    "title": "HW 1 - In-person voting trends",
    "section": "Exercises",
    "text": "Exercises\nDue to COVID-19 pandemic, many states made alternatives in-person voting, such as voting by mail, more widely available for the 2020 U.S. election. The general consensus was that voters who were more Democratic leaning would be more likely to vote by mail, while more Republican leaning voters would largely vote in-person. This was supported by multiple surveys, including this survey conducted by Pew Research.\nThe goal of this analysis is to use regression analysis to explore the relationship between a county’s political leanings and the proportion of votes cast in-person in 2020. The ultimate question we want to answer is “Did counties with more Republican leanings have a larger proportion of votes cast in-person in the 2020 election?”\nWe will use the proportion of votes cast for Donald Trump in 2016 (pctTrump_2016) as a measure of a county’s political leaning. Counties with a higher proportion of votes for Trump in 2016 are considered to have more Republican leanings.\n\n\n\n\n\n\nNote\n\n\n\nAll narrative should be written in complete sentences, and all visualizations should have informative titles and axis labels.\n\n\n\nPart 1: Counties in North Carolina\nFor this part of the analysis, we will focus on counties in North Carolina. We will use the data sets election_nc and county_map_data.\n\nVisualize the distribution of the response variable inperson_pct and calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\nLet’s view the data in another way. Use the code below to make a map of North Carolina with the color of each county filled in based on the percentage of votes cast in-person in the 2020 election. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 - 3 observations you have from the plot?\nWhat is a feature that is apparent in the map that wasn’t apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not apparent in the map?\n\n\n\nelection_map_data &lt;- left_join(election_nc, county_map_data)\n\nggplot() +\n  geom_polygon(data = county_map_data,\n    mapping = aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = election_map_data, \n    mapping = aes(x = long, y = lat, group = group,\n    fill = inperson_pct)\n    ) +\n  labs(\n    x = \"___\",\n    y = \"___\",\n    fill = \"___\",\n    title = \"___\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\nCreate a visualization of the relationship between inperson_pct and pctTrump_2016. Use the visualization to describe the relationship between the two variables.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you haven’t yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe can use a linear regression model to better quantify the relationship between the variables.\n\nFit the linear model to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nWrite the regression equation using mathematical notation.\n\nNow let’s use the model coefficients to describe the relationship.\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nDoes it make sense to interpret the intercept? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\nIf the linear model is a good fit to these data, there should be no structure left in the residuals and the residuals should have constant variance. Augment the data with the model to obtain the residuals and predicted values for each observation, and call the augmented data frame nc_election_aug (You will use this name in Exercise 8). Then, make a plot of the residuals vs. the fitted values, and based on this plot, and provide a brief explanation for whether these two conditions are met. Hint: Zoom out on the plot by extending the limits of the y-axis.\n\n\n\n\n\n\n\nWarning\n\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates.\n\n\n\nWe might also be interested in our observations being independent, particularly if we are to use these data for inference. To evaluate whether the independence condition is met, we will examine a map of the counties in North Carolina with the color filled based on the value of the residuals.\n\nBriefly explain why we may want to view the residuals on a map to assess independence.\nBriefly explain what pattern (if any) we would expect to observe on the map if the independence condition is satisfied.\n\nFill in the name of your model in the code below to calculate the residuals and add them to election_map_data. Then, a map with the color of each county filled in based on the value of the residual. Hint: Start with the code from Exercise 2.\nIs the independence condition satisfied? Briefly explain based on what you observe from the plot.\n\nnc_election_aug &lt;- nc_election_aug %&gt;% \n  bind_cols(fips = election_nc$fips)\n\nelection_map_data &lt;- left_join(election_map_data, nc_election_aug)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore moving on to the next part, make sure you render your document and commit (with a meaningful commit message) and push all updates.\n\n\n\n\nPart 2: Inference for the U.S.\nTo get a better understanding of the trend across the entire United States, we analyze data from a random sample of 200 counties. This data is in the election_sample data frame. Because these counties were randomly selected out of the 3,006 counties in the United States, we can reasonably treat the counties as independent observations.\n\nFit the linear model to these sample data to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Neatly display the model output with 3 digits.\nConduct a hypothesis test for the slope using a permutation test. In your response, state the null and alternative hypotheses in words, and state the conclusion in the context of the data.\nNext, construct a 95% confidence interval for the slope using bootstrapping. Interpret the confidence interval in the context of the data.\nComment on whether the hypothesis test and confidence interval support the general consensus that Republican voters were more likely to vote in-person in the 2020 election? A brief explanation is sufficient but it should be based on your conclusions from Exercises 10 and 11.\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-1.html#submission",
    "href": "hw/hw-1.html#submission",
    "title": "HW 1 - In-person voting trends",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-1.html#grading",
    "href": "hw/hw-1.html#grading",
    "title": "HW 1 - In-person voting trends",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-1.html#footnotes",
    "href": "hw/hw-1.html#footnotes",
    "title": "HW 1 - In-person voting trends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML.↩︎"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html",
    "href": "ae/ae-11-volcanoes.html",
    "title": "AE 11: Multinomial classification",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-11-volcanoes-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#packages",
    "href": "ae/ae-11-volcanoes.html#packages",
    "title": "AE 11: Multinomial classification",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#data",
    "href": "ae/ae-11-volcanoes.html#data",
    "title": "AE 11: Multinomial classification",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of on volcanoes. The data come from The Smithsonian Institution via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"ae\", \"data/volcano.csv\"))\n\nRows: 958 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): volcano_name, primary_volcano_type, last_eruption_year, country, r...\ndbl  (8): volcano_number, latitude, longitude, elevation, population_within_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, a bit of data prep:\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "href": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "title": "AE 11: Multinomial classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nCreate a map of volcanoes that is faceted by volcano_type.\n\n\nworld &lt;- map_data(\"world\")\n\nworld_map &lt;- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(x = longitude, y = latitude,\n        color = volcano_type, \n        shape = volcano_type),\n    alpha = 0.5\n  ) +\n  facet_wrap(~volcano_type) +\n  scale_color_OkabeIto()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#build-a-new-model",
    "href": "ae/ae-11-volcanoes.html#build-a-new-model",
    "title": "AE 11: Multinomial classification",
    "section": "Build a new model",
    "text": "Build a new model\n\nBuild a new model that uses a recipe that includes geographic information (latitude and longitude). How does this model compare to the original? Note:\nUse the same test/train split as well as same cross validation folds. Code for these is provided below.\n\n\n# test/train split\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)\n\n# cv folds\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5\n\n\nNew recipe, including geographic information:\n\nvolcano_rec2 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nOriginal model specification and new workflow:\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec2) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_center()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\nFit resamples:\n\nvolcano_fit_rs2 &lt;- volcano_wflow2 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs2\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1 &lt;split [574/144]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n2 &lt;split [574/144]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n3 &lt;split [574/144]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n4 &lt;split [575/143]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n5 &lt;split [575/143]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n\n\nCollect metrics:\n\ncollect_metrics(volcano_fit_rs2)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.606     5  0.0138 Preprocessor1_Model1\n2 roc_auc  hand_till  0.695     5  0.0245 Preprocessor1_Model1\n\n\nROC curves:\n\nvolcano_fit_rs2 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#roc-curves",
    "href": "ae/ae-11-volcanoes.html#roc-curves",
    "title": "AE 11: Multinomial classification",
    "section": "ROC curves",
    "text": "ROC curves\n\nRecreate the ROC curve from the slides.\n\n\nfinal_fit &lt;- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_predictions(final_fit) %&gt;%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n  geom_path(size = 1) +\n  scale_color_OkabeIto() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  theme_minimal() +\n  labs(color = NULL)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#acknowledgement",
    "href": "ae/ae-11-volcanoes.html#acknowledgement",
    "title": "AE 11: Multinomial classification",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://juliasilge.com/blog/multinomial-volcano-eruptions."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html",
    "href": "ae/ae-6-the-office-cv.html",
    "title": "AE 6: The Office",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-6-the-office-cv-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#packages",
    "href": "ae/ae-6-the-office-cv.html#packages",
    "title": "AE 6: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#load-data",
    "href": "ae/ae-6-the-office-cv.html#load-data",
    "title": "AE 6: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_episodes &lt;- read_csv(\"data/office_episodes.csv\")"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "href": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "title": "AE 6: The Office",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#specify-model",
    "href": "ae/ae-6-the-office-cv.html#specify-model",
    "title": "AE 6: The Office",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-recipe",
    "href": "ae/ae-6-the-office-cv.html#create-recipe",
    "title": "AE 6: The Office",
    "section": "Create recipe",
    "text": "Create recipe\nCreate the recipe from class. Call it office_rec1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-workflow",
    "href": "ae/ae-6-the-office-cv.html#create-workflow",
    "title": "AE 6: The Office",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it office_wflow1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#cross-validation",
    "href": "ae/ae-6-the-office-cv.html#cross-validation",
    "title": "AE 6: The Office",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "href": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "title": "AE 6: The Office",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "href": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "title": "AE 6: The Office",
    "section": "Another model - Model 2",
    "text": "Another model - Model 2\nCreate a different (simpler, involving fewer variables) recipe and call it office_rec2. Conduct 10-fold cross validation and summarize metrics. Describe how the two models compare to each other based on cross validation metrics."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html",
    "href": "ae/ae-1-dcbikeshare.html",
    "title": "AE 02: Bike rentals in DC",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-1-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 02: Bike rentals in DC",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#data",
    "href": "ae/ae-1-dcbikeshare.html#data",
    "title": "AE 02: Bike rentals in DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250)\n\n\n\n\n\n\n\nggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExercise 2\nDescribe the distribution of daily bike rentals and the distribution of temperature based on the visualizations created in Exercise 1. Include the shape, center, spread, and presence of any potential outliers.\n[Add your answer here]\n\n\nExercise 3\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n[Add your answer here]\n\n\nExercise 4\nDescribe the relationship between daily bike rentals and temperature based on the visualization created in Exercise 1. Comment on how we expect the number of bike rentals to change as the temperature increases.\n[Add your answer here]\n\n\nExercise 5\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\n\\[\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp\\_orig} + \\epsilon\\]\nbe the best fit for the data? Why or why not?\nNo."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 6\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 7\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 8\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 9\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#modeling",
    "href": "ae/ae-1-dcbikeshare.html#modeling",
    "title": "AE 02: Bike rentals in DC",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 10\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 11\nUsing the data you filtered in Exercise 10, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 13\nInterpret the slope in the context of the data.\n[Add your answer here]\n\n\nExercise 14\nInterpret the intercept in the context of the data.\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#synthesis",
    "href": "ae/ae-1-dcbikeshare.html#synthesis",
    "title": "AE 02: Bike rentals in DC",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 15\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]\n\nThe following exercises will be completed only if time permits.\n\n\nExercise 16\nPick another season. Based on the visualization in Exercise 8, would you expect the slope of the relationship between temperature and daily bike rentals to be smaller or larger than the slope of the model you’ve been working with so far? Explain your reasoning.\n[Add your answer here]\n\n\nExercise 17\nFor this season you picked in Exercise 16, fit a linear model for predicting daily bike rentals from temperature. Note, you will need to filter your data for this season first. Use the output to write out the estimated regression equation and interpret the slope and the intercept of this model.\n\n# add your code here\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-9-odds.html",
    "href": "ae/ae-9-odds.html",
    "title": "AE 9: Odds",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-9-odds-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-9-odds.html#packages",
    "href": "ae/ae-9-odds.html#packages",
    "title": "AE 9: Odds",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(here::here(\"ae\", \"data/framingham.csv\")) %&gt;%\n  select(totChol, TenYearCHD) %&gt;%\n  drop_na() %&gt;%\n  mutate(high_risk = as.factor(TenYearCHD)) %&gt;%\n  select(totChol, high_risk)"
  },
  {
    "objectID": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "href": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "title": "AE 9: Odds",
    "section": "Linear regression vs. logistic regression",
    "text": "Linear regression vs. logistic regression\nState whether a linear regression model or logistic regression model is more appropriate for each scenario:\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie’s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year."
  },
  {
    "objectID": "ae/ae-9-odds.html#heart-disease",
    "href": "ae/ae-9-odds.html#heart-disease",
    "title": "AE 9: Odds",
    "section": "Heart disease",
    "text": "Heart disease\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\n\n\n\n\nheart_disease %&gt;%\n  count(high_risk)\n\n# A tibble: 2 × 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635\n\n\n\n\nCalculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease?\n\n\n\nLogistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet \\(pi\\) be the probability an adult is high risk. The statistical model is\n\\[\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\\]\n\nheart_disease_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits.\n\n\n\nCalculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease?\nWhat are the odds they are high risk for heart disease?\nWhat is the probability they are high risk for heart disease? Use the odds to calculate your answer.\n\n\n\nComparing observations\nSuppose a person’s cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease to change?\nHow do you expect the odds that this person is high risk for heart disease to change?"
  },
  {
    "objectID": "ae/ae-0-movies.html",
    "href": "ae/ae-0-movies.html",
    "title": "Movie budgets and revenues",
    "section": "",
    "text": "Important\n\n\n\nThis application exercise is a demo only. You do not have a corresponding repository for it and you’re not expected to turn in anything for it.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\nlibrary(tidyverse) # for data analysis and visualisation\nlibrary(scales)    # for pretty axis labels\nlibrary(DT)        # for interactive table"
  },
  {
    "objectID": "ae/ae-0-movies.html#data",
    "href": "ae/ae-0-movies.html#data",
    "title": "Movie budgets and revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\")\n\nView the first 10 rows of data.\n\nmovies\n\n# A tibble: 7,668 × 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S… R      Drama  1980 June 13…   8.4 9.27e5 Stanley… Steph… Jack… United…\n 2 The B… R      Adve…  1980 July 2,…   5.8 6.5 e4 Randal … Henry… Broo… United…\n 3 Star … PG     Acti…  1980 June 20…   8.7 1.2 e6 Irvin K… Leigh… Mark… United…\n 4 Airpl… PG     Come…  1980 July 2,…   7.7 2.21e5 Jim Abr… Jim A… Robe… United…\n 5 Caddy… R      Come…  1980 July 25…   7.3 1.08e5 Harold … Brian… Chev… United…\n 6 Frida… R      Horr…  1980 May 9, …   6.4 1.23e5 Sean S.… Victo… Bets… United…\n 7 The B… R      Acti…  1980 June 20…   7.9 1.88e5 John La… Dan A… John… United…\n 8 Ragin… R      Biog…  1980 Decembe…   8.2 3.3 e5 Martin … Jake … Robe… United…\n 9 Super… PG     Acti…  1980 June 19…   6.8 1.01e5 Richard… Jerry… Gene… United…\n10 The L… R      Biog…  1980 May 16,…   7   1   e4 Walter … Bill … Davi… United…\n# … with 7,658 more rows, and 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;,\n#   company &lt;chr&gt;, runtime &lt;dbl&gt;\n\n\nThe ___ dataset has ___ observations and ___ variables."
  },
  {
    "objectID": "ae/ae-0-movies.html#analysis",
    "href": "ae/ae-0-movies.html#analysis",
    "title": "Movie budgets and revenues",
    "section": "Analysis",
    "text": "Analysis\n\nGross over time\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\nThen, we will filter for these genres and visualize the average gross revenue over time.\n\nmovies %&gt;%\n  filter(genre %in% genre_list) %&gt;% \n  group_by(genre,year) %&gt;%\n  summarise(avg_gross = mean(gross)) %&gt;%\n  ggplot(mapping = aes(x = year, y = avg_gross, color= genre)) +\n    geom_point() + \n    geom_line() +\n    scale_color_viridis_d() +\n    scale_y_continuous(labels = label_dollar()) +\n    labs(\n      x = \"Year\",\n      y = \"Average Gross Revenue (US Dollars)\",\n      color = \"Genre\",\n      title = \"Gross Revenue Over Time\"\n    )\n\n`summarise()` has grouped output by 'genre'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 47 rows containing missing values (geom_point).\n\n\nWarning: Removed 23 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n\n\nThe plot suggests …\n\n\nBudget and gross\nNext, let’s see the relationship between a movie’s budget and its gross revenue.\n\nmovies %&gt;%\n  filter(genre %in% genre_list, budget &gt; 0) %&gt;% \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d() +\n  labs(\n    x = \"Log-transformed Budget\",\n    y = \"Log-transformed Gross Revenue\"\n  )\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 35 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 35 rows containing missing values (geom_point)."
  },
  {
    "objectID": "ae/ae-0-movies.html#exercises",
    "href": "ae/ae-0-movies.html#exercises",
    "title": "Movie budgets and revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nIn the remaining time, discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-0-movies.html#appendix",
    "href": "ae/ae-0-movies.html#appendix",
    "title": "Movie budgets and revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies %&gt;% \n  distinct(genre) %&gt;%\n  arrange(genre) %&gt;% \n  datatable()"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html",
    "href": "ae/ae-2-dcbikeshare.html",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-2-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#data",
    "href": "ae/ae-2-dcbikeshare.html#data",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")\n\nSee AE 1 for the first part of this analysis."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 1\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 2\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 3\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#modeling",
    "href": "ae/ae-2-dcbikeshare.html#modeling",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 5\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 6\nUsing the data you filtered in Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#synthesis",
    "href": "ae/ae-2-dcbikeshare.html#synthesis",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 7\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources for the introductory data science courses offered by the Duke University Department of Statistical Science, go to the Duke Container Manager website, cmgr.oit.duke.edu/containers.\nIf this is your first time accessing the containers, click on reserve STA210 on the Reservations available menu on the right. You only need to do this once, and when you do, you’ll see this container moved to the My reservations menu on the left.\nNext, click on STA210 under My reservations to access the RStudio instance you’ll use for the course."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2024 - CSC 2626: Imitation Learning for Robotics",
    "section": "",
    "text": "Course Overview\nIn the next few decades we are going to witness millions of people, from various backgrounds and levels of technical expertise, needing to effectively interact with robotic technologies on a daily basis. As such, people will need to modify the behavior of their robots without explicitly writing code, but by providing only a small number of kinesthetic or visual demonstrations, or even natural language commands. At the same time, robots should try to infer and predict the human’s intentions and internal objectives from past interactions, in order to provide assistance before it is explicitly asked. This graduate-level course will examine some of the most important papers in imitation learning for robot control, placing more emphasis on developments in the last 10 years. Its purpose is to familiarize students with the frontiers of this research area, to help them identify open problems, and to enable them to make a research contribution.\nThis course will broadly cover the following areas:\n\nImitating the policies of demonstrators (people, expensive algorithms, optimal controllers)\nConnections between imitation learning, optimal control, and reinforcement learning\nLearning the cost functions that best explain a set of demonstrations\nShared autonomy between humans and robots for real-time control\n\n\n\nPrerequisites\nYou need to be comfortable with: introductory machine learning concepts (such as from CSC411/CSC413/ECE521 or equivalent), linear algebra, basic multivariable calculus, intro to probability. You also need to have strong programming skills in Python. Note: if you don’t meet all the prerequisites above please contact the instructor by email. Optional, but recommended: experience with neural networks, such as from CSC321, introductory-level familiarity with reinforcement learning and control.\n\n\nCourse Delivery Details\n\nLectures: In-person, Mondays @ 1pm-4pm ET, Carr Hall 404\nAnnouncements will be posted on Quercus\nDiscussions will take place on Piazza\nZoom recordings will be posted on Quercus after lectures\nAnonymous feedback form for suggested improvements",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "slides/lec-15.html#announcements",
    "href": "slides/lec-15.html#announcements",
    "title": "Exam 2 review",
    "section": "Announcements",
    "text": "Announcements\n\nExam 2 is released tomorrow (Friday) at 9am, due at 11:59pm on Monday\nKeys for HW and labs posted"
  },
  {
    "objectID": "slides/lec-15.html#application-exercise",
    "href": "slides/lec-15.html#application-exercise",
    "title": "Exam 2 review",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-7-exam-2-review"
  },
  {
    "objectID": "slides/lec-28.html#remaining-deadlines-for-project",
    "href": "slides/lec-28.html#remaining-deadlines-for-project",
    "title": "Wrap up",
    "section": "Remaining deadlines for project",
    "text": "Remaining deadlines for project\n\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30\nPeer evaluations due Mon, Apr 25 + Sat, Apr 30\n\n\nAny questions related to projects?"
  },
  {
    "objectID": "slides/lec-28.html#grading",
    "href": "slides/lec-28.html#grading",
    "title": "Wrap up",
    "section": "Grading",
    "text": "Grading\n\nCheck your grades on Sakai, make sure they match Gradescope, email/Slack me if not by April 30\nWatch for new feedback released on Gradescope"
  },
  {
    "objectID": "slides/lec-28.html#evaluations",
    "href": "slides/lec-28.html#evaluations",
    "title": "Wrap up",
    "section": "Evaluations",
    "text": "Evaluations\n\nCourse evaluation\nTA evaluation"
  },
  {
    "objectID": "slides/lec-28.html#application-exercise",
    "href": "slides/lec-28.html#application-exercise",
    "title": "Wrap up",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-13-tale-of-two-creeks"
  },
  {
    "objectID": "slides/lec-17.html#topics",
    "href": "slides/lec-17.html#topics",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Topics",
    "text": "Topics\n\nConditions for inference\nMulticollinearity"
  },
  {
    "objectID": "slides/lec-17.html#computational-setup",
    "href": "slides/lec-17.html#computational-setup",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)      # for tables\nlibrary(patchwork)  # for laying out plots\nlibrary(rms)        # for vif\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-17.html#data-rail_trail",
    "href": "slides/lec-17.html#data-rail_trail",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\nrail_trail &lt;- read_csv(here::here(\"slides\", \"data/rail_trail.csv\"))\nrail_trail\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# … with 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/lec-17.html#variables",
    "href": "slides/lec-17.html#variables",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\nPredictors\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of “Fall”, “Spring”, or “Summer”\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of “weekday” or “weekend”"
  },
  {
    "objectID": "slides/lec-17.html#full-model",
    "href": "slides/lec-17.html#full-model",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit$fit)"
  },
  {
    "objectID": "slides/lec-17.html#model-conditions",
    "href": "slides/lec-17.html#model-conditions",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/lec-17.html#residuals-vs.-predicted-values",
    "href": "slides/lec-17.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values\n\nggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/lec-17.html#linearity-residuals-vs.-predicted",
    "href": "slides/lec-17.html#linearity-residuals-vs.-predicted",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Linearity: Residuals vs. predicted",
    "text": "Linearity: Residuals vs. predicted\n\nDoes the linearity condition appear to be met?"
  },
  {
    "objectID": "slides/lec-17.html#linearity-residuals-vs.-predicted-1",
    "href": "slides/lec-17.html#linearity-residuals-vs.-predicted-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Linearity: Residuals vs. predicted",
    "text": "Linearity: Residuals vs. predicted\nIf there is some pattern in the plot of residuals vs. predicted values, you can look at individual plots of residuals vs. each predictor to try to identify the issue."
  },
  {
    "objectID": "slides/lec-17.html#linearity-residuals-vs.-each-predictor",
    "href": "slides/lec-17.html#linearity-residuals-vs.-each-predictor",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Linearity: Residuals vs. each predictor",
    "text": "Linearity: Residuals vs. each predictor"
  },
  {
    "objectID": "slides/lec-17.html#checking-linearity",
    "href": "slides/lec-17.html#checking-linearity",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of residuals vs. predicted shows a fan shaped pattern\nThe plots of residuals vs. high and low temperature also shows a similar pattern and vs. precipitation does not show a random scatter\nThe linearity condition is not satisfied."
  },
  {
    "objectID": "slides/lec-17.html#checking-constant-variance",
    "href": "slides/lec-17.html#checking-constant-variance",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/lec-17.html#checking-constant-variance-1",
    "href": "slides/lec-17.html#checking-constant-variance-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied."
  },
  {
    "objectID": "slides/lec-17.html#checking-normality",
    "href": "slides/lec-17.html#checking-normality",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking normality",
    "text": "Checking normality"
  },
  {
    "objectID": "slides/lec-17.html#overlaying-a-density-plot-on-a-histogram",
    "href": "slides/lec-17.html#overlaying-a-density-plot-on-a-histogram",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Overlaying a density plot on a histogram",
    "text": "Overlaying a density plot on a histogram\n\n📋 github.com/sta210-s22/ae-8-rail-trail\n\n\n\n\nEx 3. Recreate the following visualization in R based on the results of the model."
  },
  {
    "objectID": "slides/lec-17.html#checking-independence",
    "href": "slides/lec-17.html#checking-independence",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-1",
    "href": "slides/lec-17.html#checking-independence-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. order of data collection:\n\nggplot(rt_full_aug, aes(y = .resid, x = 1:nrow(rt_full_aug))) +\n  geom_point() +\n  labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-2",
    "href": "slides/lec-17.html#checking-independence-2",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. predicted values by season:"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-3",
    "href": "slides/lec-17.html#checking-independence-3",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. predicted values by day_type:"
  },
  {
    "objectID": "slides/lec-17.html#checking-independence-4",
    "href": "slides/lec-17.html#checking-independence-4",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Checking independence",
    "text": "Checking independence\nNo clear pattern in the residuals vs. order of data collection plot and the model predicts similarly for seasons and day types. Independence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/lec-17.html#why-multicollinearity-is-a-problem",
    "href": "slides/lec-17.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWe can’t include two variables that have a perfect linear association with each other\nIf we did so, we could not find unique estimates for the model coefficients"
  },
  {
    "objectID": "slides/lec-17.html#example",
    "href": "slides/lec-17.html#example",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Example",
    "text": "Example\nSuppose the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec-17.html#example-1",
    "href": "slides/lec-17.html#example-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/lec-17.html#why-multicollinearity-is-a-problem-1",
    "href": "slides/lec-17.html#why-multicollinearity-is-a-problem-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference or prediction"
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity",
    "href": "slides/lec-17.html#detecting-multicollinearity",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when… - There are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nInclude a quadratic in the model mean-centering the variable first\nIncluding interactions between two or more continuous variables"
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/lec-17.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship"
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity-vif",
    "href": "slides/lec-17.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/lec-17.html#detecting-multicollinearity-vif-1",
    "href": "slides/lec-17.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity - Variables with similar values of VIF are typically the ones correlated with each other\n\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/lec-17.html#vif-for-sat-model",
    "href": "slides/lec-17.html#vif-for-sat-model",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "VIF For SAT Model",
    "text": "VIF For SAT Model\n\nvif(rt_full_fit$fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/lec-17.html#model-without-hightemp",
    "href": "slides/lec-17.html#model-without-hightemp",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ . - hightemp, data = rail_trail)\n  \nm1 %&gt;%\n  tidy() %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038\n\n\n\n\nglance(m1) %&gt;%\n  select(adj.r.squared, AIC, BIC)\n\n# A tibble: 1 × 3\n  adj.r.squared   AIC   BIC\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         0.421 1088. 1108."
  },
  {
    "objectID": "slides/lec-17.html#model-without-avgtemp",
    "href": "slides/lec-17.html#model-without-avgtemp",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 %&gt;%\n  tidy() %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100\n\n\n\n\nglance(m2) %&gt;%\n  select(adj.r.squared, AIC, BIC)\n\n# A tibble: 1 × 3\n  adj.r.squared   AIC   BIC\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         0.473 1079. 1099."
  },
  {
    "objectID": "slides/lec-17.html#choosing-a-model",
    "href": "slides/lec-17.html#choosing-a-model",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel with hightemp removed:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel with avgtemp removed:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model with avgtemp removed is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/lec-17.html#recap",
    "href": "slides/lec-17.html#recap",
    "title": "MLR: Inference conditions + multicollinearity",
    "section": "Recap",
    "text": "Recap\n\nConditions for inference\nMulticollinearity"
  },
  {
    "objectID": "slides/lec-13.html#announcements",
    "href": "slides/lec-13.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nMy Monday office hours moved to 8-9pm"
  },
  {
    "objectID": "slides/lec-13.html#topics",
    "href": "slides/lec-13.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nFeature engineering with recipes\nWorkflows to bring together models and recipes\nRMSE and \\(R^2\\) for model evaluation\nCross validation"
  },
  {
    "objectID": "slides/lec-13.html#computational-setup",
    "href": "slides/lec-13.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-13.html#the-office",
    "href": "slides/lec-13.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/lec-13.html#data-goal",
    "href": "slides/lec-13.html#data-goal",
    "title": "Feature engineering",
    "section": "Data & goal",
    "text": "Data & goal\n\nData: The data come from data.world, by way of TidyTuesday\nGoal: Predict imdb_rating from other variables in the dataset\n\n\noffice_ratings &lt;- read_csv(here::here(\"slides\", \"data/office_ratings.csv\"))\noffice_ratings\n\n# A tibble: 188 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# … with 178 more rows"
  },
  {
    "objectID": "slides/lec-13.html#train-test",
    "href": "slides/lec-13.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/lec-13.html#training-data",
    "href": "slides/lec-13.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 × 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# … with 131 more rows"
  },
  {
    "objectID": "slides/lec-13.html#recap-feature-engineering",
    "href": "slides/lec-13.html#recap-feature-engineering",
    "title": "Feature engineering",
    "section": "Recap: Feature engineering",
    "text": "Recap: Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "slides/lec-13.html#recap-modeling-workflow-revisited",
    "href": "slides/lec-13.html#recap-modeling-workflow-revisited",
    "title": "Feature engineering",
    "section": "Recap: Modeling workflow, revisited",
    "text": "Recap: Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/lec-13.html#initiate-a-recipe",
    "href": "slides/lec-13.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5"
  },
  {
    "objectID": "slides/lec-13.html#step-1-alter-roles",
    "href": "slides/lec-13.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn’t a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec %&gt;%\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4"
  },
  {
    "objectID": "slides/lec-13.html#step-2-add-features",
    "href": "slides/lec-13.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec %&gt;%\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date"
  },
  {
    "objectID": "slides/lec-13.html#working-with-recipes",
    "href": "slides/lec-13.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\n\nNote\n\n\nThis is optional, we’ll show the results for demonstrative purposes. It doesn’t need to be part of your modeling pipeline, but I find it assuring to see the effects of the recipe steps as I build the recipe."
  },
  {
    "objectID": "slides/lec-13.html#step-2-prep-and-bake",
    "href": "slides/lec-13.html#step-2-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 2: Prep and bake",
    "text": "Step 2: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) %&gt;%\n  glimpse()\n\nRows: 141\nColumns: 8\n$ season         &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2…\n$ episode        &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20,…\n$ title          &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Review…\n$ total_votes    &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1…\n$ air_date       &lt;date&gt; 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-0…\n$ imdb_rating    &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, …\n$ air_date_dow   &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, …\n$ air_date_month &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr, Jan, May, …"
  },
  {
    "objectID": "slides/lec-13.html#step-3-add-more-features",
    "href": "slides/lec-13.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec %&gt;%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date"
  },
  {
    "objectID": "slides/lec-13.html#step-3-prep-and-bake",
    "href": "slides/lec-13.html#step-3-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 3: Prep and bake",
    "text": "Step 3: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) %&gt;%\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n$ air_date_USThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USChristmasDay    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USNewYearsDay     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USIndependenceDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "slides/lec-13.html#step-4-convert-numbers-to-factors",
    "href": "slides/lec-13.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec %&gt;%\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season"
  },
  {
    "objectID": "slides/lec-13.html#step-4-prep-and-bake",
    "href": "slides/lec-13.html#step-4-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 4: Prep and bake",
    "text": "Step 4: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) %&gt;%\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;fct&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n$ air_date_USThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USChristmasDay    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USNewYearsDay     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USIndependenceDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "slides/lec-13.html#step-5-make-dummy-variables",
    "href": "slides/lec-13.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec %&gt;%\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()"
  },
  {
    "objectID": "slides/lec-13.html#step-5-prep-and-bake",
    "href": "slides/lec-13.html#step-5-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 5: Prep and bake",
    "text": "Step 5: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) %&gt;%\n  glimpse()\n\nRows: 141\nColumns: 33\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n$ air_date_USThanksgivingDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USChristmasDay    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USNewYearsDay     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USIndependenceDay &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X2                  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X3                  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X4                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X5                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0…\n$ season_X6                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ season_X7                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ season_X8                  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ season_X9                  &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0…\n$ air_date_dow_Mon           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Tue           &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Wed           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Thu           &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ air_date_dow_Fri           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Sat           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Feb         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Mar         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Apr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1…\n$ air_date_month_May         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0…\n$ air_date_month_Jun         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Jul         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Aug         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Sep         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ air_date_month_Oct         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Nov         &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Dec         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "slides/lec-13.html#step-6-remove-zero-variance-predictors",
    "href": "slides/lec-13.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec %&gt;%\n  step_zv(all_predictors())\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-13.html#step-6-prep-and-bake",
    "href": "slides/lec-13.html#step-6-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 6: Prep and bake",
    "text": "Step 6: Prep and bake\n\noffice_rec_trained &lt;- prep(office_rec)\nbake(office_rec_trained, office_train) %&gt;%\n  glimpse()\n\nRows: 141\nColumns: 22\n$ episode            &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,…\n$ title              &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Re…\n$ total_votes        &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144…\n$ imdb_rating        &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8…\n$ season_X2          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X3          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X4          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X5          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n$ season_X6          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, …\n$ season_X7          &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ season_X8          &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X9          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ air_date_dow_Tue   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ air_date_dow_Thu   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ air_date_month_Feb &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ air_date_month_Mar &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ air_date_month_Apr &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ air_date_month_May &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ air_date_month_Sep &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ air_date_month_Oct &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ air_date_month_Nov &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ air_date_month_Dec &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "slides/lec-13.html#putting-it-altogether",
    "href": "slides/lec-13.html#putting-it-altogether",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  # make title's role ID\n  update_role(title, new_role = \"ID\") %&gt;%\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) %&gt;%\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %&gt;%\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) %&gt;%\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/lec-13.html#putting-it-altogether-1",
    "href": "slides/lec-13.html#putting-it-altogether-1",
    "title": "Feature engineering",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\noffice_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n   outcome          1\n predictor          4\n\nOperations:\n\nDate features from air_date\nHoliday features from air_date\nFactor variables from season\nDummy variables from all_nominal_predictors()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "slides/lec-13.html#specify-model",
    "href": "slides/lec-13.html#specify-model",
    "title": "Feature engineering",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-13.html#build-workflow",
    "href": "slides/lec-13.html#build-workflow",
    "title": "Feature engineering",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\noffice_wflow &lt;- workflow() %&gt;%\n  add_model(office_spec) %&gt;%\n  add_recipe(office_rec)\n\n\nSee next slide for workflow…"
  },
  {
    "objectID": "slides/lec-13.html#view-workflow",
    "href": "slides/lec-13.html#view-workflow",
    "title": "Feature engineering",
    "section": "View workflow",
    "text": "View workflow\n\noffice_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_holiday()\n• step_num2factor()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-13.html#fit-model-to-training-data",
    "href": "slides/lec-13.html#fit-model-to-training-data",
    "title": "Feature engineering",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\noffice_fit &lt;- office_wflow %&gt;%\n  fit(data = office_train)\n\ntidy(office_fit)\n\n# A tibble: 21 × 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  6.40     0.510        12.5   1.51e-23\n 2 episode     -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes  0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2    0.811    0.327         2.48  1.44e- 2\n 5 season_X3    1.04     0.343         3.04  2.91e- 3\n 6 season_X4    1.09     0.295         3.70  3.32e- 4\n 7 season_X5    1.08     0.348         3.11  2.34e- 3\n 8 season_X6    1.00     0.367         2.74  7.18e- 3\n 9 season_X7    1.02     0.352         2.89  4.52e- 3\n10 season_X8    0.497    0.348         1.43  1.55e- 1\n# … with 11 more rows\n\n\n\n\nSo many predictors!"
  },
  {
    "objectID": "slides/lec-13.html#model-fit-summary",
    "href": "slides/lec-13.html#model-fit-summary",
    "title": "Feature engineering",
    "section": "Model fit summary",
    "text": "Model fit summary\n\ntidy(office_fit) %&gt;% print(n = 21)\n\n# A tibble: 21 × 5\n   term                estimate std.error statistic  p.value\n   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)         6.40     0.510        12.5   1.51e-23\n 2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2           0.811    0.327         2.48  1.44e- 2\n 5 season_X3           1.04     0.343         3.04  2.91e- 3\n 6 season_X4           1.09     0.295         3.70  3.32e- 4\n 7 season_X5           1.08     0.348         3.11  2.34e- 3\n 8 season_X6           1.00     0.367         2.74  7.18e- 3\n 9 season_X7           1.02     0.352         2.89  4.52e- 3\n10 season_X8           0.497    0.348         1.43  1.55e- 1\n11 season_X9           0.621    0.345         1.80  7.41e- 2\n12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"
  },
  {
    "objectID": "slides/lec-13.html#make-predictions-for-training-data",
    "href": "slides/lec-13.html#make-predictions-for-training-data",
    "title": "Feature engineering",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\noffice_train_pred &lt;- predict(office_fit, office_train) %&gt;%\n  bind_cols(office_train %&gt;% select(imdb_rating, title))\n\noffice_train_pred\n\n# A tibble: 141 × 3\n   .pred imdb_rating title              \n   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;              \n 1  7.57         7.8 Last Day in Florida\n 2  7.77         7.6 Vandalism          \n 3  8.31         8.2 Performance Review \n 4  7.67         7.1 Here Comes Treble  \n 5  8.84         9.1 Beach Games        \n 6  8.33         8.4 Nepotism           \n 7  8.46         8.3 Phyllis' Wedding   \n 8  8.14         8.9 Livin' the Dream   \n 9  7.87         8   Promos             \n10  7.74         8   Pool Party         \n# … with 131 more rows"
  },
  {
    "objectID": "slides/lec-13.html#r-squared",
    "href": "slides/lec-13.html#r-squared",
    "title": "Feature engineering",
    "section": "R-squared",
    "text": "R-squared\nPercentage of variability in the IMDB ratings explained by the model.\n\n\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.670\n\n\n\n\n\nAre models with high or low \\(R^2\\) more preferable?"
  },
  {
    "objectID": "slides/lec-13.html#rmse",
    "href": "slides/lec-13.html#rmse",
    "title": "Feature engineering",
    "section": "RMSE",
    "text": "RMSE\nAn alternative model performance statistic: root mean square error.\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} \\]\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\n\nAre models with high or low RMSE are more preferable?"
  },
  {
    "objectID": "slides/lec-13.html#interpreting-rmse",
    "href": "slides/lec-13.html#interpreting-rmse",
    "title": "Feature engineering",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\n\nIs this RMSE considered low or high?\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\nDepends…\n\noffice_train %&gt;%\n  summarise(min = min(imdb_rating), max = max(imdb_rating))\n\n# A tibble: 1 × 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7"
  },
  {
    "objectID": "slides/lec-13.html#but-really",
    "href": "slides/lec-13.html#but-really",
    "title": "Feature engineering",
    "section": "But, really…",
    "text": "But, really…\nwho cares about predictions on training data?"
  },
  {
    "objectID": "slides/lec-13.html#make-predictions-for-testing-data",
    "href": "slides/lec-13.html#make-predictions-for-testing-data",
    "title": "Feature engineering",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\noffice_test_pred &lt;- predict(office_fit, office_test) %&gt;%\n  bind_cols(office_test %&gt;% select(imdb_rating, title))\n\noffice_test_pred\n\n# A tibble: 47 × 3\n   .pred imdb_rating title              \n   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;              \n 1  8.03         8.3 Diversity Day      \n 2  7.98         7.9 Health Care        \n 3  8.41         8.4 The Fire           \n 4  8.35         8.2 Halloween          \n 5  8.35         8.4 E-Mail Surveillance\n 6  8.68         9   The Injury         \n 7  8.32         7.9 The Carpet         \n 8  8.93         9.3 Casino Night       \n 9  8.80         8.9 Gay Witch Hunt     \n10  8.37         8.2 Initiation         \n# … with 37 more rows"
  },
  {
    "objectID": "slides/lec-13.html#evaluate-performance-for-testing-data",
    "href": "slides/lec-13.html#evaluate-performance-for-testing-data",
    "title": "Feature engineering",
    "section": "Evaluate performance for testing data",
    "text": "Evaluate performance for testing data\nRMSE of model fit to testing data\n\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.411\n\n\nR-sq of model fit to testing data\n\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.468"
  },
  {
    "objectID": "slides/lec-13.html#training-vs.-testing",
    "href": "slides/lec-13.html#training-vs.-testing",
    "title": "Feature engineering",
    "section": "Training vs. testing",
    "text": "Training vs. testing\n\n\n\n\n\n\n\n\n\nmetric\ntrain\ntest\ncomparison\n\n\n\n\nRMSE\n0.302\n0.411\nRMSE lower for training\n\n\nR-squared\n0.67\n0.468\nR-squared higher for training"
  },
  {
    "objectID": "slides/lec-13.html#evaluating-performance-on-training-data",
    "href": "slides/lec-13.html#evaluating-performance-on-training-data",
    "title": "Feature engineering",
    "section": "Evaluating performance on training data",
    "text": "Evaluating performance on training data\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test."
  },
  {
    "objectID": "slides/lec-8.html#announcements",
    "href": "slides/lec-8.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nLab 3:\n\nAny questions about lab / teamwork?\nDue Friday, Feb 4 at 5pm\n\nExam 1:\n\nReleased Friday, Feb 4\nMultiple choice questions (mostly conceptual) + open-ended exercises (like lab + homework)\nOpen book, open internet, open questions to me + Rick (head TA) only\nNo communication with others or posting questions on the internet allowed\nWhat can you do to start preparing?\n\nReview readings, assignments, feedback returned\nOrganize your notes\nCome to office hours with questions"
  },
  {
    "objectID": "slides/lec-8.html#computational-setup",
    "href": "slides/lec-8.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # for laying out plots\nlibrary(GGally)      # for pairwise plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-8.html#house-prices-in-levittown",
    "href": "slides/lec-8.html#house-prices-in-levittown",
    "title": "Multiple linear regression (MLR)",
    "section": "House prices in Levittown",
    "text": "House prices in Levittown\n\nThe data set contains the sales price and characteristics of 85 homes in Levittown, NY that sold between June 2010 and May 2011.\nLevittown was built right after WWII and was the first planned suburban community built using mass production techniques.\nThe article “Levittown, the prototypical American suburb – a history of cities in 50 buildings, day 25” gives an overview of Levittown’s controversial history."
  },
  {
    "objectID": "slides/lec-8.html#analysis-goals",
    "href": "slides/lec-8.html#analysis-goals",
    "title": "Multiple linear regression (MLR)",
    "section": "Analysis goals",
    "text": "Analysis goals\n\nWe would like to use the characteristics of a house to understand variability in the sales price.\nTo do so, we will fit a multiple linear regression model.\nUsing our model, we can answers questions such as\n\nWhat is the relationship between the characteristics of a house in Levittown and its sale price?\nGiven its characteristics, what is the expected sale price of a house in Levittown?"
  },
  {
    "objectID": "slides/lec-8.html#the-data",
    "href": "slides/lec-8.html#the-data",
    "title": "Multiple linear regression (MLR)",
    "section": "The data",
    "text": "The data\n\nlevittown &lt;- read_csv(here::here(\"slides/data/homeprices.csv\"))\nlevittown\n\n# A tibble: 85 × 7\n   bedrooms bathrooms living_area lot_size year_built property_tax sale_price\n      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1        4       1          1380     6000       1948         8360     350000\n 2        4       2          1761     7400       1951         5754     360000\n 3        4       2          1564     6000       1948         8982     350000\n 4        5       2          2904     9898       1949        11664     375000\n 5        5       2.5        1942     7788       1948         8120     370000\n 6        4       2          1830     6000       1948         8197     335000\n 7        4       1          1585     6000       1948         6223     295000\n 8        4       1           941     6800       1951         2448     250000\n 9        4       1.5        1481     6000       1948         9087     299990\n10        3       2          1630     5998       1948         9430     375000\n# … with 75 more rows"
  },
  {
    "objectID": "slides/lec-8.html#variables",
    "href": "slides/lec-8.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\nbedrooms: Number of bedrooms\nbathrooms: Number of bathrooms\nliving_area: Total living area of the house (in square feet)\nlot_size: Total area of the lot (in square feet)\nyear_built: Year the house was built\nproperty_tax: Annual property taxes (in USD)\n\nResponse: sale_price: Sales price (in USD)"
  },
  {
    "objectID": "slides/lec-8.html#eda-response-variable",
    "href": "slides/lec-8.html#eda-response-variable",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Response variable",
    "text": "EDA: Response variable"
  },
  {
    "objectID": "slides/lec-8.html#eda-predictor-variables",
    "href": "slides/lec-8.html#eda-predictor-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Predictor variables",
    "text": "EDA: Predictor variables"
  },
  {
    "objectID": "slides/lec-8.html#eda-response-vs.-predictors",
    "href": "slides/lec-8.html#eda-response-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: Response vs. Predictors",
    "text": "EDA: Response vs. Predictors"
  },
  {
    "objectID": "slides/lec-8.html#eda-all-variables",
    "href": "slides/lec-8.html#eda-all-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "EDA: All variables",
    "text": "EDA: All variables\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggpairs(levittown) +\n  theme(\n    axis.text.y = element_text(size = 10),\n    axis.text.x = element_text(angle = 45, size = 10),\n    strip.text.y = element_text(angle = 0, hjust = 0)\n    )"
  },
  {
    "objectID": "slides/lec-8.html#single-vs.-multiple-predictors",
    "href": "slides/lec-8.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs. multiple predictors",
    "text": "Single vs. multiple predictors\nSo far we’ve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/lec-8.html#multiple-linear-regression-mlr",
    "href": "slides/lec-8.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{sale_price}} ~ = & ~\n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{bedrooms} + \\hat{\\beta}_2 \\text{bathrooms} + \\hat{\\beta}_3 \\text{living_area} \\\\\n&+ \\hat{\\beta}_4 \\text{lot_size} + \\hat{\\beta}_5 \\text{year_built} + \\hat{\\beta}_6 \\text{property_tax}\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values sale_price follow a Normal distribution."
  },
  {
    "objectID": "slides/lec-8.html#regression-model",
    "href": "slides/lec-8.html#regression-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Regression Model",
    "text": "Regression Model\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#the-mlr-model",
    "href": "slides/lec-8.html#the-mlr-model",
    "title": "Multiple linear regression (MLR)",
    "section": "The MLR model",
    "text": "The MLR model\nFor a given observation \\((x_{i1}, x_{i2} \\ldots, x_{ip}, y_i)\\)\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{8mm} \\epsilon_i \\sim N(0,\\sigma_\\epsilon^2)\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#prediction",
    "href": "slides/lec-8.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#model-fit",
    "href": "slides/lec-8.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\nprice_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(sale_price ~ bedrooms + bathrooms + living_area + lot_size +\n        year_built + property_tax, data = levittown)\n\ntidy(price_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7148818.957\n3820093.694\n-1.871\n0.065\n\n\nbedrooms\n-12291.011\n9346.727\n-1.315\n0.192\n\n\nbathrooms\n51699.236\n13094.170\n3.948\n0.000\n\n\nliving_area\n65.903\n15.979\n4.124\n0.000\n\n\nlot_size\n-0.897\n4.194\n-0.214\n0.831\n\n\nyear_built\n3760.898\n1962.504\n1.916\n0.059\n\n\nproperty_tax\n1.476\n2.832\n0.521\n0.604"
  },
  {
    "objectID": "slides/lec-8.html#model-equation",
    "href": "slides/lec-8.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{price}} = & -7148818.957 - 12291.011 \\times \\text{bedrooms}\\\\[5pt]  \n&+ 51699.236 \\times \\text{bathrooms}  + 65.903 \\times \\text{living area}\\\\[5pt]\n&- 0.897 \\times \\text{lot size} +  3760.898 \\times \\text{year built}\\\\[5pt]\n&+ 1.476 \\times \\text{property tax}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#interpreting-hatbeta_j",
    "href": "slides/lec-8.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for living_area is 65.90. This means for each additional square foot of living area, we expect the sale price of a house in Levittown, NY to increase by $65.90, on average, holding all other predictor variables constant."
  },
  {
    "objectID": "slides/lec-8.html#prediction-1",
    "href": "slides/lec-8.html#prediction-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1,050 square feet of living area, 6,000 square foot lot size, built in 1948 with $6,306 in property taxes?\n\n\n\n-7148818.957 - 12291.011 * 3 + 51699.236 * 1 + \n  65.903 * 1050 - 0.897 * 6000 + 3760.898 * 1948 + \n  1.476 * 6306\n\n[1] 265360.4\n\n\n\nThe predicted sale price for a house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes is $265,360."
  },
  {
    "objectID": "slides/lec-8.html#prediction-revisit",
    "href": "slides/lec-8.html#prediction-revisit",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisit",
    "text": "Prediction, revisit\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_house &lt;- tibble(\n  bedrooms = 3, bathrooms = 1, \n  living_area = 1050, lot_size = 6000, \n  year_built = 1948, property_tax = 6306\n  )\n\npredict(price_fit, new_house)\n\n# A tibble: 1 × 1\n    .pred\n    &lt;dbl&gt;\n1 265360."
  },
  {
    "objectID": "slides/lec-8.html#confidence-interval-for-hatmu_y",
    "href": "slides/lec-8.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 95% confidence interval for the estimated mean price of houses in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"conf_int\", level = 0.95)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     238482.     292239."
  },
  {
    "objectID": "slides/lec-8.html#prediction-interval-for-haty",
    "href": "slides/lec-8.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 95% prediction interval for an individual house in Levittown, NY with 3 bedrooms, 1 bathroom, 1050 square feet of living area, 6000 square foot lot size, built in 1948 with $6306 in property taxes.\n\n\n\npredict(price_fit, new_house, type = \"pred_int\", level = 0.95)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1     167277.     363444."
  },
  {
    "objectID": "slides/lec-8.html#cautions",
    "href": "slides/lec-8.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/lec-8.html#recap",
    "href": "slides/lec-8.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted a coefficient \\(\\hat{\\beta}_j\\)\nUsed the model to calculate predicted values and the corresponding intervals"
  },
  {
    "objectID": "slides/lec-11.html#announcements",
    "href": "slides/lec-11.html#announcements",
    "title": "Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nHW 2 posted tomorrow, due in a week\nLab on Monday: Work data selection! (Read project assignment over the weekend)\nMid-semester course evaluation (to be posted)"
  },
  {
    "objectID": "slides/lec-11.html#topics",
    "href": "slides/lec-11.html#topics",
    "title": "Model comparison",
    "section": "Topics",
    "text": "Topics\n\nANOVA for Multiple Linear Regression and sum of squares\nComparing models with \\(R^2\\) vs. \\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam’s razor and parsimony\nOverfitting and spending our data"
  },
  {
    "objectID": "slides/lec-11.html#computational-setup",
    "href": "slides/lec-11.html#computational-setup",
    "title": "Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-11.html#data-restaurant-tips",
    "href": "slides/lec-11.html#data-restaurant-tips",
    "title": "Model comparison",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# … with 159 more rows"
  },
  {
    "objectID": "slides/lec-11.html#variables",
    "href": "slides/lec-11.html#variables",
    "title": "Model comparison",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/lec-11.html#outcome-tip",
    "href": "slides/lec-11.html#outcome-tip",
    "title": "Model comparison",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/lec-11.html#predictors",
    "href": "slides/lec-11.html#predictors",
    "title": "Model comparison",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/lec-11.html#relevel-categorical-predictors",
    "href": "slides/lec-11.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips %&gt;%\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/lec-11.html#predictors-again",
    "href": "slides/lec-11.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/lec-11.html#outcome-vs.-predictors",
    "href": "slides/lec-11.html#outcome-vs.-predictors",
    "title": "Model comparison",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/lec-11.html#fit-and-summarize-model",
    "href": "slides/lec-11.html#fit-and-summarize-model",
    "title": "Model comparison",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\ntip_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit, conf.int = TRUE) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n-0.893\n0.553\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n1.591\n2.083\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n0.204\n1.813\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n0.430\n2.345\n\n\n\n\n\n\n\n\nIs this the best model to explain variation in tips?"
  },
  {
    "objectID": "slides/lec-11.html#another-model-summary",
    "href": "slides/lec-11.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/lec-11.html#analysis-of-variance-anova-1",
    "href": "slides/lec-11.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can’t be explained by the model (left in the residuals)\n\nIf the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be “valuable” (at least one of the \\(\\beta\\)s not equal to 0)"
  },
  {
    "objectID": "slides/lec-11.html#anova-output",
    "href": "slides/lec-11.html#anova-output",
    "title": "Model comparison",
    "section": "ANOVA output",
    "text": "ANOVA output\n\nanova(tip_fit$fit) %&gt;%\n  tidy() %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/lec-11.html#anova-output-with-totals",
    "href": "slides/lec-11.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/lec-11.html#sum-of-squares",
    "href": "slides/lec-11.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y})^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model"
  },
  {
    "objectID": "slides/lec-11.html#r-squared-r2",
    "href": "slides/lec-11.html#r-squared-r2",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]\n\n\n\nglance(tip_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.641         0.635  2.04      98.3 1.56e-36     3  -358.  726.  742.\n# … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/lec-11.html#r-squared-r2-1",
    "href": "slides/lec-11.html#r-squared-r2-1",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/lec-11.html#adjusted-r2",
    "href": "slides/lec-11.html#adjusted-r2",
    "title": "Model comparison",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/lec-11.html#r2-and-adjusted-r2",
    "href": "slides/lec-11.html#r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]"
  },
  {
    "objectID": "slides/lec-11.html#using-r2-and-adjusted-r2",
    "href": "slides/lec-11.html#using-r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/lec-11.html#comparing-models-with-r2_adj",
    "href": "slides/lec-11.html#comparing-models-with-r2_adj",
    "title": "Model comparison",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) %&gt;% \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.674         0.664\n\n\n\n\ntip_fit_2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) %&gt;% \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.683         0.662"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic",
    "href": "slides/lec-11.html#aic-bic",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic-1",
    "href": "slides/lec-11.html#aic-bic-1",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic-2",
    "href": "slides/lec-11.html#aic-bic-2",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/lec-11.html#aic-bic-3",
    "href": "slides/lec-11.html#aic-bic-3",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/lec-11.html#using-aic-bic",
    "href": "slides/lec-11.html#using-aic-bic",
    "title": "Model comparison",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e. models with fewer terms)"
  },
  {
    "objectID": "slides/lec-11.html#comparing-models-with-aic-and-bic",
    "href": "slides/lec-11.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) %&gt;% \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  714.  736.\n\n\n\n\ntip_fit_2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) %&gt;% \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  720.  757."
  },
  {
    "objectID": "slides/lec-11.html#commonalities-between-criteria",
    "href": "slides/lec-11.html#commonalities-between-criteria",
    "title": "Model comparison",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/lec-11.html#parsimony-and-occams-razor",
    "href": "slides/lec-11.html#parsimony-and-occams-razor",
    "title": "Model comparison",
    "section": "Parsimony and Occam’s razor",
    "text": "Parsimony and Occam’s razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam’s razor because he “shaved” his explanations down to the bare minimum\nParsimony in modeling:\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/lec-11.html#in-pursuit-of-occams-razor",
    "href": "slides/lec-11.html#in-pursuit-of-occams-razor",
    "title": "Model comparison",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e. parsimonious model"
  },
  {
    "objectID": "slides/lec-11.html#alternate-views",
    "href": "slides/lec-11.html#alternate-views",
    "title": "Model comparison",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/lec-11.html#other-concerns-with-our-approach",
    "href": "slides/lec-11.html#other-concerns-with-our-approach",
    "title": "Model comparison",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we’re making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n“train” the model on the training data and pick a few models we’re genuinely considering as potentially good models\ntest those models on the testing set"
  },
  {
    "objectID": "slides/lec-11.html#recap",
    "href": "slides/lec-11.html#recap",
    "title": "Model comparison",
    "section": "Recap",
    "text": "Recap\n\nANOVA for Multiple Linear Regression and sum of squares\nComparing models with \\(R^2\\) vs. \\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam’s razor and parsimony\nOverfitting and spending our data"
  },
  {
    "objectID": "slides/lec-6.html#computational-setup",
    "href": "slides/lec-6.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-6.html#data-duke-forest-houses",
    "href": "slides/lec-6.html#data-duke-forest-houses",
    "title": "SLR: Mathematical models for inference",
    "section": "Data: Duke Forest houses",
    "text": "Data: Duke Forest houses"
  },
  {
    "objectID": "slides/lec-6.html#the-regression-model",
    "href": "slides/lec-6.html#the-regression-model",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model",
    "text": "The regression model\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/lec-6.html#inference-for-simple-linear-regression",
    "href": "slides/lec-6.html#inference-for-simple-linear-regression",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the interval, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/lec-6.html#confidence-interval-via-bootstrapping",
    "href": "slides/lec-6.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-i",
    "href": "slides/lec-6.html#bootstrapping-pipeline-i",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline I",
    "text": "Bootstrapping pipeline I\n\n# #| code-line-numbers: \"|1|3|4\"\n#| \nset.seed(119)\n\nduke_forest %&gt;%\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 × 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# … with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-ii",
    "href": "slides/lec-6.html#bootstrapping-pipeline-ii",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline II",
    "text": "Bootstrapping pipeline II\n\n# #| code-line-numbers: \"|5\"\n\nset.seed(119)\n\nduke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98,000 × 3\n# Groups:   replicate [1,000]\n   replicate   price  area\n       &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1         1  535000  2334\n 2         1  520000  2637\n 3         1  540000  2165\n 4         1  155000  1620\n 5         1  567000  3931\n 6         1  420000  1745\n 7         1  400000  4769\n 8         1  579000  2926\n 9         1  615000  2203\n10         1 1030000  4475\n# … with 97,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-iii",
    "href": "slides/lec-6.html#bootstrapping-pipeline-iii",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline III",
    "text": "Bootstrapping pipeline III\n\n# #| code-line-numbers: \"|6\"\n\nset.seed(119)\n\nduke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept  200401.\n 2         1 area          122.\n 3         2 intercept  120000.\n 4         2 area          156.\n 5         3 intercept  190879.\n 6         3 area          126.\n 7         4 intercept  206842.\n 8         4 area          127.\n 9         5 intercept  211231.\n10         5 area          124.\n# … with 1,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#bootstrapping-pipeline-iv",
    "href": "slides/lec-6.html#bootstrapping-pipeline-iv",
    "title": "SLR: Mathematical models for inference",
    "section": "Bootstrapping pipeline IV",
    "text": "Bootstrapping pipeline IV\n\n# #| code-line-numbers: \"|3\"\n\nset.seed(119)\n\nboot_dist &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  fit()"
  },
  {
    "objectID": "slides/lec-6.html#visualize-the-bootstrap-distribution",
    "href": "slides/lec-6.html#visualize-the-bootstrap-distribution",
    "title": "SLR: Mathematical models for inference",
    "section": "Visualize the bootstrap distribution",
    "text": "Visualize the bootstrap distribution\n\n# #| code-line-numbers: \"|2\"\n\nboot_dist %&gt;%\n  filter(term == \"area\") %&gt;%\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10)"
  },
  {
    "objectID": "slides/lec-6.html#compute-the-ci",
    "href": "slides/lec-6.html#compute-the-ci",
    "title": "SLR: Mathematical models for inference",
    "section": "Compute the CI",
    "text": "Compute the CI\nTwo methods:\n\nPercentile method\nStandard error method"
  },
  {
    "objectID": "slides/lec-6.html#but-first",
    "href": "slides/lec-6.html#but-first",
    "title": "SLR: Mathematical models for inference",
    "section": "But first…",
    "text": "But first…\n\nobs_fit &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  fit()\n\nobs_fit\n\n# A tibble: 2 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/lec-6.html#percentile-method",
    "href": "slides/lec-6.html#percentile-method",
    "title": "SLR: Mathematical models for inference",
    "section": "Percentile method",
    "text": "Percentile method\n\n# #| code-line-numbers: \"|4\"\n\nboot_dist %&gt;%\n  get_confidence_interval(\n    level = 0.95,\n    type = \"percentile\",\n    point_estimate = obs_fit\n  )\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          91.0     215.\n2 intercept -22046.   289004."
  },
  {
    "objectID": "slides/lec-6.html#standard-error-method",
    "href": "slides/lec-6.html#standard-error-method",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error method",
    "text": "Standard error method\n\n# #| code-line-numbers: \"|4\"\n\nboot_dist %&gt;%\n  get_confidence_interval(\n    level = 0.95,\n    type = \"se\",\n    point_estimate = obs_fit\n  )\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          96.1     223.\n2 intercept -39805.   273109."
  },
  {
    "objectID": "slides/lec-6.html#research-question-and-hypotheses",
    "href": "slides/lec-6.html#research-question-and-hypotheses",
    "title": "SLR: Mathematical models for inference",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n\n“Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-6.html#hypothesis-testing-framework",
    "href": "slides/lec-6.html#hypothesis-testing-framework",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e. what we’re testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/lec-6.html#quantify-the-variability-of-the-slope",
    "href": "slides/lec-6.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor testing\n\nTwo approaches:\n\nVia simulation (what we’ll review from lab)\nVia mathematical models (what we’ll cover in the remainder of class)\n\nRandomizing to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/lec-6.html#permutation-described",
    "href": "slides/lec-6.html#permutation-described",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nSet the null hypothesis to be true, and measure the natural variability in the data due to sampling but not due to variables being correlated by permuting permute one variable to eliminate any existing relationship between the variables\nEach price value is randomly assigned to area of a given house, i.e. area and price are no longer matched for a given house\n\n\n\n\n# A tibble: 98 × 3\n   price_Observed price_Permuted  area\n            &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1        1520000         342500  6040\n 2        1030000         750000  4475\n 3         420000         645000  1745\n 4         680000         697500  2091\n 5         428500         428500  1772\n 6         456000         481000  1950\n 7        1270000         610000  3909\n 8         557450         680000  2841\n 9         697500         485000  3924\n10         650000         105000  2173\n# … with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-visualized",
    "href": "slides/lec-6.html#permutation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nThe permutation removes the linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-6.html#permutation-repeated",
    "href": "slides/lec-6.html#permutation-repeated",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)"
  },
  {
    "objectID": "slides/lec-6.html#concluding-the-hypothesis-test",
    "href": "slides/lec-6.html#concluding-the-hypothesis-test",
    "title": "SLR: Mathematical models for inference",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 159\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: “Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-i",
    "href": "slides/lec-6.html#permutation-pipeline-i",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\n# #| code-line-numbers: \"|1|3|4\"\n#| \nset.seed(1125)\n\nduke_forest %&gt;%\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 × 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# … with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-ii",
    "href": "slides/lec-6.html#permutation-pipeline-ii",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\n# #| code-line-numbers: \"|5\"\n\nset.seed(1125)\n\nduke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  hypothesize(null = \"independence\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98 × 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# … with 88 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-iii",
    "href": "slides/lec-6.html#permutation-pipeline-iii",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\n# #| code-line-numbers: \"|6\"\n\nset.seed(1125)\n\nduke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98,000 × 3\n# Groups:   replicate [1,000]\n     price  area replicate\n     &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;\n 1  465000  6040         1\n 2  481000  4475         1\n 3 1020000  1745         1\n 4  520000  2091         1\n 5  592000  1772         1\n 6  650000  1950         1\n 7  473000  3909         1\n 8  705000  2841         1\n 9  785000  3924         1\n10  671500  2173         1\n# … with 97,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-iv",
    "href": "slides/lec-6.html#permutation-pipeline-iv",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\n# #| code-line-numbers: \"|7\"\n\nset.seed(1125)\n\nduke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term       estimate\n       &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1         1 intercept 553355.  \n 2         1 area           2.35\n 3         2 intercept 635824.  \n 4         2 area         -27.3 \n 5         3 intercept 536072.  \n 6         3 area           8.57\n 7         4 intercept 598649.  \n 8         4 area         -13.9 \n 9         5 intercept 556202.  \n10         5 area           1.33\n# … with 1,990 more rows"
  },
  {
    "objectID": "slides/lec-6.html#permutation-pipeline-v",
    "href": "slides/lec-6.html#permutation-pipeline-v",
    "title": "SLR: Mathematical models for inference",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\n# #| code-line-numbers: \"|3\"\n\nset.seed(1125)\n\nnull_dist &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  fit()"
  },
  {
    "objectID": "slides/lec-6.html#visualize-the-null-distribution",
    "href": "slides/lec-6.html#visualize-the-null-distribution",
    "title": "SLR: Mathematical models for inference",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\n# #| code-line-numbers: \"|2\"\n\nnull_dist %&gt;%\n  filter(term == \"area\") %&gt;%\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10, color = \"white\")"
  },
  {
    "objectID": "slides/lec-6.html#reason-around-the-p-value",
    "href": "slides/lec-6.html#reason-around-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the area of a Duke Forest house and in its price (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 98 houses where the slope fo the model predicting price from area is 159 or even more extreme?"
  },
  {
    "objectID": "slides/lec-6.html#compute-the-p-value",
    "href": "slides/lec-6.html#compute-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = obs_fit,\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\n\n# A tibble: 2 × 2\n  term      p_value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 area            0\n2 intercept       0"
  },
  {
    "objectID": "slides/lec-6.html#the-regression-model-revisited",
    "href": "slides/lec-6.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/lec-6.html#ht-and-ci-recapped",
    "href": "slides/lec-6.html#ht-and-ci-recapped",
    "title": "SLR: Mathematical models for inference",
    "section": "HT and CI, recapped",
    "text": "HT and CI, recapped\n\nHypothesis test:\n\nDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price.\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price.\n\nConfidence interval: Provide a plausible range of values for \\(\\beta_1\\) at a given confidence level."
  },
  {
    "objectID": "slides/lec-6.html#ht-and-ci-revisited",
    "href": "slides/lec-6.html#ht-and-ci-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "HT and CI, revisited",
    "text": "HT and CI, revisited\n\nEarlier we computed a CI and conducted a HT via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we’ll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the CI and the p-value for the HT"
  },
  {
    "objectID": "slides/lec-6.html#mathematical-representation-of-the-model",
    "href": "slides/lec-6.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\nindependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/lec-6.html#mathematical-representation-visualized",
    "href": "slides/lec-6.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/lec-6.html#regression-standard-error",
    "href": "slides/lec-6.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error (the spread of the distribution of the response, for a given value of the predictor variable):\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/lec-6.html#standard-error-of-hatbeta_1",
    "href": "slides/lec-6.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\nor…\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/lec-21.html#announcements",
    "href": "slides/lec-21.html#announcements",
    "title": "LR: Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nHW 3 and Lab 5 due tomorrow\nMonday’s lab: Work on projects, make sure to get ~10 mins with your TA to review proposal comments and get feedback on plans\nOver the weekend, due Monday night: Complete team peer evaluations\nOffice hours reminder: Monday on Zoom, Thursday in person (Old Chem 213)"
  },
  {
    "objectID": "slides/lab-3.html#goals",
    "href": "slides/lab-3.html#goals",
    "title": "Lab 3 - Coffee ratings",
    "section": "Goals",
    "text": "Goals\n\nMeet your team!\nIce breaker / get to know your team\nTeam agreement\nLab 3 - Coffee ratings"
  },
  {
    "objectID": "slides/lab-3.html#meet-your-team",
    "href": "slides/lab-3.html#meet-your-team",
    "title": "Lab 3 - Coffee ratings",
    "section": "Meet your team!",
    "text": "Meet your team!\n\nClick here to find your team.\nSit with your team."
  },
  {
    "objectID": "slides/lab-3.html#icebreaker",
    "href": "slides/lab-3.html#icebreaker",
    "title": "Lab 3 - Coffee ratings",
    "section": "Icebreaker",
    "text": "Icebreaker\n\nQuick introductions: Name and hometown\nChoose a reporter: The person whose birthday is closest to January 31\nIdentify 8 things everyone in the group has in common:\n\nNot clothes, e.g., we’re all wearing shoes!\nNot body parts, e.g., we all have a nose!\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lab-3.html#team-name-agreement",
    "href": "slides/lab-3.html#team-name-agreement",
    "title": "Lab 3 - Coffee ratings",
    "section": "Team name + agreement",
    "text": "Team name + agreement\n\nCome up with a team name. You can’t have the same name as another group in the class, so be creative!\n\nYour TA will get your team name by the end of lab.\n\nFill out the team agreement. The goals of the agreement are to…\n\nGain a common understanding of the team’s goals and expectations for collaboration\nMake a plan for team communication\nMake a plan for working outside of lab"
  },
  {
    "objectID": "slides/lab-3.html#team-workflow",
    "href": "slides/lab-3.html#team-workflow",
    "title": "Lab 3 - Coffee ratings",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are markers in today’s lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\nOnly one submission for the team on Gradescope."
  },
  {
    "objectID": "slides/lab-3.html#team-workflow-in-action",
    "href": "slides/lab-3.html#team-workflow-in-action",
    "title": "Lab 3 - Coffee ratings",
    "section": "Team workflow, in action",
    "text": "Team workflow, in action\n\nComplete the “Workflow: Using Git and GitHub as a team” section of the lab in your teams.\nWhen done, pause and wait for your TA to walk you through the rest of the slides before continuing to the following section.\n\n\n\n\n10:00"
  },
  {
    "objectID": "slides/lab-3.html#tips-for-working-in-a-team",
    "href": "slides/lab-3.html#tips-for-working-in-a-team",
    "title": "Lab 3 - Coffee ratings",
    "section": "Tips for working in a team",
    "text": "Tips for working in a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other."
  },
  {
    "objectID": "slides/lab-3.html#the-data",
    "href": "slides/lab-3.html#the-data",
    "title": "Lab 3 - Coffee ratings",
    "section": "The data",
    "text": "The data"
  },
  {
    "objectID": "slides/lab-3.html#the-data-an-outlier",
    "href": "slides/lab-3.html#the-data-an-outlier",
    "title": "Lab 3 - Coffee ratings",
    "section": "The data + an outlier",
    "text": "The data + an outlier"
  },
  {
    "objectID": "slides/lab-3.html#the-data-influential-point",
    "href": "slides/lab-3.html#the-data-influential-point",
    "title": "Lab 3 - Coffee ratings",
    "section": "The data + influential point",
    "text": "The data + influential point"
  },
  {
    "objectID": "slides/lab-3.html#influential-point",
    "href": "slides/lab-3.html#influential-point",
    "title": "Lab 3 - Coffee ratings",
    "section": "Influential point",
    "text": "Influential point\nAn observation is influential if removing it substantially changes the coefficients of the regression model."
  },
  {
    "objectID": "slides/lab-3.html#influential-points",
    "href": "slides/lab-3.html#influential-points",
    "title": "Lab 3 - Coffee ratings",
    "section": "Influential points",
    "text": "Influential points\n\nInfluential points have a large impact on the coefficients and standard errors used for inference\nThese points can sometimes be identified in a scatterplot if there is only one predictor variable, this is often not the case when there are multiple predictors\nWe will use measures to quantify an individual observation’s influence on the regression model: leverage, standardized residuals, and Cook’s distance"
  },
  {
    "objectID": "slides/lab-3.html#remember-augment",
    "href": "slides/lab-3.html#remember-augment",
    "title": "Lab 3 - Coffee ratings",
    "section": "Remember augment()?",
    "text": "Remember augment()?\n\nmtcars_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(mpg ~ disp, data = mtcars)\n\naugment(mtcars_fit$fit)\n\n# A tibble: 32 × 9\n   .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 Mazda RX4          21    160     23.0 -2.01  0.0418   3.29 8.65e-3     -0.630\n 2 Mazda RX4 Wag      21    160     23.0 -2.01  0.0418   3.29 8.65e-3     -0.630\n 3 Datsun 710         22.8  108     25.1 -2.35  0.0629   3.28 1.87e-2     -0.746\n 4 Hornet 4 Drive     21.4  258     19.0  2.43  0.0328   3.27 9.83e-3      0.761\n 5 Hornet Sportabout  18.7  360     14.8  3.94  0.0663   3.22 5.58e-2      1.25 \n 6 Valiant            18.1  225     20.3 -2.23  0.0313   3.28 7.82e-3     -0.696\n 7 Duster 360         14.3  360     14.8 -0.462 0.0663   3.31 7.70e-4     -0.147\n 8 Merc 240D          24.4  147.    23.6  0.846 0.0461   3.30 1.72e-3      0.267\n 9 Merc 230           22.8  141.    23.8 -0.997 0.0482   3.30 2.50e-3     -0.314\n10 Merc 280           19.2  168.    22.7 -3.49  0.0396   3.24 2.48e-2     -1.10 \n# … with 22 more rows"
  },
  {
    "objectID": "slides/lab-3.html#model-diagnostics-1",
    "href": "slides/lab-3.html#model-diagnostics-1",
    "title": "Lab 3 - Coffee ratings",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nUse the augment() function to output statistics that can be used to diagnose the model, along with the predicted values and residuals:\n\noutcome and predictor variables in the model\n.fitted: predicted values\n.se.fit: standard errors of predicted values\n.resid: residuals\n.hat: leverage\n.sigma: estimate of residual standard deviation when the corresponding observation is dropped from model\n.cooksd: Cook’s distance\n.std.resid: standardized residuals"
  },
  {
    "objectID": "slides/lec-5.html#announcements",
    "href": "slides/lec-5.html#announcements",
    "title": "SLR: Simulation based-inference",
    "section": "Announcements",
    "text": "Announcements\n\nHW 1 posted tomorrow, due next Friday"
  },
  {
    "objectID": "slides/lec-5.html#computational-setup",
    "href": "slides/lec-5.html#computational-setup",
    "title": "SLR: Simulation based-inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for pretty tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-5.html#terminology",
    "href": "slides/lec-5.html#terminology",
    "title": "SLR: Simulation based-inference",
    "section": "Terminology",
    "text": "Terminology\n\nOutcome: y\nPredictor: x\nObserved y, \\(y\\): truth\nPredicted y, \\(\\hat{y}\\): fitted, estimated\nResidual: difference between observed and predicted outcome for a given value of predictor"
  },
  {
    "objectID": "slides/lec-5.html#model-evaluation",
    "href": "slides/lec-5.html#model-evaluation",
    "title": "SLR: Simulation based-inference",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nOne concern in evaluating models is how well they do for prediction\nWe’re generally interested in how well a model might do for predicting the outcome for a new observation, not for predicting the outcome for an observation we used to fit the model (and already know its observed value)\nEvaluating predictive performance: Split the data into testing and training sets, build models using only the training set, and evaluate their performance on the testing set, and repeat many times to see how your model holds up to “new” data\nQuantifying variability of of estimates: Bootstrap the data, fit a model, obtain coefficient estimates and/or measures of strength of fit, and repeat many times to see how your model holds up to “new” data\nToday we introduced these concepts, throughout the semester we’ll learn how to implement them (i.e., write the code) and how to interpret their results"
  },
  {
    "objectID": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-nc",
    "href": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-nc",
    "title": "SLR: Simulation based-inference",
    "section": "Uninsurance vs. HS graduation in NC",
    "text": "Uninsurance vs. HS graduation in NC"
  },
  {
    "objectID": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-ny",
    "href": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-ny",
    "title": "SLR: Simulation based-inference",
    "section": "Uninsurance vs. HS graduation in NY",
    "text": "Uninsurance vs. HS graduation in NY\n\n\nCode\ncounty_2019_ny &lt;- county_2019 %&gt;%\n  as_tibble() %&gt;%\n  filter(state == \"New York\") %&gt;%\n  select(name, hs_grad, uninsured)\n\nggplot(county_2019_ny,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"New York counties, 2015 - 2019\"\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"pink\")"
  },
  {
    "objectID": "slides/lec-5.html#data-splitting",
    "href": "slides/lec-5.html#data-splitting",
    "title": "SLR: Simulation based-inference",
    "section": "Data splitting",
    "text": "Data splitting"
  },
  {
    "objectID": "slides/lec-5.html#bootstrapping",
    "href": "slides/lec-5.html#bootstrapping",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/lec-5.html#comparing-ny-and-nc",
    "href": "slides/lec-5.html#comparing-ny-and-nc",
    "title": "SLR: Simulation based-inference",
    "section": "Comparing NY and NC",
    "text": "Comparing NY and NC\n\nWhy are the fits from the NY models more variable than those from the NC models?"
  },
  {
    "objectID": "slides/lec-5.html#data-sale-prices-of-houses-in-duke-forest",
    "href": "slides/lec-5.html#data-sale-prices-of-houses-in-duke-forest",
    "title": "SLR: Simulation based-inference",
    "section": "Data: Sale prices of houses in Duke Forest",
    "text": "Data: Sale prices of houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest"
  },
  {
    "objectID": "slides/lec-5.html#exploratory-analysis",
    "href": "slides/lec-5.html#exploratory-analysis",
    "title": "SLR: Simulation based-inference",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis"
  },
  {
    "objectID": "slides/lec-5.html#modeling",
    "href": "slides/lec-5.html#modeling",
    "title": "SLR: Simulation based-inference",
    "section": "Modeling",
    "text": "Modeling\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/lec-5.html#sample-to-population",
    "href": "slides/lec-5.html#sample-to-population",
    "title": "SLR: Simulation based-inference",
    "section": "Sample to population",
    "text": "Sample to population\n\nFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159.\n\n\nThis estimate is valid for the single sample of 98 houses.\nBut what if we’re not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all houses in Duke Forest?"
  },
  {
    "objectID": "slides/lec-5.html#statistical-inference-1",
    "href": "slides/lec-5.html#statistical-inference-1",
    "title": "SLR: Simulation based-inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference allows provide methods and tools for us to use the single sample we have observed to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we’re interested in"
  },
  {
    "objectID": "slides/lec-5.html#inference-for-simple-linear-regression",
    "href": "slides/lec-5.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation based-inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the interval, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval",
    "href": "slides/lec-5.html#confidence-interval",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval-for-the-slope-1",
    "href": "slides/lec-5.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like “For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take X dollars.”\n\nShould X be $10? $100? $1000?\nIf we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by $10? $100? $1000?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/lec-5.html#quantify-the-variability-of-the-slope",
    "href": "slides/lec-5.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation based-inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\nTwo approaches:\n\nVia simulation (what we’ll do today)\nVia mathematical models (what we’ll do in the next class)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-1",
    "href": "slides/lec-5.html#bootstrap-sample-1",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-2",
    "href": "slides/lec-5.html#bootstrap-sample-2",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-3",
    "href": "slides/lec-5.html#bootstrap-sample-3",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-4",
    "href": "slides/lec-5.html#bootstrap-sample-4",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-5",
    "href": "slides/lec-5.html#bootstrap-sample-5",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth…"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-samples-1---5",
    "href": "slides/lec-5.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-samples-1---100",
    "href": "slides/lec-5.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/lec-5.html#slopes-of-bootstrap-samples",
    "href": "slides/lec-5.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation based-inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take ___ dollars."
  },
  {
    "objectID": "slides/lec-5.html#slopes-of-bootstrap-samples-1",
    "href": "slides/lec-5.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation based-inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take ___ dollars."
  },
  {
    "objectID": "slides/lec-5.html#confidence-level",
    "href": "slides/lec-5.html#confidence-level",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between $0 and $250? How about $150 and $170? How about $90 and $210?"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval-1",
    "href": "slides/lec-5.html#confidence-interval-1",
    "title": "SLR: Simulation based-inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-i",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nobserved_fit &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  fit()\n\nobserved_fit\n\n# A tibble: 2 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\n# #| code-line-numbers: \"1,5,6\"\n\nset.seed(1120)\n\nboot_fits &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\nboot_fits\n\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# … with 190 more rows"
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\n# #| code-line-numbers: \"5\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-iv",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-iv",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope IV",
    "text": "Computing the CI for the slope IV\nStandard error method: Alternatively, compute the 95% CI as the point estimate \\(\\pm\\) ~2 standard deviations of the bootstrap distribution:\n\n# #| code-line-numbers: \"5\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"se\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          90.8     228.\n2 intercept -56788.   290093."
  },
  {
    "objectID": "slides/lec-5.html#precision-vs.-accuracy",
    "href": "slides/lec-5.html#precision-vs.-accuracy",
    "title": "SLR: Simulation based-inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/lec-5.html#precision-vs.-accuracy-1",
    "href": "slides/lec-5.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation based-inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nHow can we get best of both worlds – high precision and high accuracy?"
  },
  {
    "objectID": "slides/lec-5.html#changing-confidence-level",
    "href": "slides/lec-5.html#changing-confidence-level",
    "title": "SLR: Simulation based-inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\n# #| code-line-numbers: \"|4\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/lec-5.html#changing-confidence-level-1",
    "href": "slides/lec-5.html#changing-confidence-level-1",
    "title": "SLR: Simulation based-inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          104.     212.\n2 intercept  -24380.  256730.\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          56.3     226.\n2 intercept -61950.   370395."
  },
  {
    "objectID": "slides/lec-5.html#recap",
    "href": "slides/lec-5.html#recap",
    "title": "SLR: Simulation based-inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can’t continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability"
  },
  {
    "objectID": "slides/lec-5.html#sampling-is-natural",
    "href": "slides/lec-5.html#sampling-is-natural",
    "title": "SLR: Simulation based-inference",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough, that’s exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that’s an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/lec-5.html#statistical-significance",
    "href": "slides/lec-5.html#statistical-significance",
    "title": "SLR: Simulation based-inference",
    "section": "Statistical significance",
    "text": "Statistical significance\n\n\nDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?"
  },
  {
    "objectID": "slides/lec-5.html#hypotheses",
    "href": "slides/lec-5.html#hypotheses",
    "title": "SLR: Simulation based-inference",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nWe want to answer the question “Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-5.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/lec-5.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Simulation based-inference",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-5.html#hypothesis-testing-framework",
    "href": "slides/lec-5.html#hypothesis-testing-framework",
    "title": "SLR: Simulation based-inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e. what we’re testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/lec-22.html#topics",
    "href": "slides/lec-22.html#topics",
    "title": "LR: Inference + conditions",
    "section": "Topics",
    "text": "Topics\n\nBulding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/lec-22.html#computational-setup",
    "href": "slides/lec-22.html#computational-setup",
    "title": "LR: Inference + conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-22.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-22.html#risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/lec-22.html#data-prep",
    "href": "slides/lec-22.html#data-prep",
    "title": "LR: Inference + conditions",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(here::here(\"slides\", \"data/framingham.csv\")) %&gt;%\n  select(age, education, TenYearCHD, totChol, currentSmoker) %&gt;%\n  drop_na() %&gt;%\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n\n# A tibble: 4,086 × 6\n     age education TenYearCHD totChol currentSmoker high_risk\n   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;    \n 1    39 4                  0     195 0             0        \n 2    46 2                  0     250 0             0        \n 3    48 1                  0     245 1             0        \n 4    61 3                  1     225 1             1        \n 5    46 3                  0     285 1             0        \n 6    43 2                  0     228 0             0        \n 7    63 1                  1     205 0             1        \n 8    45 2                  0     313 1             0        \n 9    52 1                  0     260 0             0        \n10    43 1                  0     225 1             0        \n# … with 4,076 more rows"
  },
  {
    "objectID": "slides/lec-22.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/lec-22.html#modeling-risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nFrom age and education:\n\nrisk_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/lec-22.html#model-output",
    "href": "slides/lec-22.html#model-output",
    "title": "LR: Inference + conditions",
    "section": "Model output",
    "text": "Model output\n\ntidy(risk_fit, conf.int = TRUE) %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.385 + 0.073 ~ \\text{age} - 0.242 ~ \\text{ed2} - 0.235 ~ \\text{ed3} - 0.020 ~ \\text{ed4}}\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#hypothesis-test-for-beta_j",
    "href": "slides/lec-22.html#hypothesis-test-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Hypothesis test for \\(\\beta_j\\)",
    "text": "Hypothesis test for \\(\\beta_j\\)\nHypotheses: \\(H_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_j \\neq 0\\)\n\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}\\]\n\n\nP-value: \\(P(|Z| &gt; |z|)\\), where \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/lec-22.html#confidence-interval-for-beta_j",
    "href": "slides/lec-22.html#confidence-interval-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\nWe can calculate the .vocab[C% confidence interval] for \\(\\beta_j\\) as the following:\n\\[\n\\Large{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}\n\\]\nwhere \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution\n\nThis is an interval for the change in the log-odds for every one unit increase in \\(x_j\\)."
  },
  {
    "objectID": "slides/lec-22.html#interpretation-in-terms-of-the-odds",
    "href": "slides/lec-22.html#interpretation-in-terms-of-the-odds",
    "title": "LR: Inference + conditions",
    "section": "Interpretation in terms of the odds",
    "text": "Interpretation in terms of the odds\nThe change in odds for every one unit increase in \\(x_j\\).\n\\[\n\\Large{e^{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}}\n\\]\n\nInterpretation: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), the odds multiply by a factor of \\(e^{\\hat{\\beta}_j - z^* SE_{\\hat{\\beta}_j}}\\) to \\(e^{\\hat{\\beta}_j + z^* SE_{\\hat{\\beta}_j}}\\), holding all else constant."
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age",
    "href": "slides/lec-22.html#coefficient-for-age",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\nHypotheses:\n\\[\nH_0: \\beta_{1} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{1} \\neq 0\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age-1",
    "href": "slides/lec-22.html#coefficient-for-age-1",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\nTest statistic:\n\\[\nz = \\frac{0.0733 - 0}{0.00547} = 13.4\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age-2",
    "href": "slides/lec-22.html#coefficient-for-age-2",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\nP-value:\n\\[\nP(|Z| &gt; |13.4|) \\approx 0\n\\]\n\n\n2 * pnorm(13.4,lower.tail = FALSE)\n\n[1] 6.046315e-41"
  },
  {
    "objectID": "slides/lec-22.html#coefficient-for-age-3",
    "href": "slides/lec-22.html#coefficient-for-age-3",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\nConclusion:\nThe p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that age is a statistically significant predictor of whether someone is high risk of having heart disease, after accounting for education."
  },
  {
    "objectID": "slides/lec-22.html#log-likelihood",
    "href": "slides/lec-22.html#log-likelihood",
    "title": "LR: Inference + conditions",
    "section": "Log likelihood",
    "text": "Log likelihood\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\nDeviance = \\(-2 \\log L\\)\n\n\\(-2 \\log L\\) follows a \\(\\chi^2\\) distribution with \\(n - p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/lec-22.html#comparing-nested-models",
    "href": "slides/lec-22.html#comparing-nested-models",
    "title": "LR: Inference + conditions",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes predictors \\(x_1, \\ldots, x_q\\)\nFull Model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\nTo do so, we will use the Drop-in-deviance test, also known as the Nested Likelihood Ratio test"
  },
  {
    "objectID": "slides/lec-22.html#drop-in-deviance-test",
    "href": "slides/lec-22.html#drop-in-deviance-test",
    "title": "LR: Inference + conditions",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\n\n\nP-value: \\(P(\\chi^2 &gt; G)\\), calculated using a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models"
  },
  {
    "objectID": "slides/lec-22.html#chi2-distribution",
    "href": "slides/lec-22.html#chi2-distribution",
    "title": "LR: Inference + conditions",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/lec-22.html#model-with-age-and-education",
    "href": "slides/lec-22.html#model-with-age-and-education",
    "title": "LR: Inference + conditions",
    "section": "Model with age and education",
    "text": "Model with age and education\n\nShould we add currentSmoker to this model?\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264"
  },
  {
    "objectID": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model",
    "href": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nFirst model, reduced:\n\nrisk_fit_reduced &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\n\nSecond model, full:\n\nrisk_fit_full &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-1",
    "href": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-1",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate deviance for each model:\n\n(dev_reduced &lt;- glance(risk_fit_reduced)$deviance)\n\n[1] 3244.187\n\n(dev_full &lt;- glance(risk_fit_full)$deviance)\n\n[1] 3221.901\n\n\n\nDrop-in-deviance test statistic:\n\n(test_stat &lt;- dev_reduced - dev_full)\n\n[1] 22.2863"
  },
  {
    "objectID": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-2",
    "href": "slides/lec-22.html#should-we-add-currentsmoker-to-the-model-2",
    "title": "LR: Inference + conditions",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate the p-value using a pchisq(), with degrees of freedom equal to the number of new model terms in the second model:\n\npchisq(test_stat, 1, lower.tail = FALSE) \n\n[1] 2.348761e-06\n\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that the coefficient of currentSmoker is not equal to 0. Therefore, we should add it to the model."
  },
  {
    "objectID": "slides/lec-22.html#drop-in-deviance-test-in-r",
    "href": "slides/lec-22.html#drop-in-deviance-test-in-r",
    "title": "LR: Inference + conditions",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced$fit, risk_fit_full$fit, test = \"Chisq\") %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  Resid..Df Resid..Dev    df Deviance     p.value\n      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1      4081      3244.    NA     NA   NA         \n2      4080      3222.     1     22.3  0.00000235"
  },
  {
    "objectID": "slides/lec-22.html#model-selection",
    "href": "slides/lec-22.html#model-selection",
    "title": "LR: Inference + conditions",
    "section": "Model selection",
    "text": "Model selection\nUse AIC or BIC for model selection\n\\[\n\\begin{align}\n&AIC = - 2 * \\log L - \\color{purple}{n\\log(n)}+ 2(p +1)\\\\[5pt]\n&BIC =- 2 * \\log L - \\color{purple}{n\\log(n)} + log(n)\\times(p+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#aic-from-the-glance-function",
    "href": "slides/lec-22.html#aic-from-the-glance-function",
    "title": "LR: Inference + conditions",
    "section": "AIC from the glance() function",
    "text": "AIC from the glance() function\nLet’s look at the AIC for the model that includes age, education, and currentSmoker\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nCalculating AIC\n\n- 2 * glance(risk_fit_full)$logLik + 2 * (5 + 1)\n\n[1] 3233.901"
  },
  {
    "objectID": "slides/lec-22.html#comparing-the-models-using-aic",
    "href": "slides/lec-22.html#comparing-the-models-using-aic",
    "title": "LR: Inference + conditions",
    "section": "Comparing the models using AIC",
    "text": "Comparing the models using AIC\nLet’s compare the full and reduced models using AIC.\n\nglance(risk_fit_reduced)$AIC\n\n[1] 3254.187\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nBased on AIC, which model would you choose?"
  },
  {
    "objectID": "slides/lec-22.html#comparing-the-models-using-bic",
    "href": "slides/lec-22.html#comparing-the-models-using-bic",
    "title": "LR: Inference + conditions",
    "section": "Comparing the models using BIC",
    "text": "Comparing the models using BIC\nLet’s compare the full and reduced models using BIC\n\nglance(risk_fit_reduced)$BIC\n\n[1] 3285.764\n\nglance(risk_fit_full)$BIC\n\n[1] 3271.793\n\n\n\nBased on BIC, which model would you choose?"
  },
  {
    "objectID": "slides/lec-22.html#the-model",
    "href": "slides/lec-22.html#the-model",
    "title": "LR: Inference + conditions",
    "section": "The model",
    "text": "The model\nLet’s predict high_risk from age, total cholesterol, and whether the patient is a current smoker:\n\nrisk_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ age + totChol + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) %&gt;% \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-6.673\n0.378\n-17.647\n0.000\n-7.423\n-5.940\n\n\nage\n0.082\n0.006\n14.344\n0.000\n0.071\n0.094\n\n\ntotChol\n0.002\n0.001\n1.940\n0.052\n0.000\n0.004\n\n\ncurrentSmoker1\n0.443\n0.094\n4.733\n0.000\n0.260\n0.627"
  },
  {
    "objectID": "slides/lec-22.html#conditions-for-logistic-regression",
    "href": "slides/lec-22.html#conditions-for-logistic-regression",
    "title": "LR: Inference + conditions",
    "section": "Conditions for logistic regression",
    "text": "Conditions for logistic regression\n\nLinearity: The log-odds have a linear relationship with the predictors.\nRandomness: The data were obtained from a random process\nIndependence: The observations are independent from one another."
  },
  {
    "objectID": "slides/lec-22.html#empirical-logit",
    "href": "slides/lec-22.html#empirical-logit",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit",
    "text": "Empirical logit\nThe empirical logit is the log of the observed odds:\n\\[\n\\text{logit}(\\hat{p}) = \\log\\Big(\\frac{\\hat{p}}{1 - \\hat{p}}\\Big) = \\log\\Big(\\frac{\\# \\text{Yes}}{\\# \\text{No}}\\Big)\n\\]"
  },
  {
    "objectID": "slides/lec-22.html#calculating-empirical-logit-categorical-predictor",
    "href": "slides/lec-22.html#calculating-empirical-logit-categorical-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (categorical predictor)",
    "text": "Calculating empirical logit (categorical predictor)\nIf the predictor is categorical, we can calculate the empirical logit for each level of the predictor.\n\nheart_disease %&gt;%\n  count(currentSmoker, high_risk) %&gt;%\n  group_by(currentSmoker) %&gt;%\n  mutate(prop = n/sum(n)) %&gt;%\n  filter(high_risk == \"1\") %&gt;%\n  mutate(emp_logit = log(prop/(1-prop)))\n\n# A tibble: 2 × 5\n# Groups:   currentSmoker [2]\n  currentSmoker high_risk     n  prop emp_logit\n  &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 0             1           301 0.145     -1.77\n2 1             1           318 0.158     -1.67"
  },
  {
    "objectID": "slides/lec-22.html#calculating-empirical-logit-quantitative-predictor",
    "href": "slides/lec-22.html#calculating-empirical-logit-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (quantitative predictor)",
    "text": "Calculating empirical logit (quantitative predictor)\n\nDivide the range of the predictor into intervals with approximately equal number of cases. (If you have enough observations, use 5 - 10 intervals.)\nCalculate the mean value of the predictor in each interval.\nCompute the empirical logit for each interval.\n\n\nThen, create a plot of the empirical logit versus the mean value of the predictor in each interval."
  },
  {
    "objectID": "slides/lec-22.html#empirical-logit-plot-in-r-quantitative-predictor",
    "href": "slides/lec-22.html#empirical-logit-plot-in-r-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)"
  },
  {
    "objectID": "slides/lec-22.html#empirical-logit-plot-in-r-interactions",
    "href": "slides/lec-22.html#empirical-logit-plot-in-r-interactions",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (interactions)",
    "text": "Empirical logit plot in R (interactions)\n\nemplogitplot2(high_risk ~ age + currentSmoker, data = heart_disease, \n              ngroups = 10, \n              putlegend = \"bottomright\")"
  },
  {
    "objectID": "slides/lec-22.html#checking-linearity",
    "href": "slides/lec-22.html#checking-linearity",
    "title": "LR: Inference + conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\n\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\nemplogitplot1(high_risk ~ totChol, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n✅ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the predictor variables."
  },
  {
    "objectID": "slides/lec-22.html#checking-randomness",
    "href": "slides/lec-22.html#checking-randomness",
    "title": "LR: Inference + conditions",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n✅ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S. in regards to health characteristics and risk of heart disease."
  },
  {
    "objectID": "slides/lec-22.html#checking-independence",
    "href": "slides/lec-22.html#checking-independence",
    "title": "LR: Inference + conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n\n✅ The independence condition is satisfied. It is reasonable to conclude that the participants’ health characteristics are independent of one another."
  },
  {
    "objectID": "slides/lab-0.html#meet-your-ta",
    "href": "slides/lab-0.html#meet-your-ta",
    "title": "Lab 0 - Meet + greet",
    "section": "Meet your TA!",
    "text": "Meet your TA!"
  },
  {
    "objectID": "slides/lab-0.html#meet-each-other",
    "href": "slides/lab-0.html#meet-each-other",
    "title": "Lab 0 - Meet + greet",
    "section": "Meet each other!",
    "text": "Meet each other!\nIn breakout rooms:\n\nName, year, major, hometown\nWhat did you do over the winter break?\nWhat do you hope to get out of this course?\nAnything else you want to share/ask?\n\n\n\n\n08:00"
  },
  {
    "objectID": "slides/lab-0.html#take-the-survey",
    "href": "slides/lab-0.html#take-the-survey",
    "title": "Lab 0 - Meet + greet",
    "section": "Take the survey",
    "text": "Take the survey\n\nGo to the course website:\nhttps://sta210-s22.github.io/website\nClick on the instructions for Lab 0 on the course schedule\nTake the Getting to know you survey"
  },
  {
    "objectID": "slides/lab-0.html#what-to-expect-in-lab",
    "href": "slides/lab-0.html#what-to-expect-in-lab",
    "title": "Lab 0 - Meet + greet",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nWork on the lab assignment (individual at first, but in teams for the rest of the semester)\nLab instructions posted on the course website.\nStart each lab by finding your assignment repo in the course GitHub organization\nMore on all of these tools in lecture tomorrow and in lab next week"
  },
  {
    "objectID": "slides/lab-0.html#wrap-up",
    "href": "slides/lab-0.html#wrap-up",
    "title": "Lab 0 - Meet + greet",
    "section": "Wrap up",
    "text": "Wrap up\nAny questions about the course?"
  },
  {
    "objectID": "slides/lec-27.html#exam-instructions",
    "href": "slides/lec-27.html#exam-instructions",
    "title": "Exam 3 review",
    "section": "Exam instructions",
    "text": "Exam instructions\n\nThe exam is an individual assignment. Everything in your repository is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor. For example, you may not communicate with other students, the TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nNo TA office hours will be held during the exam. You may not email the TAs questions about the exam.\nIf you have questions, email or Slack me."
  },
  {
    "objectID": "slides/lec-27.html#exam-coverage-and-format",
    "href": "slides/lec-27.html#exam-coverage-and-format",
    "title": "Exam 3 review",
    "section": "Exam coverage and format",
    "text": "Exam coverage and format\n\nFocuses on content Weeks 09 - 14, but can include material from previous weeks\n\nSimilar format as previous exams\n\nPart 1: Multiple choice/fill-in-the-blank questions on Sakai\nPart 2: Open-ended data analysis in GitHub and submitted on Gradescope"
  },
  {
    "objectID": "slides/lec-27.html#part-2-of-the-exam",
    "href": "slides/lec-27.html#part-2-of-the-exam",
    "title": "Exam 3 review",
    "section": "Part 2 of the exam",
    "text": "Part 2 of the exam\n\nGoal: Assess your understanding of the course material and how the methods you learned are applied to the analysis of real-world data.\n\nInclude all of your analysis steps in your exam write up, unless stated otherwise.\n\nFor example, if the exam says “assume conditions are met,” You can reference that information in your write up but don’t have to recheck the conditions."
  },
  {
    "objectID": "slides/lec-27.html#assessment-criteria",
    "href": "slides/lec-27.html#assessment-criteria",
    "title": "Exam 3 review",
    "section": "Assessment criteria",
    "text": "Assessment criteria\n\nYou can identify the correct approach, analysis method, and/or inferential results required to answer the question.\nYou understand the correct conditions and diagnostics needed to determine whether the conclusions drawn from the model will be reliable\nYou can write results and conclusions in a meaningful way that can be understood by a general audience (think a business or research partner)\nYou can produce a report that is suitable for a professional audience (e.g., narrative is written in complete sentences, all graphs have proper titles and axis labels, there is not extraneous output, all Latex is rendered)\nYou can conduct the analysis using a reproducible data analysis workflow that incorporates version control"
  },
  {
    "objectID": "slides/lec-27.html#application-exercise",
    "href": "slides/lec-27.html#application-exercise",
    "title": "Exam 3 review",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-s22/ae-12-exam-3-review"
  },
  {
    "objectID": "slides/lec-3.html#announcements",
    "href": "slides/lec-3.html#announcements",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Announcements",
    "text": "Announcements\n\nIf you’re just joining the class, welcome! Go to the course website and review content you’ve missed, read the syllabus, and complete the Getting to know you survey.\nLab 1 is due Friday, at 5pm, on Gradescope."
  },
  {
    "objectID": "slides/lec-3.html#recap-of-last-lecture",
    "href": "slides/lec-3.html#recap-of-last-lecture",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/lec-3.html#interested-in-the-math-behind-it-all",
    "href": "slides/lec-3.html#interested-in-the-math-behind-it-all",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Interested in the math behind it all?",
    "text": "Interested in the math behind it all?\nSee the supplemental notes on Deriving the Least-Squares Estimates for Simple Linear Regression for more mathematical details on the derivations of the estimates of \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "slides/lec-3.html#outline",
    "href": "slides/lec-3.html#outline",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Outline",
    "text": "Outline\n\nUse tidymodels to fit and summarize regression models in R\nComplete an application exercise on exploratory data analysis and modeling"
  },
  {
    "objectID": "slides/lec-3.html#computational-setup",
    "href": "slides/lec-3.html#computational-setup",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/lec-3.html#movie-ratings",
    "href": "slides/lec-3.html#movie-ratings",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/lec-3.html#data-prep",
    "href": "slides/lec-3.html#data-prep",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango %&gt;%\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/lec-3.html#data-visualization",
    "href": "slides/lec-3.html#data-visualization",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/lec-3.html#step-1-specify-model",
    "href": "slides/lec-3.html#step-1-specify-model",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-3.html#step-2-set-model-fitting-engine",
    "href": "slides/lec-3.html#step-2-set-model-fitting-engine",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\n# #| code-line-numbers: \"|2\"\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/lec-3.html#step-3-fit-model-estimate-parameters",
    "href": "slides/lec-3.html#step-3-fit-model-estimate-parameters",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\nusing formula syntax\n\n# #| code-line-numbers: \"|3\"\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(audience ~ critics, data = movie_scores)\n\nparsnip model object\n\nFit time:  4ms \n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187"
  },
  {
    "objectID": "slides/lec-3.html#a-closer-look-at-model-output",
    "href": "slides/lec-3.html#a-closer-look-at-model-output",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\nmovie_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\nFit time:  2ms \n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is likely just rounding error in the hand calculation."
  },
  {
    "objectID": "slides/lec-3.html#the-regression-output",
    "href": "slides/lec-3.html#the-regression-output",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "The regression output",
    "text": "The regression output\nWe’ll focus on the first column for now…\n\n# #| code-line-numbers: \"|4\"\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(audience ~ critics, data = movie_scores) %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/lec-3.html#prediction",
    "href": "slides/lec-3.html#prediction",
    "title": "SLR: Model fitting in R with tidymodels",
    "section": "Prediction",
    "text": "Prediction\n\n# #| code-line-numbers: \"|2|5\"\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 50)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  58.2"
  },
  {
    "objectID": "slides/lec-19.html#announcements",
    "href": "slides/lec-19.html#announcements",
    "title": "Probabilities, odds, odds ratios",
    "section": "Announcements",
    "text": "Announcements\n\nExam 2 scores are posted, please review the feedback asap\nProject proposals due tomorrow (Friday), at 5pm"
  },
  {
    "objectID": "slides/lec-19.html#topics",
    "href": "slides/lec-19.html#topics",
    "title": "Probabilities, odds, odds ratios",
    "section": "Topics",
    "text": "Topics\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/lec-19.html#computational-setup",
    "href": "slides/lec-19.html#computational-setup",
    "title": "Probabilities, odds, odds ratios",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-19.html#risk-of-coronary-heart-disease",
    "href": "slides/lec-19.html#risk-of-coronary-heart-disease",
    "title": "Probabilities, odds, odds ratios",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College"
  },
  {
    "objectID": "slides/lec-19.html#high-risk-vs.-education",
    "href": "slides/lec-19.html#high-risk-vs.-education",
    "title": "Probabilities, odds, odds ratios",
    "section": "High risk vs. education",
    "text": "High risk vs. education\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#compare-the-odds-for-two-groups",
    "href": "slides/lec-19.html#compare-the-odds-for-two-groups",
    "title": "Probabilities, odds, odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nWe want to compare the risk of heart disease for those with a High School diploma/GED and those with a college degree.\nWe’ll use the odds to compare the two groups\n\n\\[\n\\text{odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{\\text{# of successes}}{\\text{# of failures}}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#compare-the-odds-for-two-groups-1",
    "href": "slides/lec-19.html#compare-the-odds-for-two-groups-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nOdds of having high risk for the High school or GED group: \\(\\frac{147}{1106} = 0.133\\)\nOdds of having high risk for the College group: \\(\\frac{70}{403} = 0.174\\)\nBased on this, we see those with a college degree had higher odds of having high risk for heart disease than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/lec-19.html#odds-ratio-or",
    "href": "slides/lec-19.html#odds-ratio-or",
    "title": "Probabilities, odds, odds ratios",
    "section": "Odds ratio (OR)",
    "text": "Odds ratio (OR)\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\nLet’s summarize the relationship between the two groups. To do so, we’ll use the odds ratio (OR).\n\\[\nOR = \\frac{\\text{odds}_1}{\\text{odds}_2} = \\frac{\\omega_1}{\\omega_2}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#or-college-vs.-high-school-or-ged",
    "href": "slides/lec-19.html#or-college-vs.-high-school-or-ged",
    "title": "Probabilities, odds, odds ratios",
    "section": "OR: College vs. High school or GED",
    "text": "OR: College vs. High school or GED\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{HS}} = \\frac{0.174}{0.133} = \\mathbf{1.308}\\]\n\nThe odds of having high risk for heart disease are 1.30 times higher for those with a college degree than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/lec-19.html#or-college-vs.-some-high-school",
    "href": "slides/lec-19.html#or-college-vs.-some-high-school",
    "title": "Probabilities, odds, odds ratios",
    "section": "OR: College vs. Some high school",
    "text": "OR: College vs. Some high school\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{Some HS}} = \\frac{70/403}{323/1397} = 0.751\\]\n\nThe odds of having high risk for having heart disease for those with a college degree are 0.751 times the odds of having high risk for heart disease for those with some high school."
  },
  {
    "objectID": "slides/lec-19.html#more-natural-interpretation",
    "href": "slides/lec-19.html#more-natural-interpretation",
    "title": "Probabilities, odds, odds ratios",
    "section": "More natural interpretation",
    "text": "More natural interpretation\n\nIt’s more natural to interpret the odds ratio with a statement with the odds ratio greater than 1.\nThe odds of having high risk for heart disease are 1.33 times higher for those with some high school than those with a college degree."
  },
  {
    "objectID": "slides/lec-19.html#making-the-table-1",
    "href": "slides/lec-19.html#making-the-table-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Making the table 1",
    "text": "Making the table 1\nFirst, rename the levels of the categorical variables:\n\nheart_disease &lt;- heart_disease %&gt;%\n  mutate(\n    high_risk_names = if_else(high_risk == \"1\", \"High risk\", \"Not high risk\"),\n    education_names = case_when(\n      education == \"1\" ~ \"Some high school\",\n      education == \"2\" ~ \"High school or GED\",\n      education == \"3\" ~ \"Some college or vocational school\",\n      education == \"4\" ~ \"College\"\n    ),\n    education_names = fct_relevel(education_names, \"Some high school\", \"High school or GED\", \"Some college or vocational school\", \"College\")\n  )"
  },
  {
    "objectID": "slides/lec-19.html#making-the-table-2",
    "href": "slides/lec-19.html#making-the-table-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Making the table 2",
    "text": "Making the table 2\nThen, make the table:\n\nheart_disease %&gt;%\n  count(education_names, high_risk_names) %&gt;%\n  pivot_wider(names_from = high_risk_names, values_from = n) %&gt;%\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code",
    "href": "slides/lec-19.html#deeper-look-into-the-code",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %&gt;%\n  count(education_names, high_risk_names)\n\n# A tibble: 8 × 3\n  education_names                   high_risk_names     n\n  &lt;fct&gt;                             &lt;chr&gt;           &lt;int&gt;\n1 Some high school                  High risk         323\n2 Some high school                  Not high risk    1397\n3 High school or GED                High risk         147\n4 High school or GED                Not high risk    1106\n5 Some college or vocational school High risk          88\n6 Some college or vocational school Not high risk     601\n7 College                           High risk          70\n8 College                           Not high risk     403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-1",
    "href": "slides/lec-19.html#deeper-look-into-the-code-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %&gt;%\n  count(education_names, high_risk_names) %&gt;%\n  pivot_wider(names_from = high_risk_names, values_from = n)\n\n# A tibble: 4 × 3\n  education_names                   `High risk` `Not high risk`\n  &lt;fct&gt;                                   &lt;int&gt;           &lt;int&gt;\n1 Some high school                          323            1397\n2 High school or GED                        147            1106\n3 Some college or vocational school          88             601\n4 College                                    70             403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-2",
    "href": "slides/lec-19.html#deeper-look-into-the-code-2",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %&gt;%\n  count(education_names, high_risk_names) %&gt;%\n  pivot_wider(names_from = high_risk_names, values_from = n) %&gt;%\n  kable()\n\n\n\n\neducation_names\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#deeper-look-into-the-code-3",
    "href": "slides/lec-19.html#deeper-look-into-the-code-3",
    "title": "Probabilities, odds, odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease %&gt;%\n  count(education_names, high_risk_names) %&gt;%\n  pivot_wider(names_from = high_risk_names, values_from = n) %&gt;%\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/lec-19.html#categorical-predictor",
    "href": "slides/lec-19.html#categorical-predictor",
    "title": "Probabilities, odds, odds ratios",
    "section": "Categorical predictor",
    "text": "Categorical predictor\nRecall: Education - 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\nheart_edu_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ education, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046"
  },
  {
    "objectID": "slides/lec-19.html#interpreting-education4---log-odds",
    "href": "slides/lec-19.html#interpreting-education4---log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting education4 - log-odds",
    "text": "Interpreting education4 - log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe log-odds of having high risk for heart disease are expected to be 0.286 less for those with a college degree compared to those with some high school (the baseline group)."
  },
  {
    "objectID": "slides/lec-19.html#interpreting-education4---odds",
    "href": "slides/lec-19.html#interpreting-education4---odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting education4 - odds",
    "text": "Interpreting education4 - odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe odds of having high risk for heart disease for those with a college degree are expected to be 0.751 (exp(-0.286)) times the odds for those with some high school."
  },
  {
    "objectID": "slides/lec-19.html#coefficients-odds-ratios",
    "href": "slides/lec-19.html#coefficients-odds-ratios",
    "title": "Probabilities, odds, odds ratios",
    "section": "Coefficients + odds ratios",
    "text": "Coefficients + odds ratios\nThe model coefficient, -0.286, is the expected change in the log-odds when going from the Some high school group to the College group.\n\nTherefore, \\(e^{-0.286}\\) = 0.751 is the expected change in the odds when going from the Some high school group to the College group.\n\n\n\\[\nOR  = e^{\\hat{\\beta}_j} = \\exp\\{\\hat{\\beta}_j\\}\n\\]"
  },
  {
    "objectID": "slides/lec-19.html#quantitative-predictor",
    "href": "slides/lec-19.html#quantitative-predictor",
    "title": "Probabilities, odds, odds ratios",
    "section": "Quantitative predictor",
    "text": "Quantitative predictor\n\nheart_age_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_age_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0"
  },
  {
    "objectID": "slides/lec-19.html#interpreting-age-log-odds",
    "href": "slides/lec-19.html#interpreting-age-log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting age: log-odds",
    "text": "Interpreting age: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\nFor each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.076."
  },
  {
    "objectID": "slides/lec-19.html#interpreting-age-odds",
    "href": "slides/lec-19.html#interpreting-age-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpreting age: odds",
    "text": "Interpreting age: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\n\nFor each additional year in age, the odds of having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.076)).\nAlternate interpretation: For each additional year in age, the odds of having high risk for heart disease are expected to increase by 8%."
  },
  {
    "objectID": "slides/lec-19.html#multiple-predictors",
    "href": "slides/lec-19.html#multiple-predictors",
    "title": "Probabilities, odds, odds ratios",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nheart_edu_age_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ education + age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_age_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000"
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-log-odds",
    "href": "slides/lec-19.html#interpretation-in-terms-of-log-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The log-odds of having high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, holding age constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-log-odds-1",
    "href": "slides/lec-19.html#interpretation-in-terms-of-log-odds-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the log-odds of having high risk for heart disease are expected to increase by 0.073, holding education level constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-odds",
    "href": "slides/lec-19.html#interpretation-in-terms-of-odds",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The odds of having high risk for heart disease for those with a college degree are expected to be 0.98 (exp(-0.020)) times the odds for those with some high school, holding age constant."
  },
  {
    "objectID": "slides/lec-19.html#interpretation-in-terms-of-odds-1",
    "href": "slides/lec-19.html#interpretation-in-terms-of-odds-1",
    "title": "Probabilities, odds, odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the odds having high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.073)), holding education level constant."
  },
  {
    "objectID": "slides/lec-19.html#recap",
    "href": "slides/lec-19.html#recap",
    "title": "Probabilities, odds, odds ratios",
    "section": "Recap",
    "text": "Recap\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/lec-24.html#topics",
    "href": "slides/lec-24.html#topics",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Topics",
    "text": "Topics\n\nPredictions\nModel selection\nChecking conditions"
  },
  {
    "objectID": "slides/lec-24.html#computational-setup",
    "href": "slides/lec-24.html#computational-setup",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(colorblindr)\nlibrary(pROC)\nlibrary(Stat2Data)\nlibrary(nnet)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/lec-24.html#nhanes-data",
    "href": "slides/lec-24.html#nhanes-data",
    "title": "MultiLR: Prediction + inferential models",
    "section": "NHANES Data",
    "text": "NHANES Data\n\nNational Health and Nutrition Examination Survey is conducted by the National Center for Health Statistics (NCHS).\nThe goal is to “assess the health and nutritional status of adults and children in the United States”.\nThis survey includes an interview and a physical examination."
  },
  {
    "objectID": "slides/lec-24.html#variables",
    "href": "slides/lec-24.html#variables",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Variables",
    "text": "Variables\nGoal: Use a person’s age and whether they do regular physical activity to predict their self-reported health rating.\n\nOutcome: HealthGen: Self-reported rating of participant’s health in general. Excellent, Vgood, Good, Fair, or Poor.\nPredictors:\n\nAge: Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities."
  },
  {
    "objectID": "slides/lec-24.html#the-data",
    "href": "slides/lec-24.html#the-data",
    "title": "MultiLR: Prediction + inferential models",
    "section": "The data",
    "text": "The data\n\nnhanes_adult &lt;- NHANES %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  select(HealthGen, Age, PhysActive, Education) %&gt;%\n  drop_na() %&gt;%\n  mutate(obs_num = 1:n())\n\n\nglimpse(nhanes_adult)\n\nRows: 6,465\nColumns: 5\n$ HealthGen  &lt;fct&gt; Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, …\n$ Age        &lt;int&gt; 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,…\n$ PhysActive &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, …\n$ Education  &lt;fct&gt; High School, High School, High School, Some College, Colleg…\n$ obs_num    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …"
  },
  {
    "objectID": "slides/lec-24.html#model-in-r",
    "href": "slides/lec-24.html#model-in-r",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model in R",
    "text": "Model in R\n\nhealth_fit &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)\n\nhealth_fit &lt;- repair_call(health_fit, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-24.html#model-summary",
    "href": "slides/lec-24.html#model-summary",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model summary",
    "text": "Model summary\n\ntidy(health_fit) %&gt;% print(n = 12)\n\n# A tibble: 12 × 6\n   y.level term            estimate std.error statistic  p.value\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Vgood   (Intercept)    1.27        0.154      8.23   1.80e-16\n 2 Vgood   Age           -0.0000361   0.00259   -0.0139 9.89e- 1\n 3 Vgood   PhysActiveYes -0.332       0.0949    -3.50   4.72e- 4\n 4 Good    (Intercept)    1.99        0.150     13.3    2.81e-40\n 5 Good    Age           -0.00304     0.00256   -1.19   2.35e- 1\n 6 Good    PhysActiveYes -1.01        0.0921   -11.0    4.80e-28\n 7 Fair    (Intercept)    1.03        0.174      5.94   2.89e- 9\n 8 Fair    Age            0.00113     0.00302    0.373  7.09e- 1\n 9 Fair    PhysActiveYes -1.66        0.109    -15.2    4.14e-52\n10 Poor    (Intercept)   -1.34        0.299     -4.47   7.65e- 6\n11 Poor    Age            0.0193      0.00505    3.83   1.30e- 4\n12 Poor    PhysActiveYes -2.67        0.236    -11.3    1.20e-29"
  },
  {
    "objectID": "slides/lec-24.html#calculating-probabilities",
    "href": "slides/lec-24.html#calculating-probabilities",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\n\nFor categories \\(2,\\ldots,K\\), the probability that the \\(i^{th}\\) observation is in the \\(j^{th}\\) category is\n\\[\n\\hat{\\pi}_{ij} = \\frac{e^{\\hat{\\beta}_{0j} + \\hat{\\beta}_{1j}x_{i1} + \\dots + \\hat{\\beta}_{pj}x_{ip}}}{1 + \\sum\\limits_{k=2}^K e^{\\hat{\\beta}_{0k} + \\hat{\\beta}_{1k}x_{i1} + \\dots \\hat{\\beta}_{pk}x_{ip}}}\n\\]\nFor the baseline category, \\(k=1\\), we calculate the probability \\(\\hat{\\pi}_{i1}\\) as\n\\[\n\\hat{\\pi}_{i1} = 1- \\sum\\limits_{k=2}^K \\hat{\\pi}_{ik}\n\\]"
  },
  {
    "objectID": "slides/lec-24.html#predicted-health-rating",
    "href": "slides/lec-24.html#predicted-health-rating",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Predicted health rating",
    "text": "Predicted health rating\nWe can use our model to predict a person’s perceived health rating given their age and whether they exercise.\n\nhealth_aug &lt;- augment(health_fit, new_data = nhanes_adult)\nhealth_aug\n\n# A tibble: 6,465 × 11\n   HealthGen   Age PhysActive Education      obs_num .pred_class .pred_Excellent\n   &lt;fct&gt;     &lt;int&gt; &lt;fct&gt;      &lt;fct&gt;            &lt;int&gt; &lt;fct&gt;                 &lt;dbl&gt;\n 1 Good         34 No         High School          1 Good                 0.0687\n 2 Good         34 No         High School          2 Good                 0.0687\n 3 Good         34 No         High School          3 Good                 0.0687\n 4 Good         49 No         Some College         4 Good                 0.0691\n 5 Vgood        45 Yes        College Grad         5 Vgood                0.155 \n 6 Vgood        45 Yes        College Grad         6 Vgood                0.155 \n 7 Vgood        45 Yes        College Grad         7 Vgood                0.155 \n 8 Vgood        66 Yes        Some College         8 Vgood                0.157 \n 9 Vgood        58 Yes        College Grad         9 Vgood                0.156 \n10 Fair         54 Yes        9 - 11th Grade      10 Vgood                0.156 \n# … with 6,455 more rows, and 4 more variables: .pred_Vgood &lt;dbl&gt;,\n#   .pred_Good &lt;dbl&gt;, .pred_Fair &lt;dbl&gt;, .pred_Poor &lt;dbl&gt;"
  },
  {
    "objectID": "slides/lec-24.html#actual-vs.-predicted-health-rating",
    "href": "slides/lec-24.html#actual-vs.-predicted-health-rating",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Actual vs. predicted health rating",
    "text": "Actual vs. predicted health rating\nFor each observation, the predicted perceived health rating is the category with the highest predicted probability.\n\nhealth_aug %&gt;% select(contains(\"pred\"))\n\n# A tibble: 6,465 × 6\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   &lt;fct&gt;                 &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# … with 6,455 more rows"
  },
  {
    "objectID": "slides/lec-24.html#confusion-matrix",
    "href": "slides/lec-24.html#confusion-matrix",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nhealth_conf &lt;- health_aug %&gt;% \n  count(HealthGen, .pred_class, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = .pred_class, values_from = n)\n\nhealth_conf\n\n# A tibble: 5 × 6\n  HealthGen Excellent Vgood  Good  Fair  Poor\n  &lt;fct&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 Excellent         0   528   210     0     0\n2 Vgood             0  1341   743     0     0\n3 Good              0  1226  1316     0     0\n4 Fair              0   296   625     0     0\n5 Poor              0    24   156     0     0"
  },
  {
    "objectID": "slides/lec-24.html#actual-vs.-predicted-health-rating-1",
    "href": "slides/lec-24.html#actual-vs.-predicted-health-rating-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Actual vs. predicted health rating",
    "text": "Actual vs. predicted health rating\n\nWhy do you think no observations were predicted to have a rating of “Excellent”, “Fair”, or “Poor”?"
  },
  {
    "objectID": "slides/lec-24.html#comparing-nested-models",
    "href": "slides/lec-24.html#comparing-nested-models",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the following hypotheses:\n\n\\(H_0: \\beta_{q+1} = \\dots = \\beta_p = 0\\)\n\\(H_A: \\text{ at least 1 }\\beta_j \\text{ is not } 0\\)\n\nTo do so, we will use the drop-in-deviance test (very similar to logistic regression)"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model",
    "href": "slides/lec-24.html#add-education-to-the-model",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nWe consider adding the participants’ Education level to the model.\n\nEducation takes values 8thGrade, 9-11thGrade, HighSchool, SomeCollege, and CollegeGrad\n\nModels we’re testing:\n\nReduced model: Age, PhysActive\nFull model: Age, PhysActive, Education\n\n\n\n\\[\n\\begin{align}\n&H_0: \\beta_{9-11thGrade} = \\beta_{HighSchool} = \\beta_{SomeCollege} = \\beta_{CollegeGrad} = 0\\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to }0\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model-1",
    "href": "slides/lec-24.html#add-education-to-the-model-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nreduced_fit &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  fit(HealthGen ~ Age + PhysActive,\n  data = nhanes_adult)\n\nfull_fit &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  fit(HealthGen ~ Age + PhysActive + Education,\n  data = nhanes_adult)\n  \nreduced_fit &lt;- repair_call(reduced_fit, data = nhanes_adult)\nfull_fit &lt;- repair_call(full_fit, data = nhanes_adult)"
  },
  {
    "objectID": "slides/lec-24.html#add-education-to-the-model-2",
    "href": "slides/lec-24.html#add-education-to-the-model-2",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nanova(reduced_fit$fit, full_fit$fit, test = \"Chisq\") %&gt;%\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nResid. df\nResid. Dev\nTest\nDf\nLR stat.\nPr(Chi)\n\n\n\n\nAge + PhysActive\n25848\n16994.23\n\nNA\nNA\nNA\n\n\nAge + PhysActive + Education\n25832\n16505.10\n1 vs 2\n16\n489.132\n0\n\n\n\n\n\n\nAt least one coefficient associated with Education is non-zero. Therefore, we will include Education in the model."
  },
  {
    "objectID": "slides/lec-24.html#model-with-education",
    "href": "slides/lec-24.html#model-with-education",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Model with Education",
    "text": "Model with Education\n\ntidy(full_fit, conf.int = T) %&gt;% print(n = 28)\n\n# A tibble: 28 × 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Vgood   (Intercept)   5.82e-1   0.301      1.93   5.36e- 2 -0.00914   1.17   \n 2 Vgood   Age           1.12e-3   0.00266    0.419  6.75e- 1 -0.00411   0.00634\n 3 Vgood   PhysActiveY… -2.64e-1   0.0985    -2.68   7.33e- 3 -0.457    -0.0711 \n 4 Vgood   Education9 …  7.68e-1   0.308      2.49   1.27e- 2  0.164     1.37   \n 5 Vgood   EducationHi…  7.01e-1   0.280      2.51   1.21e- 2  0.153     1.25   \n 6 Vgood   EducationSo…  7.88e-1   0.271      2.90   3.71e- 3  0.256     1.32   \n 7 Vgood   EducationCo…  4.08e-1   0.268      1.52   1.28e- 1 -0.117     0.933  \n 8 Good    (Intercept)   2.04e+0   0.272      7.51   5.77e-14  1.51      2.57   \n 9 Good    Age          -1.72e-3   0.00263   -0.651  5.15e- 1 -0.00688   0.00345\n10 Good    PhysActiveY… -7.58e-1   0.0961    -7.88   3.16e-15 -0.946    -0.569  \n11 Good    Education9 …  3.60e-1   0.275      1.31   1.90e- 1 -0.179     0.899  \n12 Good    EducationHi…  8.52e-2   0.247      0.345  7.30e- 1 -0.399     0.569  \n13 Good    EducationSo… -1.13e-2   0.239     -0.0472 9.62e- 1 -0.480     0.457  \n14 Good    EducationCo… -8.91e-1   0.236     -3.77   1.65e- 4 -1.35     -0.427  \n15 Fair    (Intercept)   2.12e+0   0.288      7.35   1.91e-13  1.55      2.68   \n16 Fair    Age           3.35e-4   0.00312    0.107  9.14e- 1 -0.00578   0.00645\n17 Fair    PhysActiveY… -1.19e+0   0.115    -10.4    3.50e-25 -1.42     -0.966  \n18 Fair    Education9 … -2.24e-1   0.279     -0.802  4.22e- 1 -0.771     0.323  \n19 Fair    EducationHi… -8.32e-1   0.252     -3.31   9.44e- 4 -1.33     -0.339  \n20 Fair    EducationSo… -1.34e+0   0.246     -5.46   4.71e- 8 -1.82     -0.861  \n21 Fair    EducationCo… -2.51e+0   0.253     -9.91   3.67e-23 -3.00     -2.01   \n22 Poor    (Intercept)  -2.00e-1   0.411     -0.488  6.26e- 1 -1.01      0.605  \n23 Poor    Age           1.79e-2   0.00509    3.53   4.21e- 4  0.00797   0.0279 \n24 Poor    PhysActiveY… -2.27e+0   0.242     -9.38   6.81e-21 -2.74     -1.79   \n25 Poor    Education9 … -3.60e-1   0.353     -1.02   3.08e- 1 -1.05      0.332  \n26 Poor    EducationHi… -1.15e+0   0.334     -3.44   5.86e- 4 -1.81     -0.494  \n27 Poor    EducationSo… -1.07e+0   0.316     -3.40   6.77e- 4 -1.69     -0.454  \n28 Poor    EducationCo… -2.32e+0   0.366     -6.34   2.27e-10 -3.04     -1.60"
  },
  {
    "objectID": "slides/lec-24.html#compare-nhanes-models-using-aic",
    "href": "slides/lec-24.html#compare-nhanes-models-using-aic",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Compare NHANES models using AIC",
    "text": "Compare NHANES models using AIC\nReduced model:\n\nglance(reduced_fit)$AIC\n\n[1] 17018.23\n\n\n\nFull model:\n\nglance(full_fit)$AIC\n\n[1] 16561.1"
  },
  {
    "objectID": "slides/lec-24.html#conditions-for-inference",
    "href": "slides/lec-24.html#conditions-for-inference",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Conditions for inference",
    "text": "Conditions for inference\nWe want to check the following conditions for inference for the multinomial logistic regression model:\n\nLinearity: Is there a linear relationship between the log-odds and the predictor variables?\nRandomness: Was the sample randomly selected? Or can we reasonably treat it as random?\nIndependence: Are the observations independent?"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity",
    "href": "slides/lec-24.html#checking-linearity",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\nSimilar to logistic regression, we will check linearity by examining empirical logit plots between each level of the response and the quantitative predictor variables.\n\nnhanes_adult &lt;- nhanes_adult %&gt;%\n  mutate(\n    Excellent = factor(if_else(HealthGen == \"Excellent\", \"1\", \"0\")),\n    Vgood = factor(if_else(HealthGen == \"Vgood\", \"1\", \"0\")),\n    Good = factor(if_else(HealthGen == \"Good\", \"1\", \"0\")),\n    Fair = factor(if_else(HealthGen == \"Fair\", \"1\", \"0\")),\n    Poor = factor(if_else(HealthGen == \"Poor\", \"1\", \"0\"))\n  )"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-1",
    "href": "slides/lec-24.html#checking-linearity-1",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\nemplogitplot1(Excellent ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Excellent vs. Age\")\nemplogitplot1(Vgood ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Vgood vs. Age\")"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-2",
    "href": "slides/lec-24.html#checking-linearity-2",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\nemplogitplot1(Good ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Good vs. Age\")\nemplogitplot1(Fair ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Fair vs. Age\")"
  },
  {
    "objectID": "slides/lec-24.html#checking-linearity-3",
    "href": "slides/lec-24.html#checking-linearity-3",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Poor ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Poor vs. Age\")\n\n\n\n✅ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the quantitative predictor variable, Age."
  },
  {
    "objectID": "slides/lec-24.html#checking-randomness",
    "href": "slides/lec-24.html#checking-randomness",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n✅ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S.."
  },
  {
    "objectID": "slides/lec-24.html#checking-independence",
    "href": "slides/lec-24.html#checking-independence",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Checking independence",
    "text": "Checking independence\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n✅ The independence condition is satisfied. It is reasonable to conclude that the participants’ health and behavior characteristics are independent of one another."
  },
  {
    "objectID": "slides/lec-24.html#recap",
    "href": "slides/lec-24.html#recap",
    "title": "MultiLR: Prediction + inferential models",
    "section": "Recap",
    "text": "Recap\n\nPredictions\nModel selection for inference\nChecking conditions for inference"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Equation 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Equation 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Equation 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Equation 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Equation 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Equation 5 and Equation 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Equation 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Note\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Equation 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Equation 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Equation 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Equation 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#discussion-forum",
    "href": "course-support.html#discussion-forum",
    "title": "Course support",
    "section": "Discussion forum",
    "text": "Discussion forum\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The online discussion forum is the best venue for these! We will use Conversations as the online discussion forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go Conversations), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 210” in the subject line. Barring extenuating circumstances, I will respond to STA 210 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission and discussion will take place on GitHub instead.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "exams/exam-3.html",
    "href": "exams/exam-3.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 3 is released on Friday, April 15 at 9 am ET and must be completed by Mon, April 18 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes.\n\nGo here to complete Part 1 of the exam.\nThis portion is comprised of 10 multiple choice / fill in the blank questions may only be submitted one time, so start it when you can set aside ~30 minutes to work on it. You will likely be done quicker but it’s best to set aside ample time.\n\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope, like a usual lab and homework.\n\nGo to the GitHub organization for the course and find the exam-3- repo to complete Part 2 of your exam.\nAdd your answers to the exam-3.qmd file in your repo.\nYou can work on this portion at your own pace and come back to it however many times you like until the deadline. I recommend setting aside ~2 hours. You will likely be done quicker, but it’s best to set aside ample time.\n\n\nBoth portions must be completed and submitted by Mon, April 18 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor on Slack or email or request an appointment to meet on Zoom.\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-3.html#overview",
    "href": "exams/exam-3.html#overview",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 3 is released on Friday, April 15 at 9 am ET and must be completed by Mon, April 18 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes.\n\nGo here to complete Part 1 of the exam.\nThis portion is comprised of 10 multiple choice / fill in the blank questions may only be submitted one time, so start it when you can set aside ~30 minutes to work on it. You will likely be done quicker but it’s best to set aside ample time.\n\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope, like a usual lab and homework.\n\nGo to the GitHub organization for the course and find the exam-3- repo to complete Part 2 of your exam.\nAdd your answers to the exam-3.qmd file in your repo.\nYou can work on this portion at your own pace and come back to it however many times you like until the deadline. I recommend setting aside ~2 hours. You will likely be done quicker, but it’s best to set aside ample time.\n\n\nBoth portions must be completed and submitted by Mon, April 18 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-3.html#academic-integrity",
    "href": "exams/exam-3.html#academic-integrity",
    "title": "Exam 2",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-3.html#rules-notes",
    "href": "exams/exam-3.html#rules-notes",
    "title": "Exam 2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor on Slack or email or request an appointment to meet on Zoom."
  },
  {
    "objectID": "exams/exam-3.html#submission",
    "href": "exams/exam-3.html#submission",
    "title": "Exam 2",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "Introduction\nCSC2626 involves a course project in which you will implement a research idea on the topic of imitation learning specifically related to robotics (e.g. control, human-robot interaction, computer vision) or related areas. Any of the broad topic descriptions on the course website, under which weekly readings are listed, can act as a good starting point for picking a project. The purpose of the final project is to give you the chance to spend a significant amount of time focusing on a single research direction. The types of projects that we envision include the following:\n\nImplement the main algorithm described in one of the papers listed on the course website, try to replicate the results, and run it in 2-3 new scenarios.\nProvide an empirical evaluation and comparison of at least three algorithms from the papers in the reading list, on 1-2 illustrative scenarios.\nProvide a theoretical analysis of an algorithm from the papers in the reading list. This could included expected runtime, asymptotic analysis, variance analysis, existence of a property, or something different.\nExtend the main algorithm described in one of the papers listed on the course website in a non-trivial way, and evaluate it on 1-2 new scenarios.\nDesign a new algorithm, and provide sufficient evaluation to demonstrate the merit of the idea, at minimum 1-2 scenarios.\n\nYou can get full marks for the project component by selecting any of these types of projects. You are not expected to produce a novel research idea, although courses like this are meant to create the conditions for students to attempt it. We encourage you to try.\n\n\nPolicy on Collaboration\nPrevious iterations of the course have allowed group collaborations for the final project. This year, we will experiment with only allowing individual final projects. Exceptions to this rule can be made only in rare cases provided there is good reason to do so. Email the instructor if this applies to you.\n\n\nProject Proposal\nYou are expected to describe a well-defined research goal in the proposal. When choosing this goal try to identify the minimum viable objective that you think is likely to work and you can accomplish, just to get you started, some nice-to-haves that you will do provided there is time, and a short review of related work. The definition of your research project may change during the course of a month and a half that you will be working on your project, but your proposal should be as specific and well-defined as possible, otherwise we cannot provide helpful feedback. If you are unsure about your plans, contact the instructor well before the proposal due date. Proposals should not be based only on papers covered in class by the due date. Students are encouraged to look further ahead in the schedule and to start planning their project definition well ahead of the due date.\nProposals are limited to 3 pages (excluding references), with the following suggested structure: 1/4 page for abstract/introduction 1/2 page for the problem definition, 1/2 page for related work, 3/4 page for your proposed method, 1/2 page for your proposed evaluation, 1/4 page for your planned timeline, 1/4 page for anticipated risks and your mitigation plan, and as many pages for references as you need. Proposals should follow the paper template provided by the RSS conference (Robotics: Science and Systems). You will submit your proposal by uploading the pdf on Quercus. The marking scheme for the project proposal is shown in the Grading page of this website.\n\nProject Proposal FAQ\n\nCan I extend a project I completed in a previous class? Yes, you are welcome to do this as long as you provide your final report from that class and include an appendix to the proposal that clarifies what is being added to the previous project.\nCan I extend a project I completed or am working on as part of my research/thesis? Yes, you are welcome to do this as long as you include an appendix to the proposal that clarifies what is being added to the research you have done so far outside this course.\nWhich simulator or dataset should I use? Use the one that is going to allow you to quickly try ideas and prototype. I would not recommend starting with game engines like Unreal Engine 4 and Unity, unless you know what you are doing. Similarly, choose the easiest dataset to get started. Toy data is fine. So are simple scenarios. Start with the easiest and most predictable setting/environment, and only increase complexity if you are making progress. I do not suggest starting from the most complex environment and gradually moving to simpler ones.\nI need a GPU but I don’t have access to one. What should I do? Email the instructor early on if this is a problem. You should also look into Google Colab, and any GPU desktops provided by your department (if any).\n\n\n\n\nPresentation\nThis is a 10min presentation during class, during which you will present the main idea of your project and the progress you have made until that point. It is not required that you have finished your project by this point.\n\n\nFinal Report and Code\nThe final report needs to have at least five pages that include: 1/4 page abstract, 1/2 page introduction, 1/2 page related works, 1.5+ pages describing your method, 1.5+ pages describing your results and evaluation, 1/2 page limitations. The final report may include as many references and appendices as you need. Figures and tables are encouraged. Final reports should follow the same paper template as the proposal. Submit your final project report in pdf form, and a zip file with your code or a link to a github repository to the instructor. The marking scheme for the finalproject is shown in the Grading page of this website.\nDo not forget to enjoy your project and have fun with it. This is an opportunity to learn and investigate what you find exciting!",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "computing-troubleshooting.html",
    "href": "computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from your reserved container, go to status.oit.duke.edu and scroll down to Teaching and Learning Tools. Under this heading you’ll find an entry called Container Manager (CMGR Coursework Containers).\n\nIf the status shows something other than Operational, this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course forum to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!\nIf the status shows Operational, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course forum with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nEither way you can also fill out the form here, which will notify our the R TA for the department as well as our undergraduate coordinator. They’ll be able to help diagnose the issue."
  },
  {
    "objectID": "lecs/w04/lec04.html#todays-agenda",
    "href": "lecs/w04/lec04.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n• Guided policy search (GPS)\n• Policy learning from adaptive MPC with privileged information (PLATO)\n• Combining behavioral cloning and RL\n• Dynamic movement primitives (DMPs)\n• Expert iteration & learning to search\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:\nhttps://www.youtube.com/watch?v=IV8Eze9Hxrc&ab_channel=Dynamicfieldtheory"
  },
  {
    "objectID": "lecs/w04/lec04.html#section-1",
    "href": "lecs/w04/lec04.html#section-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\(\\boxed{\\begin{aligned} p(x_{t+1} | x_t, u_t) &= \\mathcal{N}(f(x_t, u_t), \\Sigma) \\\\\nf(x_t, u_t) &\\approx A_t x_t + B_t u_t \\\\\nA_t &= \\frac{df}{dx_t} \\quad B_t = \\frac{df}{du_t}\n\\end{aligned}}\\)"
  },
  {
    "objectID": "lecs/w04/lec04.html#section-2",
    "href": "lecs/w04/lec04.html#section-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\(\\boxed{\\begin{aligned} \\text{iLQR/iLQG control} \\\\\n\\mathbf{u}_t = \\mathbf{K}_t (\\mathbf{x}_t - \\hat{\\mathbf{x}}_t) + \\mathbf{k}_t + \\hat{\\mathbf{u}}_t \\\\\n\\text{is deterministic. We need it stochastic.}\n\\end{aligned}}\\)\n\n\n\\(p(u_t \\mid x_t) = \\mathcal{N}(K_t (x_t - \\hat{x}_t) + k_t + \\hat{u}_t, \\Sigma_t)\\)\n\n\\(p(\\tau) = p(x_1) \\prod_{t=1}^{T} p(u_t \\mid x_t) p(x_{t+1} \\mid x_t, u_t)\\)"
  },
  {
    "objectID": "lecs/w04/lec04.html#learning-a-globally-valid-nn-policy-an-example",
    "href": "lecs/w04/lec04.html#learning-a-globally-valid-nn-policy-an-example",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning a globally-valid NN policy: an example",
    "text": "Learning a globally-valid NN policy: an example\n\nWe want to solve a stochastic version of this problem\n\n\\[\n\\begin{align}\n\\min_{\\mathbf{u}_1, \\ldots, \\mathbf{u}_T, \\mathbf{x}_1, \\ldots, \\mathbf{x}_T, \\theta} \\sum_{t=1}^{T} c(\\mathbf{x}_t, \\mathbf{u}_t) \\text{s.t.} \\mathbb{x}_t = f(x_{t-1}, u_{t-1}) \\\\\n\\text{s.t.} \\mathbf{u}_t = \\pi_\\theta (x_t)\n\\end{align}\n\\]\n\nwhere both the policy and the dynamics are stochastic. I.e. we want to learn a globally-valid policy that imitates the actions of locally-valid iLQG policies."
  },
  {
    "objectID": "lecs/w04/lec04.html#kl-divergence-constraints",
    "href": "lecs/w04/lec04.html#kl-divergence-constraints",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "KL-Divergence Constraints",
    "text": "KL-Divergence Constraints\n\n\n\n▶ Modified cost function\n\\[\n\\min_{p(\\tau) \\in \\mathcal{N}(\\tau)} E_p[l(\\tau)] \\text{ s.t. } D_{\\text{KL}}(p(\\tau)||\\hat{p}(\\tau)) \\leq \\epsilon\n\\]\n\n\n\n\n\\[\n\\begin{align}\n\\color{red} \\leftarrow\n\\boxed{\n    \\begin{align}\n    \\color{red}\\text{Want to solve a stochastic version of} \\\\\n    \\color{black} \\min_{u_1, \\ldots, u_T, x_1, \\ldots, x_T} \\sum_{t=1}^{T} c(x_t, u_t) \\quad \\text{s.t.} \\quad x_t = f(x_{t-1}, u_{t-1}) \\\\\n    \\color{red}\\text{with an additional trust-region inequality constraint}\n    \\end{align}\n    } \\\\\n\\end{align}\n\\]\n\n\n\\(\\color{black} \\hat{p}(\\tau)\\) is the previous trajectory\ndistribution. Same dynamics,\ndifferent policy."
  },
  {
    "objectID": "lecs/w04/lec04.html#kl-divergence-constraints-1",
    "href": "lecs/w04/lec04.html#kl-divergence-constraints-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "KL-Divergence Constraints",
    "text": "KL-Divergence Constraints\n\n▶ Modified cost function\n\\[\n\\min_{p(\\tau) \\in \\mathcal{N}(\\tau)} E_p[l(\\tau)] \\text{ s.t. } D_{\\text{KL}}(p(\\tau)||\\hat{p}(\\tau)) \\leq \\epsilon\n\\]\n▶ Lagrangian of this problem (\\(\\eta\\) dual variable)\n\\[\\mathcal{L}_{\\text{traj}}(p(\\tau), \\eta) = E_p[l(\\tau)] + \\eta[D_{\\text{KL}}(p(\\tau)||\\hat{p}(\\tau)) - \\epsilon]\\]\n▶ Assuming \\(p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{u}_t) = \\hat{p}(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{u}_t) = \\mathcal{N}(f_{\\mathbf{x}t}\\mathbf{x}_t + f_{\\mathbf{u}t}\\mathbf{u}_t, \\mathbf{F}_t)\\)\n\\[\n\\mathcal{L}_{\\text{traj}}(p(\\tau), \\eta) = \\left[\\sum_t E_{p(\\mathbf{x}_t, \\mathbf{u}_t)}[l(\\mathbf{x}_t, \\mathbf{u}_t) - \\eta \\log p(\\mathbf{u}_t|\\mathbf{x}_t)]\\right] - \\eta H(p(\\tau)) - \\eta \\epsilon\n\\]\n\n▶ Augmented cost function\n\\[\\tilde{l}(\\mathbf{x}_t, \\mathbf{u}_t) = \\frac{1}{\\eta}l(\\mathbf{x}_t, \\mathbf{u}_t) - \\log \\hat{p}(\\mathbf{u}_t|\\mathbf{x}_t)\\]\n\n\n▶ Solved by dual gradient descent"
  },
  {
    "objectID": "lecs/w04/lec04.html#dual-gradient-descent",
    "href": "lecs/w04/lec04.html#dual-gradient-descent",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Dual Gradient Descent",
    "text": "Dual Gradient Descent\n\n\n\\(\\min_x f(x) \\quad \\text{s.t.} \\quad C(x) = 0\\)\n\n\\(L(x, \\lambda) = f(x) + \\lambda C(x)\\)\n\n\\(g(\\lambda) = L(x^*(\\lambda), \\lambda)\\)\n\n\n\nhttp://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf"
  },
  {
    "objectID": "lecs/w04/lec04.html#general-parameterized-policies",
    "href": "lecs/w04/lec04.html#general-parameterized-policies",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "General Parameterized Policies",
    "text": "General Parameterized Policies\n\n▶ Objective\n\\[\n\\min_{\\theta,\\, p(\\tau)} \\mathbb{E}_{p(\\tau)}[\\ell(\\tau)] \\quad \\text{s.t.} \\quad D_{KL}(p(x_t)\\pi_\\theta(u_t|x_t)\\,\\|\\,p(x_t, u_t)) = 0,\\ \\forall t\n\\]\n▶ Lagrangian of the problem \\[\n\\begin{align}\n\\mathcal{L}_{GPS}(\\theta, p, \\lambda) &= \\mathbb{E}_{p(\\tau)}[\\ell(\\tau)] + \\sum_{t=1}^{T} \\lambda_t D_{KL}(p(x_t)\\pi_\\theta(u_t|x_t)\\,\\|\\,p(x_t, u_t)) \\\\\n\\end{align}\n\\]\n\nOR \\[\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad = \\mathbb{E}_{p(\\tau)}[\\ell(\\tau) - \\eta\\log\\hat{p}(\\tau)] - \\eta\\mathcal{H}(p) + \\sum_{t=1}^{T} \\lambda_t D_{KL}(p(x_t)\\pi_\\theta(u_t|x_t)\\,\\|\\,p(x_t, u_t))\n\\]\nif we include the trust-region constraint"
  },
  {
    "objectID": "lecs/w04/lec04.html#general-parameterized-policies-1",
    "href": "lecs/w04/lec04.html#general-parameterized-policies-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "General Parameterized Policies",
    "text": "General Parameterized Policies"
  },
  {
    "objectID": "lecs/w04/lec04.html#experiments-conducted",
    "href": "lecs/w04/lec04.html#experiments-conducted",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments Conducted",
    "text": "Experiments Conducted\n▶ 2D, 3D peg insertion (discontinuous dynamics)\n▶ Octopus arm control (high-dimensional state and action space)\n▶ Planar swimming (three-link snake)\n▶ Walking (seven-link biped to maintain a target velocity)"
  },
  {
    "objectID": "lecs/w04/lec04.html#trajectory-optimization",
    "href": "lecs/w04/lec04.html#trajectory-optimization",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Trajectory Optimization",
    "text": "Trajectory Optimization"
  },
  {
    "objectID": "lecs/w04/lec04.html#trajectory-optimization-1",
    "href": "lecs/w04/lec04.html#trajectory-optimization-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Trajectory Optimization",
    "text": "Trajectory Optimization"
  },
  {
    "objectID": "lecs/w04/lec04.html#neural-network-policy-learning-with-gps",
    "href": "lecs/w04/lec04.html#neural-network-policy-learning-with-gps",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Neural Network Policy Learning with GPS",
    "text": "Neural Network Policy Learning with GPS"
  },
  {
    "objectID": "lecs/w04/lec04.html#training-with-privileged-information",
    "href": "lecs/w04/lec04.html#training-with-privileged-information",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Training with privileged information",
    "text": "Training with privileged information\n\n\n\n\\[\n\\boxed{\n    \\begin{align}\n\\min_{p,\\, \\theta} \\mathbb{E}_{\\tau \\sim p(\\tau)}[c(\\tau)] \\quad \\\\\n\\text{s.t.} \\quad p(u_t \\mid x_t) = \\pi_{\\theta}(u_t \\mid x_t)\n\\end{align}\n}\n\\]\n\n\\[\n\\boxed{\n    \\begin{align}\n\\min_{p,\\, \\theta} \\mathbb{E}_{\\tau \\sim p(\\tau)}[c(\\tau)] \\quad \\\\\n\\text{s.t.} \\quad p(u_t \\mid x_t) = \\pi_{\\theta}(u_t \\mid o_t)\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "lecs/w04/lec04.html#section-3",
    "href": "lecs/w04/lec04.html#section-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=Q4bMcUk6pcw&t=40s&ab_channel=RAIL"
  },
  {
    "objectID": "lecs/w04/lec04.html#todays-agenda-1",
    "href": "lecs/w04/lec04.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n• Guided policy search (GPS)\n• Policy learning from adaptive MPC with privileged information (PLATO)\n• Combining behavioral cloning and RL\n• Dynamic movement primitives (DMPs)\n• Expert iteration & learning to search\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:\nhttps://www.youtube.com/watch?v=IV8Eze9Hxrc&ab_channel=Dynamicfieldtheory"
  },
  {
    "objectID": "lecs/w04/lec04.html#section-4",
    "href": "lecs/w04/lec04.html#section-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=clHp6QgVyAU"
  },
  {
    "objectID": "lecs/w04/lec04.html#problem-set-up",
    "href": "lecs/w04/lec04.html#problem-set-up",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Problem Set-up",
    "text": "Problem Set-up\n\n\nStates \\(x\\), actions \\(u\\).\n\nThe policy could only control the system from observations \\(o\\).\n\nThe policy \\(\\pi_\\theta(u \\mid o_t)\\), parametrized by \\(\\theta\\).\n\nAt test time, the agent chooses actions according to \\(\\pi_\\theta(u \\mid o_t)\\) at each time step \\(t\\), and experiences a loss \\(c(x_t \\mid o_t) \\in [0, 1]\\).\n\nThe next state is distributed by dynamics \\(p(x_{t+1} \\mid x_t, u_t)\\).\n\nThe objective is to learn policy \\(\\pi_\\theta(u \\mid o_t)\\) such that\n\\[\n\\arg\\min_\\pi J(\\pi) = \\mathbb{E}_\\pi \\left[ \\sum_{t=1}^{T} c(x_t, u_t) \\right].\n\\]\nAt the same time, let’s define expected cost from state \\(x_t\\) at time \\(t\\) as\n\\[\nJ(\\pi \\mid x_t) = \\mathbb{E}_\\pi \\left[ \\sum_{t=1}^{T} c(x_t, u_t) \\mid x_t \\right].\n\\]"
  },
  {
    "objectID": "lecs/w04/lec04.html#adaptive-mpc-teacher",
    "href": "lecs/w04/lec04.html#adaptive-mpc-teacher",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Adaptive MPC Teacher",
    "text": "Adaptive MPC Teacher\nOne naive way is to train the policy with supervised learning from data generated from an MPC teacher. However, because the state distribution for the teacher and learner are different, the learned policy might fail.\nIn order to overcome this challenge, an adaptive MPC teacher is used which generates actions from a controller obtained by:\n\\[\n\\pi^t_\\lambda(u \\mid x_t, \\theta) \\leftarrow \\arg\\min_\\pi J_t(\\pi \\mid x_t) + \\lambda D_{KL}(\\pi(u \\mid x_t) \\,\\|\\, \\pi_\\theta(u \\mid o_t)) \\qquad \\tag{1}\n\\]\nwhere \\(\\lambda\\) determines the relative importance of matching the learner policy versus optimizing the expected return. Note that the particular MPC algorithm is based on iLQG."
  },
  {
    "objectID": "lecs/w04/lec04.html#algorithm",
    "href": "lecs/w04/lec04.html#algorithm",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm",
    "text": "Algorithm\nAlgorithm 1: PLATO algorithm\nInitialize data \\(D \\leftarrow \\emptyset\\)\nfor \\(i = 1\\) to \\(N\\) do\n for \\(t = 1\\) to \\(T\\) do\n  \\(\\pi_\\lambda^{t}(u_t \\mid x_t, \\theta) \\leftarrow \\arg\\min_{\\pi} J_t(\\pi \\mid x_t) + \\lambda D_{KL}(\\pi(u \\mid x_t) \\,\\|\\, \\pi_{\\theta}(u \\mid o_t))\\)\n  Sample \\(u_t \\sim \\pi_\\lambda^{t}(u \\mid x_t, \\theta)\\)\n  \\(\\pi^{*}(u_t \\mid x_t) \\leftarrow \\arg\\min_{\\pi} J(\\pi)\\)\n  Sample \\(u_t^{*} \\sim \\pi^{*}(u \\mid x_t)\\)\n  Append \\((o_t, u_t^{*})\\) to dataset \\(D\\)\n  State evolves \\(x_{t+1} \\sim p(x_{t+1} \\mid x_t, u_t)\\)\n end for\nTrain \\(\\pi_\\theta\\) on \\(D\\)\nend for"
  },
  {
    "objectID": "lecs/w04/lec04.html#training-the-learners-policy",
    "href": "lecs/w04/lec04.html#training-the-learners-policy",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Training the learner’s policy",
    "text": "Training the learner’s policy\nDuring the supervised learning phase, we minimize the KL-divergence between the learner policy \\(\\pi_\\theta\\) and precomputed near-optimal policies \\(\\pi^*\\), which is estimated by iLQG:\n\\[\n\\theta \\leftarrow \\arg\\min_\\theta \\sum_{(x_t, o_t) \\in D} D_{KL}(\\pi_\\theta(u \\mid o_t) \\,\\|\\, \\pi^*(u \\mid x_t)).\n\\]\nSince both \\(\\pi_\\theta\\) and \\(\\pi^*\\) are conditionally Gaussian, the KL divergence can be expressed in closed form if we ignore the terms not involving the learner policy means \\(\\mu_\\theta(o_t)\\):\n\\[\n\\min_\\theta \\sum_{(x_t, o_t) \\in D} \\left\\| \\mu^*(x_t) - \\mu_\\theta(o_t) \\right\\|^2_{\\Sigma_{\\pi^*}^{-1/2}}.\n\\]\nIn this paper, \\(\\mu_\\theta\\) is represented by a NN, and solved by SGD."
  },
  {
    "objectID": "lecs/w04/lec04.html#theoretical-analysis",
    "href": "lecs/w04/lec04.html#theoretical-analysis",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Theoretical Analysis",
    "text": "Theoretical Analysis\nLet \\(Q_t(x, \\pi, \\tilde{\\pi})\\) denote the cost of executing \\(\\pi\\) for one time step starting from an initial state, and then executing \\(\\tilde{\\pi}\\) for the remaining \\(t - 1\\) time steps. We assume the cost-to-go difference between the learned policy and the optimal policy is bounded: \\(Q_t(x, \\pi, \\pi^*) - Q_t(x, \\pi^*, \\pi^*) \\leq \\delta\\)\n\n\n\nTheorem\nLet the cost-to-go \\(Q_t(x,\\pi,\\pi^*) - Q_t(x,\\pi^*,\\pi^*) \\leq \\delta\\) for all \\(t \\in \\{1,\\ldots,T\\}\\).\nThen for PLATO, \\(J(\\pi_\\theta) \\leq J(\\pi^*) + \\delta \\sqrt{\\epsilon_\\theta *} \\cdot O(T) + O(1)\\)\n\n\n\nTherefore, the policy learned by PLATO converges to a policy with bounded cost."
  },
  {
    "objectID": "lecs/w04/lec04.html#comparison-to-dagger",
    "href": "lecs/w04/lec04.html#comparison-to-dagger",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Comparison to DAgger",
    "text": "Comparison to DAgger\nPLATO could be viewed as a generalization of DAgger, which samples from mixture policy\n\\[\n\\pi_{\\text{mix},i} = \\beta_i \\pi^* + (1 - \\beta_i) \\pi_{\\theta_i}\n\\]\nDifferences with the DAgger:\n\nThe training data is labeled with actions from \\(\\pi^*\\).\n\nPLATO uses adaptive MPC policy to select actions at each time step, rather than the mixture policy \\(\\pi_{\\text{mix},i}\\) used."
  },
  {
    "objectID": "lecs/w04/lec04.html#todays-agenda-2",
    "href": "lecs/w04/lec04.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Guided policy search (GPS)\n• Policy learning from adaptive MPC with privileged information (PLATO)\n\n• Combining behavioral cloning and RL\n• Dynamic movement primitives (DMPs)\n• Expert iteration & learning to search\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:\nhttps://www.youtube.com/watch?v=IV8Eze9Hxrc&ab_channel=Dynamicfieldtheory"
  },
  {
    "objectID": "lecs/w04/lec04.html#section-6",
    "href": "lecs/w04/lec04.html#section-6",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=vppFvq2quQ0"
  },
  {
    "objectID": "lecs/w04/lec04.html#learning-complex-dexterous-manipulation-with-deep-reinforcement-learning-and-demonstrations",
    "href": "lecs/w04/lec04.html#learning-complex-dexterous-manipulation-with-deep-reinforcement-learning-and-demonstrations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations",
    "text": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations\n\nAravind Rajeswaran , Vikash Kumar , Abhishek Gupta, Giulia Vezzani , John Schulman , Emanuel Todorov , Sergey Levine\n\nJason Rebello\nUTIAS"
  },
  {
    "objectID": "lecs/w04/lec04.html#motivation",
    "href": "lecs/w04/lec04.html#motivation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation",
    "text": "Motivation\n\n\n• Dexterous multi-fingered hands are extremely versatile\n• Control is challenging due to high dimensionality, complex contact patterns\n• Previous methods require reward shaping\n• DRL limited to simpler manipulators and simple tasks\n• Lack of physical systems due to sample inefficiency\n\n\n\n\nObject Relocation task"
  },
  {
    "objectID": "lecs/w04/lec04.html#contributions",
    "href": "lecs/w04/lec04.html#contributions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Contributions",
    "text": "Contributions\n\n\n• Manipulation with 24-DOF hand\n• Model Free DRL\n• Used in complex tasks with variety of tools\n• Small number of human demonstrations reduces sample complexity\n• Reduces learning time\n• Robust and natural movements\n\n\n\n\nTool use task"
  },
  {
    "objectID": "lecs/w04/lec04.html#manipulation-task-1",
    "href": "lecs/w04/lec04.html#manipulation-task-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Manipulation Task 1",
    "text": "Manipulation Task 1\nObject Relocation\n\n• Move Blue ball to green position\n• Task complete when ball is epsilon ball away from target\n• Positions of ball and target are randomized\n• Main challenge is exploration (reach object, grab and move to target location)"
  },
  {
    "objectID": "lecs/w04/lec04.html#manipulation-task-2",
    "href": "lecs/w04/lec04.html#manipulation-task-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Manipulation Task 2",
    "text": "Manipulation Task 2\nIn-hand Manipulation\n\n• Reposition blue pen to match orientation of green target\n• Task complete when orientation is achieved\n• Base of hand is fixed\n• Large number of contacts with complex solutions\n• Used a well shaped reward for training an expert"
  },
  {
    "objectID": "lecs/w04/lec04.html#manipulation-task-3",
    "href": "lecs/w04/lec04.html#manipulation-task-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Manipulation Task 3",
    "text": "Manipulation Task 3\nDoor Opening\n\n• Undo latch and swing door open\n• Task complete when door touches door stopper\n• No information of latch explicitly provided\n• A lot of hidden sub-tasks\n• Position of door is randomized"
  },
  {
    "objectID": "lecs/w04/lec04.html#manipulation-task-4",
    "href": "lecs/w04/lec04.html#manipulation-task-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Manipulation Task 4",
    "text": "Manipulation Task 4\nTool Use\n\n• Pickup and hammer nail\n• Task complete when entire nail is inside the board\n• Use tool instead of just relocation\n• Multiple steps in task"
  },
  {
    "objectID": "lecs/w04/lec04.html#experimental-setup",
    "href": "lecs/w04/lec04.html#experimental-setup",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experimental Setup",
    "text": "Experimental Setup\n\n\n\n\n\nADROIT hand \n\n\nHTC headset \n\n\n\n\n24-DOF hand\nFirst, middle, ring – 4 DOF each\nLittle finger, thumb – 5 DOF each\nWrist – 2 DOF\nActuated with position control and has joint angle sensor\nMuJoCo physics simulation with friction\n25 demonstrations for each task\n\n\nHAPTIX Simulator\n\n\nCyberGlove 3"
  },
  {
    "objectID": "lecs/w04/lec04.html#methodology-preliminaries",
    "href": "lecs/w04/lec04.html#methodology-preliminaries",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Methodology (Preliminaries)",
    "text": "Methodology (Preliminaries)\n\n\nStates \\(\\downarrow\\)\nRewards\n\\(\\downarrow\\)\nInitial Probability distribution\n\\(\\downarrow\\)\n\nMDP definition: \\(\\mathcal{M} = \\{S, A, R, \\mathcal{T}, \\rho_0, \\gamma\\}\\)\n\nActions \\(\\uparrow\\)\n\\(\\uparrow\\)\nTransition\ndynamics\n\\(\\uparrow\\)\nDiscount Factor\n\n\nValue function:\n\\[\nV^{\\pi}(s) = \\mathbb{E}_{\\pi,\\mathcal{M}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\,\\big|\\, s_0 = s \\right]\n\\]\nQ function: \\(Q^{\\pi}(s, a) = \\underline{\\mathbb{E}_{\\mathcal{M}} [R(s,a)]} + \\underline{\\mathbb{E}_{s' \\sim \\mathcal{T}(s,a)} [V^{\\pi}(s')]}\\)\n\nAdvantage function: \\(A^{\\pi}(s, a) = Q^{\\pi}(s,a) - V^{\\pi}(s)\\)\n\nReward for taking\naction a in state s\nExpected reward in state s’"
  },
  {
    "objectID": "lecs/w04/lec04.html#methodology-npg",
    "href": "lecs/w04/lec04.html#methodology-npg",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Methodology (NPG)",
    "text": "Methodology (NPG)\n• Directly optimize parameters of policy to maximize objective\n\nSub-optimal \\(\\searrow\\)\nVanilla Policy Gradient: \\(g = \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i \\mid s_t^i) \\hat{A}^{\\pi}(s_t^i, a_t^i, t)\\)\nFisher Information Matrix: \\(F_{\\theta} = \\frac{1}{NT} \\sum_{i=1}^{N} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i \\mid s_t^i) \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t^i \\mid s_t^i)^{T}\\)\n\n• Fisher information matrix measures the curvature (sensitivity) of policy relative to model parameters\n• Fisher information matrix is related to the Hessian matrix"
  },
  {
    "objectID": "lecs/w04/lec04.html#methodology-npg-1",
    "href": "lecs/w04/lec04.html#methodology-npg-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Methodology (NPG)",
    "text": "Methodology (NPG)\n• Limit policy change based on parameter change\n• Fisher information matrix maps between parameter space and policy space\n• Generally use learning rate in optimization\n• Poor step size leads to poor initialization\n• Use Fisher information matrix to perform update\n\nGradient ascent update: \\(\\theta_{k+1} = \\theta_k + \\underline{\\sqrt{\\frac{\\delta}{g^T F_{\\theta_k}^{-1} g}}} \\, \\overline{F_{\\theta_k}^{-1} g}\\)\n\nNormalized step-size\nSteepest Ascent direction"
  },
  {
    "objectID": "lecs/w04/lec04.html#methodology-problems-with-rl",
    "href": "lecs/w04/lec04.html#methodology-problems-with-rl",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Methodology (Problems with RL)",
    "text": "Methodology (Problems with RL)\n• Challenges with using NPG\n      • RL requires careful reward shaping\n      • Impractical number of samples to learn (approx. 100 hours)\n      • Unnatural movement\n      • Not as robust to environmental variations\n\n• Solution\n      • Combine RL with demonstrations\n      • Guide exploration and decrease sample complexity\n      • Robust and natural looking behaviour\n      • Demonstration Augmented Policy Gradient (DAPG)"
  },
  {
    "objectID": "lecs/w04/lec04.html#methodology-pretraining-with-bc",
    "href": "lecs/w04/lec04.html#methodology-pretraining-with-bc",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Methodology (Pretraining with BC)",
    "text": "Methodology (Pretraining with BC)\n• Exploration in PG achieved with stochastic action distribution\n• Poor initialization leads to slow exploration\n• Behavioral Cloning (BC) guides exploration\n• Reduces sample complexity\n\\[\n\\text{maximize}_{\\theta} \\sum_{(s, a) \\in \\rho_D} \\ln \\pi_{\\theta}(a \\mid s)\n\\]\n• Mimic actions taken in demonstrations\n• Does not guarantee effectiveness of policy due to distributional shift"
  },
  {
    "objectID": "lecs/w04/lec04.html#methodology-fine-tuning-with-augmented-loss",
    "href": "lecs/w04/lec04.html#methodology-fine-tuning-with-augmented-loss",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Methodology (Fine-tuning with augmented loss)",
    "text": "Methodology (Fine-tuning with augmented loss)\n• BC does not make optimal use of demonstrations\n• Cannot learn subtasks (reaching, grasping, hammering)\n• BC policy (only grasping)\n• Capturing all data\n\\[\ng_{\\text{aug}} = \\sum_{(s,a) \\in \\rho_{\\pi}} \\overline{\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)} \\, A^{\\pi}(s, a) + \\sum_{(s,a) \\in \\rho_{D}} \\overline{\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)} \\, w(s, a)\n\\]\n\n\\[\nw(s, a) = \\lambda_0 \\lambda_1^k \\max_{(s', a') \\in \\rho_\\pi} A^\\pi(s', a') \\quad \\forall (s, a) \\in \\rho_D\n\\]\n\nPolicy gradient\nBehavioral cloning\n\\(\\uparrow\\)\nDataset from policy\n\\(\\uparrow\\)\nDataset from\ndemonstrations\n\\(\\uparrow\\)\nWeighting\nfunction\n\\(\\downarrow\\) iteration\n\\(\\uparrow \\quad \\uparrow\\)\nhyperparameters"
  },
  {
    "objectID": "lecs/w04/lec04.html#results-1",
    "href": "lecs/w04/lec04.html#results-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results 1",
    "text": "Results 1\nReinforcement learning from scratch\n• Can RL cope with high dimensional manipulation tasks ?\n• Is it robust to variations in environment ?\n• Are movements safe and can they be used on real hardware ?\n\n• Compare NPG vs DDPG (Deep Deterministic Policy Gradient)\n• DDPG is a policy gradient actor-critic algorithm that is off-policy\n• Stochastic policy for exploration, estimates deterministic policy\n• Score based on percentage of successful trajectories (100 samples)\n• Sparse Reward vs Reward shaping"
  },
  {
    "objectID": "lecs/w04/lec04.html#results-1-1",
    "href": "lecs/w04/lec04.html#results-1-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results 1",
    "text": "Results 1\nReinforcement learning from scratch\n\n• NPG learns with reward shaping, DDPG fails to learn\n• DDPG is sample efficient but sensitive to hyper-parameters\n• Resulting policies have unnatural behaviors\n• Poor sample efficiency, cant use on hardware\n• Cannot generalize to unseen environment (weight and ball size change)"
  },
  {
    "objectID": "lecs/w04/lec04.html#results-2",
    "href": "lecs/w04/lec04.html#results-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results 2",
    "text": "Results 2\nReinforcement learning with demonstrations\n• Does incorporating demonstrations reduce learning time?\n• Comparison of DAPG vs DDPGfD (.. from Demonstrations)?\n• Does it result in human like behaviour ?\n\n• DDPGfD better version of DDPG (demonstrations in replay buffer, prioritized experience replay, n-step returns, regularization)\n• Only use sparse rewards"
  },
  {
    "objectID": "lecs/w04/lec04.html#results-2-1",
    "href": "lecs/w04/lec04.html#results-2-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results 2",
    "text": "Results 2\nReinforcement learning with demonstrations\n\n\n\n• DAPG outperforms DDPGfD\n• DAPG requires few robot hours\n• Can be used on real hardware\n• Robust and human behavior\n• Generalizes to unseen environment"
  },
  {
    "objectID": "lecs/w04/lec04.html#future-work",
    "href": "lecs/w04/lec04.html#future-work",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Future Work",
    "text": "Future Work\n• Tests on real hardware\n• Reduce sample complexity using novelty based exploration methods\n• Learn policies from raw visual inputs and tactile sensing"
  },
  {
    "objectID": "lecs/w04/lec04.html#results",
    "href": "lecs/w04/lec04.html#results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results",
    "text": "Results\n\nhttps://www.youtube.com/watch?v=jJtBll8l_OM"
  },
  {
    "objectID": "lecs/w04/lec04.html#todays-agenda-3",
    "href": "lecs/w04/lec04.html#todays-agenda-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Guided policy search (GPS)\n• Policy learning from adaptive MPC with privileged information (PLATO)\n• Combining behavioral cloning and RL\n\n• Dynamic movement primitives (DMPs)\n• Expert iteration & learning to search\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:\nhttps://www.youtube.com/watch?v=IV8Eze9Hxrc&ab_channel=Dynamicfieldtheory"
  },
  {
    "objectID": "lecs/w04/lec04.html#section-7",
    "href": "lecs/w04/lec04.html#section-7",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=IV8Eze9Hxrc"
  },
  {
    "objectID": "lecs/w04/lec04.html#todays-agenda-4",
    "href": "lecs/w04/lec04.html#todays-agenda-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Guided policy search (GPS)\n• Policy learning from adaptive MPC with privileged information (PLATO)\n• Combining behavioral cloning and RL\n• Dynamic movement primitives (DMPs)\n\n• Expert iteration & learning to search\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yuwei Chen, Jienan Yao, and Jason Rebello as well as from YouTube recordings of lectures from Gregor Schöner, for his Autonomous Robotics course:\nhttps://www.youtube.com/watch?v=IV8Eze9Hxrc&ab_channel=Dynamicfieldtheory"
  },
  {
    "objectID": "lecs/w04/lec04.html#from-search-to-learning-and-back",
    "href": "lecs/w04/lec04.html#from-search-to-learning-and-back",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "From Search to Learning and Back",
    "text": "From Search to Learning and Back\n\n\n\nLogic Geometric Programming\nMarc Toussaint et al. IJCAI’15, RSS’17\nPDDLStream Planners\nCaelan Garret et al. ICAPS’20\nTAMP = SMT + Motion Planning\nNeil Dantam et al. IJRR’18\n. . .\n\n\\(-\\) Need to specify symbols / logic\n\\(-\\) Slow to plan, not very reactive\n\n\n\\(+\\) Generalize\n\\(+\\) No training data\n\n\n\n\nLearning to Guide TAMP\nBeomjoom Kim et al., AAAI’18\nPLOI\nTom Silver, Rohan Chitnis et al. AAAI’21\nDeep Visual Heuristics\nDanny Driess et al. ICRA’20\nLearning to Search for TAMP\nMohamed Khodeir et al. RAL’22, ICRA’23\nText2Motion\nChris Agia et al. ICRA’23\n\\(-\\) Need to specify symbols / logic\n\\(+\\) Can be made fast, reactive\n\n\\(+\\) Generalize\n\\(+\\) Few training data needed\n\n\n\n\nPaLM-E\nDanny Driess et al. ’23\nSayCan\nMichael Ahn et al. CoRL‘23\nRT-1\nAnthony Brohan et al. ’22\nDeep Affordance Foresight\nDanfei Xu et al. ICRA’21\nPlaNet, Dreamer, Hierarchical RL\nDanijar Hafner et al.\n\n\\(+\\) Symbols not needed\n\\(+\\) Fast, reactive\n\n\n\\(-\\) Do not generalize\n\\(-\\) Large data regime"
  },
  {
    "objectID": "lecs/w04/lec04.html#learning-to-plan-via-expert-iteration",
    "href": "lecs/w04/lec04.html#learning-to-plan-via-expert-iteration",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning to Plan via Expert Iteration",
    "text": "Learning to Plan via Expert Iteration\n\n\n\n\n\nIf we do multiple rounds of heuristic learning and tree search, we could potentially get:\n• monotonic improvement guarantees for the policy / planning heuristic\n• convergence to a point where tree search and the policy are equally good\n\n\nDual Policy Iteration. Sun, Gordon, Boots, Bagnell. NeurIPS’18.\nThinking Fast and Slow with Deep Learning and Tree Search. Anthony, Tian, Barber. NeurIPS’17.\nAlphaGo Zero: Mastering the Game of Go Without Human Knowledge. Silver, Schrittwieser, Simonyan, Antonoglou. Nature’17."
  },
  {
    "objectID": "lecs/w04/lec04.html#alphago-zero",
    "href": "lecs/w04/lec04.html#alphago-zero",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\n\n\n\n\n\nAlphaGo Zero: Mastering the Game of Go Without Human Knowledge. Silver, Schrittwieser, Simonyan, Antonoglou. Nature’17."
  },
  {
    "objectID": "lecs/w04/lec04.html#mcts-in-alphago-zero",
    "href": "lecs/w04/lec04.html#mcts-in-alphago-zero",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "MCTS in AlphaGo Zero",
    "text": "MCTS in AlphaGo Zero\n\nAlphaGo Zero: Mastering the Game of Go Without Human Knowledge. Silver, Schrittwieser, Simonyan, Antonoglou. Nature’17."
  },
  {
    "objectID": "lecs/w04/lec04.html#section-8",
    "href": "lecs/w04/lec04.html#section-8",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "A policy optimization framework that includes\n\nGuided Policy Search\nExpert Iteration\nAlphaGo Zero\n“Thinking fast and slow”\nAggreVaTeD (a varianbt of DAgger)\n\nas special cases and provides conditions under which we expect monotonic improvement of the fast, reactive policy."
  },
  {
    "objectID": "lecs/w04/lec04.html#section-10",
    "href": "lecs/w04/lec04.html#section-10",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\begin{array}{ll}\n\\text{1.} & \\text{Fit MLE } \\hat{P} \\text{ on states and actions from } d_{\\pi_n} \\pi_n \\text{ (Eq. 2).} \\\\\n\\text{2.} & \\eta_n \\leftarrow \\text{MBOC}(\\hat{P}), \\text{ subject to trust region } \\mathbb{E}_{s \\sim d_{\\pi_n}} D_{TV}(\\pi, \\pi_n) \\leq \\alpha \\text{ (Eq. 4)} \\\\\n\\text{3.} & \\boxed{\\text{Update to } \\pi_{n+1} \\text{ by imitating } \\eta_n, \\text{ subject to trust region } \\mathbb{E}_{s \\sim d_{\\pi_{\\eta}}} D_{TV}(\\pi, \\pi_n) \\leq \\beta \\text{ (Eq. 5)}}\n\\end{array}\n\\]\n\n\nMain difference with respect to Guided Policy Search\n\n\nGPS, including the mirror descent version, phrases the update procedure of the reactive policy as\na behavior cloning procedure, i.e., given an expert policy \\(\\eta\\), we perform \\(\\min_{\\pi} D_{KL}(d_{\\mu \\mu} \\,\\|\\, d_{\\pi \\pi})^3\\)\nNote that our approach to updating \\(\\pi\\) is fundamentally on-policy, i.e., we generate samples from \\(\\pi\\)."
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda",
    "href": "lecs/w02/lec02.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Intro to Control & Reinforcement Learning\n• Linear Quadratic Regulator (LQR)\n• Iterative LQR\n• Model Predictive Control\n• Learning dynamics and model-based RL\n\n\n\n\n\nAcknowledgments\nToday’s slides have been influenced by: Pieter Abbeel (ECE287), Sergey Levine (DeepRL), Ben Recht (ICML’18), Emo Todorov, Zico Kolter"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-1",
    "href": "lecs/w02/lec02.html#section-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n&\\underset{\\pi_0, \\ldots, \\pi_{T-1}}{\\text{minimize}} \\quad \\mathbb{E}_{e_t} \\left[ \\sum_{t=0}^{T} c(\\mathbf{x}_t, \\mathbf{u}_t) \\right] \\\\\n&\\text{subject to} \\quad \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t) \\quad \\color{red}{\\text{known dynamics}} \\\\\n&\\qquad \\qquad \\quad \\mathbf{u}_t = \\pi_t(\\mathbf{x}_{0:t}, \\mathbf{u}_{0:t-1}) \\\\\n&\\qquad \\qquad \\qquad \\qquad \\qquad \\color{red}{\\text{control law / policy}}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n&\\underset{\\theta}{\\text{maximize}} \\quad \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum_{t=0}^{T} r(\\mathbf{x}_t, \\mathbf{a}_t) \\right] \\\\\n\\\\\n&p_\\theta(\\tau) = p_\\theta(\\mathbf{x}_{0:T}, \\mathbf{a}_{0:T-1}) \\\\\n&\\quad = p(\\mathbf{x}_0) \\prod_{t=1}^{T} \\pi_\\theta(\\mathbf{a}_t | \\mathbf{x}_t) p(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) \\\\\n&\\qquad \\qquad \\qquad \\color{red}{\\text{policy}} \\quad \\color{red}{\\text{dynamics}}\n\\end{aligned}\n\\]\ncost = -reward"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-2",
    "href": "lecs/w02/lec02.html#section-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\nJ(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ c(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbb{E}_{e_t} \\left[ J(f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t)) \\right] \\right]\n\\]\n\\(\\color{red}\\uparrow\\)\n\nOptimal cost-to-go:\n“if you land at state x and you follow the optimal actions what is the expected cost you will pay?”"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-3",
    "href": "lecs/w02/lec02.html#section-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\textcolor{red}{\\swarrow \\text{For finite time horizon} \\searrow}\n\\]\n\n\n\n\\[\nJ(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ c(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbb{E}_{e_t} \\left[ J(f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t)) \\right] \\right]\n\\]\n\n\\(\\qquad \\uparrow\\)\nOptimal cost-to-go\n\n\n\\[\nV^*(\\mathbf{x}_t) = \\max_{\\mathbf{a}_t} \\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^*(\\mathbf{x}_{t+1}) \\right] \\right]\n\\]\n\n\\(\\uparrow\\)\n\nOptimal value function:\n“if you land at state x and you follow the optimal policy\nwhat is the expected reward you will accumulate?”"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-4",
    "href": "lecs/w02/lec02.html#section-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\textcolor{red}{\\swarrow \\text{For finite time horizon} \\searrow}\n\\]\n\n\n\n\\[\nJ(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ c(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbb{E}_{e_t} \\left[ J(f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t)) \\right] \\right]\n\\]\n\n\\(\\qquad \\uparrow\\)\nOptimal cost-to-go\n\n\n\\[\nV^*(\\mathbf{x}_t) = \\max_{\\mathbf{a}_t} \\underbrace{\\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^*(\\mathbf{x}_{t+1}) \\right] \\right]}_{Q^*(x_t, a_t)}\n\\]\n\n\\(\\uparrow\\)\nOptimal value function\n\n\n\n\\(\\qquad \\qquad \\qquad \\uparrow\\)\nOptimal state-action value function:\n“if you land at state x, and you commit\nto first execute action a, and then\nfollow the optimal policy how much\nreward will you accumulate?”"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-5",
    "href": "lecs/w02/lec02.html#section-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\textcolor{red}{\\swarrow \\text{For finite time horizon} \\searrow}\n\\]\n\n\n\n\\[\nJ(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ c(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbb{E}_{e_t} \\left[ J(f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t)) \\right] \\right]\n\\]\n\n\\(\\qquad \\uparrow\\)\nOptimal cost-to-go\n\n\n\\[\nV^*(\\mathbf{x}_t) = \\max_{\\mathbf{a}_t} \\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^*(\\mathbf{x}_{t+1}) \\right] \\right]\n\\]\n\n\\(\\uparrow\\)\nOptimal value function\nValue function of policy pi: “if you land at state x and you follow policy pi what is the expected reward you will accumulate?”\n\\(\\downarrow\\)\n\n\\[\nV^\\pi(\\mathbf{x}_t) = \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi(\\mathbf{a}|\\mathbf{x}_t)} \\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^\\pi(\\mathbf{x}_{t+1}) \\right] \\right]\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-6",
    "href": "lecs/w02/lec02.html#section-6",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\textcolor{red}{\\swarrow \\text{For finite time horizon} \\searrow}\n\\]\n\n\n\n\\[\nJ(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ c(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbb{E}_{e_t} \\left[ J(f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t)) \\right] \\right]\n\\]\n\n\\(\\qquad \\uparrow\\)\nOptimal cost-to-go\n\n\n\\[\nV^*(\\mathbf{x}_t) = \\max_{\\mathbf{a}_t} \\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^*(\\mathbf{x}_{t+1}) \\right] \\right]\n\\]\n\n\\(\\uparrow\\)\nOptimal value\nfunction\nValue function\nof policy pi\n\\(\\downarrow\\)\n\n\\[\nV^\\pi(\\mathbf{x}_t) = \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi(\\mathbf{a}|\\mathbf{x}_t)} \\overbrace{\\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^\\pi(\\mathbf{x}_{t+1}) \\right] \\right]}^{Q^{\\pi} (x_t, a_t)}\n\\]\n\n\nState-action value function of policy pi:\n“if you land at state x, and you commit\nto first execute action a, and then follow\npolicy pi how much reward will you\naccumulate?”"
  },
  {
    "objectID": "lecs/w02/lec02.html#section-7",
    "href": "lecs/w02/lec02.html#section-7",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "\\[\n\\textcolor{red}{\\swarrow \\text{For finite time horizon} \\searrow}\n\\]\n\n\n\n\\[\nJ(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ c(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbb{E}_{e_t} \\left[ J(f_t(\\mathbf{x}_t, \\mathbf{u}_t, e_t)) \\right] \\right]\n\\]\n\n\\(\\qquad \\uparrow\\)\nOptimal cost-to-go\n\n\n\\[\nV^*(\\mathbf{x}_t) = \\max_{\\mathbf{a}_t} \\underbrace{\\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^*(\\mathbf{x}_{t+1}) \\right] \\right]}_{Q^*(x_t, a_t)}\n\\]\n\n\\(\\uparrow\\)\nOptimal value\nfunction\n\nOptimal state-action\nvalue function\n\nValue function\nof policy pi\n\\(\\downarrow\\)\n\n\\[\nV^\\pi(\\mathbf{x}_t) = \\mathbb{E}_{\\mathbf{a}_t \\sim \\pi(\\mathbf{a}|\\mathbf{x}_t)} \\overbrace{\\left[ r(\\mathbf{x}_t, \\mathbf{a}_t) + \\mathbb{E}_{\\mathbf{x}_{t+1} \\sim p(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)} \\left[ V^\\pi(\\mathbf{x}_{t+1}) \\right] \\right]}^{Q^{\\pi} (x_t, a_t)}\n\\]\n\n\nState-action value\nfunction of policy pi"
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-1",
    "href": "lecs/w02/lec02.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Intro to Control & Reinforcement Learning\n• Linear Quadratic Regulator (LQR)\n• Iterative LQR\n• Model Predictive Control\n• Learning dynamics and model-based RL\n\n\n\n\n\nAcknowledgments\nToday’s slides have been influenced by: Pieter Abbeel (ECE287), Sergey Levine (DeepRL), Ben Recht (ICML’18), Emo Todorov, Zico Kolter"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-you-can-do-with-variants-of-lqr-control",
    "href": "lecs/w02/lec02.html#what-you-can-do-with-variants-of-lqr-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What you can do with (variants of) LQR control",
    "text": "What you can do with (variants of) LQR control"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-you-can-do-with-variants-of-lqr-control-1",
    "href": "lecs/w02/lec02.html#what-you-can-do-with-variants-of-lqr-control-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What you can do with (variants of) LQR control",
    "text": "What you can do with (variants of) LQR control\n\nPieter Abbeel, Helicopter Aerobatics"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-assumptions",
    "href": "lecs/w02/lec02.html#lqr-assumptions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR: assumptions",
    "text": "LQR: assumptions\n• You know the dynamics model of the system\n• It is linear: \\(\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t\\) \n\n\n\n\\(\\qquad \\qquad \\qquad \\qquad \\uparrow\\)\nState at the next time step\n\n\\[\n\\mathbb{R}^d\n\\]\n\\[\nA \\in \\mathbb{R}^{d \\times d}\n\\]\n\n\n\\(\\qquad \\uparrow\\)\nControl / command / action applied to the system\n\n\\[\n\\mathbb{R}^k\n\\]\n\\[\nB \\in \\mathbb{R}^{d \\times k}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#which-systems-are-linear",
    "href": "lecs/w02/lec02.html#which-systems-are-linear",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Which systems are linear?",
    "text": "Which systems are linear?\n\n\n✓ • Omnidirectional robot\n\n\n\\[\n\\begin{align}\nx_{t+1} &= x_t + v_x(t)\\delta t \\\\\ny_{t+1} &= y_t + v_y(t)\\delta t \\\\\n\\theta_{t+1} &= \\theta_t + \\omega_z(t)\\delta t\n\\end{align}\n\\quad \\Rightarrow \\quad\n\\]\n\n\\[\n\\mathbf{x}_{t+1} = I\\mathbf{x}_t + \\delta t I \\mathbf{u}_t\n\\] \\[\n\\begin{align}\nA &= I \\\\\nB &= \\delta t I\n\\end{align}\n\\]\n\n\n\n\n\nX • Simple Car\n\n\n\n\\[\n\\begin{align}\nx_{t+1} &= x_t + v_x(t)\\cos(\\theta_t)\\delta t \\\\\ny_{t+1} &= y_t + v_x(t)\\sin(\\theta_t)\\delta t \\\\\n\\theta_{t+1} &= \\theta_t + \\omega_z\\delta t\n\\end{align}\n\\quad \\Rightarrow \\quad\n\\]\n\n\n\n\n\\[\n\\mathbf{x}_{t+1} = I\\mathbf{x}_t + \\begin{bmatrix}\n\\delta t\\cos(\\theta_t) & 0 & 0 \\\\\n0 & \\delta t\\sin(\\theta_t) & 0 \\\\\n0 & 0 & \\delta t\n\\end{bmatrix} \\mathbf{u}_t\n\\] \\[\n\\begin{align}\nA &= I \\\\\nB &= B(x_t) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#the-goal-of-lqr",
    "href": "lecs/w02/lec02.html#the-goal-of-lqr",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The goal of LQR",
    "text": "The goal of LQR\n\nIf we want to stabilize around \\(x^*\\) then let \\(x\\) – \\(x^*\\) be the state\n\\(\\downarrow\\)\n\n\n• Stabilize the system around state \\(x_t = 0\\) with control \\(u_t = 0\\)\n\n• Then \\(x_{t+1} = 0\\) and the system will remain at zero forever"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-assumptions-1",
    "href": "lecs/w02/lec02.html#lqr-assumptions-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR: assumptions",
    "text": "LQR: assumptions\n• You know the dynamics model of the system\n• It is linear: \\(\\mathbf{x}_{t+1} = Ax_t + Bu_t\\)\n• There is an instantaneous cost associated with being at state \\(x_t\\) and taking the action : \\(\\mathbf{u}_t: c(\\mathbf{x}_t, \\mathbf{u}_t) = \\mathbf{x}_t^T Q \\mathbf{x}_t + \\mathbf{u}_t^T R \\mathbf{u}_t\\)\n\n\n\\(\\color{red}\\uparrow\\)\nQuadratic state cost: Penalizes deviation from the zero vector\n\n\n\\(\\color{red}\\nwarrow\\)\nQuadratic control cost: Penalizes high control signals\n\n\n\n\n\n\n\n\\(\\color{red}\\uparrow\\)\n\n\\(\\color{red}\\nearrow\\)\n\n\n\n\nSquare matrices Q and R must be positive definite:\n\\(Q = Q^T\\) and \\(\\forall x, x^T Q x &gt; 0\\)\n\\(R = R^T\\) and \\(\\forall u, u^T R u &gt; 0\\)\ni.e. positive cost for ANY nonzero state and control vector"
  },
  {
    "objectID": "lecs/w02/lec02.html#finite-horizon-lqr",
    "href": "lecs/w02/lec02.html#finite-horizon-lqr",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finite-Horizon LQR",
    "text": "Finite-Horizon LQR\n• Idea: finding controls is an optimization problem\n• Compute the control variables that minimize the cumulative cost\n\n\n\\[\n\\begin{align}\nu_0^*, \\ldots, u_{N-1}^* &= argmin_{u_0, \\ldots, u_N} \\sum_{t=0}^{N} c(\\mathbf{x}_t, \\mathbf{u}_t) \\\\\n&\\text{s.t.} \\\\\n\\mathbf{x}_1 &= A\\mathbf{x}_0 + B\\mathbf{u}_0 \\\\\n\\mathbf{x}_2 &= A\\mathbf{x}_1 + B\\mathbf{u}_1 \\\\\n&\\vdots \\\\\n\\mathbf{x}_N &= A\\mathbf{x}_{N-1} + B\\mathbf{u}_{N-1}\n\\end{align}\n\\]\n\nWe could solve this as a constrained nonlinear optimization problem. But, there is a better way: we can find a closed-form solution.\n\n\nOpen-loop plan!\nGiven first state compute action sequence"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n• Let \\(J_n(x)\\) denote the cumulative cost-to-go starting from state x and moving for n time steps.\n• I.e. cumulative future cost from now till n more steps\n• \\(J_0(x) = x^TQx\\) is the terminal cost of ending up at state x, with no actions left to perform. Recall that \\(c(x, u) = x^TQx + \\cancel{\\mathbf{u}^T R \\mathbf{u}}\\)\n\nQ: What is the optimal cumulative cost-to-go function with 1 time step left?"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-1",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\n\\(J_0(x) = x^TQx\\)\n\nFor notational convenience later on\n\n\n\n\n\\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\min_{\\mathbf{u}} \\underbrace{[\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + J_0(A\\mathbf{x} + B\\mathbf{u})]}_{\\color{red}\\textbf{In RL this would be the state-action value function}} \\\\\n\\end{align}\\]\nBellman Update\nDynamic Programming Value Iteration"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-2",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\\(J_0(x) = x^TQx\\)\n\\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\min_{\\mathbf{u}} [\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + J_0(A\\mathbf{x} + B\\mathbf{u})] \\\\\n&= \\min_{\\mathbf{u}} [\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + (A\\mathbf{x} + B\\mathbf{u})^T P_0 (A\\mathbf{x} + B\\mathbf{u})]\n\\end{align}\\]\n\nQ: How do we optimize a multivariable function with respect to some variables (in our case, the controls)?"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-3",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\\(J_0(x) = x^TQx\\)\n\\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\min_{\\mathbf{u}} [\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + J_0(A\\mathbf{x} + B\\mathbf{u})] \\\\\n&= \\min_{\\mathbf{u}} [\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + (A\\mathbf{x} + B\\mathbf{u})^T P_0 (A\\mathbf{x} + B\\mathbf{u})] \\\\\n&= \\mathbf{x}^T Q \\mathbf{x} + \\min_{\\mathbf{u}} [\\mathbf{u}^T R \\mathbf{u} + (A\\mathbf{x} + B\\mathbf{u})^T P_0 (A\\mathbf{x} + B\\mathbf{u})]\n\\end{align}\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-4",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\\(J_0(x) = x^TQx\\)\n\\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\min_{\\mathbf{u}} [\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + J_0(A\\mathbf{x} + B\\mathbf{u})] \\\\\n&= \\min_{\\mathbf{u}} [\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} + (A\\mathbf{x} + B\\mathbf{u})^T P_0 (A\\mathbf{x} + B\\mathbf{u})] \\\\\n&= \\mathbf{x}^T Q \\mathbf{x} + \\min_{\\mathbf{u}} [\\mathbf{u}^T R \\mathbf{u} + (A\\mathbf{x} + B\\mathbf{u})^T P_0 (A\\mathbf{x} + B\\mathbf{u})] \\\\\n&= \\mathbf{x}^T Q \\mathbf{x} + \\mathbf{x}^T A^T P_0 A\\mathbf{x} + \\min_{\\mathbf{u}} [\\mathbf{u}^T R \\mathbf{u} + 2\\mathbf{u}^T B^T P_0 A\\mathbf{x} + \\mathbf{u}^T B^T P_0 B\\mathbf{u}]\n\\end{align}\\]\n\n\n\\(\\uparrow\\)\nQuadratic\nterm in u\n\\(\\uparrow\\)\nLinear\nterm in u\n\\(\\uparrow\\)\nQuadratic\nterm in u\n\nA: Take the partial derivative w.r.t. controls and set it to zero. That will give you a critical point."
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-5",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\mathbf{x}^T Q \\mathbf{x} + \\mathbf{x}^T A^T P_0 A\\mathbf{x} + \\min_{\\mathbf{u}} [\\mathbf{u}^T R \\mathbf{u} + 2\\mathbf{u}^T B^T P_0 A\\mathbf{x} + \\mathbf{u}^T B^T P_0 B\\mathbf{u}]\n\\end{align}\\]\n\n\nThe minimum is attained at:\n\\(2Ru + 2B^T P_0 Ax + 2B^T P_0 Bu = \\mathbf{0}\\) \\((R + B^T P_0 B)\\mathbf{u} = -B^T P_0 Ax\\)\n\\(\\color{red}\\uparrow\\)\nQ: Is this matrix invertible? Recall R, Po are positive definite matrices.\n\n\\(R + B^T P_0 B\\) is positive definite, so it is invertible\n\n\nFrom calculus/algebra:\n\\(\\frac{\\partial}{\\partial u}(u^T M u) = (M + M^T)u\\)\n\\(\\frac{\\partial}{\\partial u}(u^T M b) = M^T b\\)\nIf M is symmetric:\n\\(\\frac{\\partial}{\\partial u}(u^T M u) = 2Mu\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-6",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-6",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\mathbf{x}^T Q \\mathbf{x} + \\mathbf{x}^T A^T P_0 A\\mathbf{x} + \\min_{\\mathbf{u}} [\\mathbf{u}^T R \\mathbf{u} + 2\\mathbf{u}^T B^T P_0 A\\mathbf{x} + \\mathbf{u}^T B^T P_0 B\\mathbf{u}]\n\\end{align}\\]\n\n\nThe minimum is attained at:\n\\(2Ru + 2B^T P_0 Ax + 2B^T P_0 Bu = \\mathbf{0}\\) \\((R + B^T P_0 B)\\mathbf{u} = -B^T P_0 Ax\\)\nSo, the optimal control for the last time step is:\n\\(\\mathbf{u} = -(R + B^T P_0 B)^{-1} B^T P_0 Ax\\)\n\\(\\mathbf{u} = K_1 \\mathbf{x}\\)\n\nLinear controller in terms of the state\n\nWe computed the location of the minimum. Now, plug it back in and compute the minimum value"
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-7",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-7",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion\n\\(J_0(x) = x^TQx\\) \\[\\begin{align}\nJ_1(\\mathbf{x}) &= \\mathbf{x}^T Q \\mathbf{x} + \\mathbf{x}^T A^T P_0 A\\mathbf{x} + \\min_{\\mathbf{u}} [\\mathbf{u}^T R \\mathbf{u} + 2\\mathbf{u}^T B^T P_0 A\\mathbf{x} + \\mathbf{u}^T B^T P_0 B\\mathbf{u}] \\\\\n&= \\mathbf{x}^T \\underbrace{Q + K_1^T R K_1 + (A + B K_1)^T P_0 (A + B K_1)}_{P_1} \\mathbf{x}\n\\end{align}\\]\n\nQ: Why is this a big deal?\nA: The cost-to-go function remains quadratic after the first recursive step."
  },
  {
    "objectID": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-8",
    "href": "lecs/w02/lec02.html#finding-the-lqr-controller-in-closed-form-by-recursion-8",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finding the LQR controller in closed-form by recursion",
    "text": "Finding the LQR controller in closed-form by recursion"
  },
  {
    "objectID": "lecs/w02/lec02.html#finite-horizon-lqr-algorithm-summary",
    "href": "lecs/w02/lec02.html#finite-horizon-lqr-algorithm-summary",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Finite-Horizon LQR: algorithm summary",
    "text": "Finite-Horizon LQR: algorithm summary\n\n\n\\(P_0 = Q\\)\n// n is the # of steps left\n\nPotential problem for states of dimension &gt;&gt; 100:\nMatrix inversion is expensive: O(k^2.3) for the best known algorithm and O(k^3) for Gaussian Elimination.\n\\(\\swarrow\\)\n\n\n\nfor n = 1…N\n\\(K_n = -(R + B^T P_{n-1} B)^{-1} B^T P_{n-1} A\\)\n\\(P_n = Q + K_n^T R K_n + (A + B K_n)^T P_{n-1} (A + B K_n)\\)\n\nOne pass backward in time:\nMatrix gains are precomputed based on the dynamics and the instantaneous cost\n\n\n\nOptimal control for time t = N – n is \\(\\mathbf{u}_t = K_t \\mathbf{x}_t\\) with cost-to-go \\(J_t(\\mathbf{x}) = \\mathbf{x}^T P_t \\mathbf{x}\\) where the states are predicted forward in time according to linear dynamics\n\nOne pass forward in time:\nPredict states, compute controls and cost-to-go"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-general-form-of-dynamics-and-cost-functions",
    "href": "lecs/w02/lec02.html#lqr-general-form-of-dynamics-and-cost-functions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR: general form of dynamics and cost functions",
    "text": "LQR: general form of dynamics and cost functions\n\n\nEven though we assumed\n\nwe can also accommodate\n\n\n\n\n\\(\\mathbf{x}_{t+1} = A \\mathbf{x}_t + B \\mathbf{u}_t\\)\n\n\n\\(\\mathbf{x}_{t+1} = A_t \\mathbf{x}_t + B_t \\mathbf{u}_t + \\mathbf{b}_t\\)\n\n\n\n\\(c(\\mathbf{x}_t, \\mathbf{u}_t) = \\mathbf{x}_t^T Q \\mathbf{x}_t + \\mathbf{u}_t^T R \\mathbf{u}_t\\)\n\n\\(c(\\mathbf{x}_t, \\mathbf{u}_t) =\n\\begin{bmatrix}\n\\mathbf{x}_t \\\\\n\\mathbf{u}_t\n\\end{bmatrix}^T\nH_t\n\\begin{bmatrix}\n\\mathbf{x}_t \\\\\n\\mathbf{u}_t\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\mathbf{x}_t \\\\\n\\mathbf{u}_t\n\\end{bmatrix}^T\n\\mathbf{h}_t\\)\n\nbut the form of the computed controls becomes \\(\\mathbf{u}_{t} = K_t \\mathbf{x}_t + \\mathbf{k}_t\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-with-stochastic-dynamics",
    "href": "lecs/w02/lec02.html#lqr-with-stochastic-dynamics",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR with stochastic dynamics",
    "text": "LQR with stochastic dynamics\nAssume \\(\\mathbf{x}_{t+1} = A_t \\mathbf{x}_t + B_t \\mathbf{u}_t + \\mathbf{w}_t\\) and \\(c(\\mathbf{x}_t, \\mathbf{u}_t) = \\mathbf{x}_t^T Q \\mathbf{x}_t + \\mathbf{u}_t^T R \\mathbf{u}_t\\)\n\n\\(\\uparrow\\) zero mean Gaussian\n\nThen the form of the optimal policy is the same as in LQR \\(\\mathbf{u}_{t} = K_t \\mathbf{x}_t + \\mathbf{k}_t\\)\nNo need to change the algorithm, as long as you observe the state at each step (closed-loop policy)\n\nLinear Quadratic Gaussian LQG"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-summary",
    "href": "lecs/w02/lec02.html#lqr-summary",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR summary",
    "text": "LQR summary\n\nAdvantages:\n\nIf system is linear LQR gives the optimal controller that takes the system’s state to 0 (or the desired target state, same thing)\n\n\n\n\nDrawbacks:\n\nLinear dynamics\nHow can you include obstacles or constraints in the specification?\nNot easy to put bounds on control values"
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-2",
    "href": "lecs/w02/lec02.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Intro to Control & Reinforcement Learning\n• Linear Quadratic Regulator (LQR)\n• Iterative LQR\n• Model Predictive Control\n• Learning dynamics and model-based RL"
  },
  {
    "objectID": "lecs/w02/lec02.html#what-happens-in-the-general-nonlinear-case",
    "href": "lecs/w02/lec02.html#what-happens-in-the-general-nonlinear-case",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What happens in the general nonlinear case?",
    "text": "What happens in the general nonlinear case?\n\n\n\\[\nu_0^*, \\ldots, u_{N-1}^* = \\arg\\min_{u_0, \\ldots, u_N} \\sum_{t=0}^{N} c(x_t, u_t)\n\\] \\[\n\\begin{aligned}\n\\mathbf{x}_1 &= f(\\mathbf{x}_0, \\mathbf{u}_0) \\\\\n\\mathbf{x}_2 &= f(\\mathbf{x}_1, \\mathbf{u}_1) \\\\\n&\\vdots \\\\\n\\mathbf{x}_N &= f(\\mathbf{x}_{N-1}, \\mathbf{u}_{N-1})\n\\end{aligned}\n\\]\n\nArbitrary differentiable functions c, f\n\n\nIdea: iteratively approximate solution by solving linearized versions of the problem via LQR"
  },
  {
    "objectID": "lecs/w02/lec02.html#iterative-lqr-ilqr",
    "href": "lecs/w02/lec02.html#iterative-lqr-ilqr",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Iterative LQR (iLQR)",
    "text": "Iterative LQR (iLQR)\nGiven an initial sequence of states \\(\\bar{x}_0, \\ldots, \\bar{x}_N\\) and actions \\(\\bar{u}_0, \\ldots, \\bar{u}_N\\):\nLinearize Dynamics\n\n\\[\nf(x_t, u_t) \\approx \\tilde{f}(\\delta x_t, \\delta u_t) = \\underbrace{f(\\bar{x}_t, \\bar{u}_t)}_{\\mathbf{b}_t} + \\underbrace{\\frac{\\partial f}{\\partial x} (\\bar{x}_t, \\bar{u}_t)}_{A_t}\\underbrace{(x_t - \\bar{x}_t)}_{\\delta x_t} + \\underbrace{\\frac{\\partial f}{\\partial u} (\\bar{x}_t, \\bar{u}_t)}_{B_t}\\underbrace{(u_t - \\bar{u}_t)}_{\\delta \\mathbf{u}_t}\n\\]\n\n\nTaylor Expand Cost\n\n\\[\nc(x_t, u_t) \\approx \\tilde{c}(\\delta x_t, \\delta u_t) = c(\\bar{x}_t, \\bar{u}_t) + \\underbrace{\\nabla_{x,u} c(\\bar{x}_t, \\bar{u}_t)}_{\\mathbf{h}_t}\n\\begin{bmatrix}\nx_t - \\bar{x}_t \\\\\nu_t - \\bar{u}_t\n\\end{bmatrix}\n+ \\frac{1}{2}\n\\begin{bmatrix}\nx_t - \\bar{x}_t \\\\\nu_t - \\bar{u}_t\n\\end{bmatrix}^T\n\\underbrace{\\nabla^2_{x,u} c(\\bar{x}_t, \\bar{u}_t)}_{H_t}\n\\begin{bmatrix}\nx_t - \\bar{x}_t \\\\\nu_t - \\bar{u}_t\n\\end{bmatrix}\n\\]\n\n\n\nUse LQR Backward Pass on the approximate dynamics \\(f(\\delta x_t, \\delta u_t)\\) and cost \\(\\tilde{c}(\\delta x_t, \\delta u_t)\\)\n\n\nDo a forward pass to get \\(\\delta u_t\\) and \\(\\delta x_t\\) and update state and action sequence and \\(\\bar{x}_0, \\ldots, \\bar{x}_N\\) and \\(\\bar{u}_0, \\ldots, \\bar{u}_N\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#iterative-lqr-convergence-tricks",
    "href": "lecs/w02/lec02.html#iterative-lqr-convergence-tricks",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Iterative LQR: convergence & tricks",
    "text": "Iterative LQR: convergence & tricks\n• New state and action sequence in iLQR is not guaranteed to be close to the linearization point (so linear approximation might be bad)\n• Trick: try to penalize magnitude of \\(\\delta u_t\\) and \\(\\delta x_t\\)\nReplace old LQR linearized cost with \\((1 - \\alpha) \\, \\tilde{c}(\\delta \\mathbf{x}_t, \\delta \\mathbf{u}_t) + \\alpha \\left( \\| \\delta \\mathbf{x}_t \\|^2 + \\| \\delta \\mathbf{u}_t \\|^2 \\right)\\)\n• Problem: Can get stuck in local optima, need to initialize well\n• Problem: Hessian might not be positive definite"
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-3",
    "href": "lecs/w02/lec02.html#todays-agenda-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Intro to Control & Reinforcement Learning\n• Linear Quadratic Regulator (LQR)\n• Iterative LQR\n• Model Predictive Control\n• Learning dynamics and model-based RL"
  },
  {
    "objectID": "lecs/w02/lec02.html#open-loop-vs.-closed-loop",
    "href": "lecs/w02/lec02.html#open-loop-vs.-closed-loop",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Open loop vs. closed loop",
    "text": "Open loop vs. closed loop\n• The instances of LQR and iLQR that we saw were open-loop\n\n• Commands are executed in sequence, without feedback\n\n• Idea: what if we throw away all commands except the first\n\n• We can execute the first command, and then replan Takes into account the changing state"
  },
  {
    "objectID": "lecs/w02/lec02.html#model-predictive-control",
    "href": "lecs/w02/lec02.html#model-predictive-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model Predictive Control",
    "text": "Model Predictive Control\nwhile True:\n\nobserve the current state \\(x_0\\)\nrun LQR/iLQR or LQG/iLQG or other planner to get \\({u}_0, \\ldots, {u}_{N-1}\\)\nExecute \\(u_0\\)\n\n\n\nPossible speedups:\n\nDon’t plan too far ahead with LQR\nExecute more than one planned action\nWarm starts and initialization\nUse faster / custom optimizer (e.g. CPLEX, sequential quadratic programming)"
  },
  {
    "objectID": "lecs/w02/lec02.html#online-trajectory-optimization-mpc",
    "href": "lecs/w02/lec02.html#online-trajectory-optimization-mpc",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Online trajectory optimization / MPC",
    "text": "Online trajectory optimization / MPC"
  },
  {
    "objectID": "lecs/w02/lec02.html#online-trajectory-optimization-mpc-1",
    "href": "lecs/w02/lec02.html#online-trajectory-optimization-mpc-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Online trajectory optimization / MPC",
    "text": "Online trajectory optimization / MPC"
  },
  {
    "objectID": "lecs/w02/lec02.html#online-trajectory-optimization-mpc-2",
    "href": "lecs/w02/lec02.html#online-trajectory-optimization-mpc-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Online trajectory optimization / MPC",
    "text": "Online trajectory optimization / MPC\nTest 3: Dynamic Maneuvers"
  },
  {
    "objectID": "lecs/w02/lec02.html#online-trajectory-optimization-mpc-3",
    "href": "lecs/w02/lec02.html#online-trajectory-optimization-mpc-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Online trajectory optimization / MPC",
    "text": "Online trajectory optimization / MPC"
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-4",
    "href": "lecs/w02/lec02.html#todays-agenda-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Intro to Control & Reinforcement Learning\n• Linear Quadratic Regulator (LQR)\n• Iterative LQR\n• Model Predictive Control\n• Learning dynamics and model-based RL"
  },
  {
    "objectID": "lecs/w02/lec02.html#learning-a-dynamics-model",
    "href": "lecs/w02/lec02.html#learning-a-dynamics-model",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning a dynamics model",
    "text": "Learning a dynamics model\n\n\n\n\nIdea #1: Collect dataset \\(D = \\{(x_t, u_t, x_{t+1})\\}\\)\ndo supervised learning to minimize \\(\\sum_{t} \\left\\| f_{\\theta}(x_t, u_t) - x_{t+1} \\right\\|^2\\)\nand then use the learned model for planning\n\nPossibly a better idea: instead of minimizing single-step prediction errors, minimize multi-step errors.\nSee “Improving Multi-step Prediction of Learned Time Series Models” by Venkatraman, Hebert, Bagnell\n\n\nPossibly a better idea: instead of predicting next state predict next change in state.\nSee “PILCO: A Model-Based and Data-Efficient Approach to Policy Search” by Deisenroth, Rasmussen"
  },
  {
    "objectID": "lecs/w02/lec02.html#model-based-rl",
    "href": "lecs/w02/lec02.html#model-based-rl",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model-based RL",
    "text": "Model-based RL\nCollect initial dataset \\(D = \\{(x_t, u_t, x_{t+1})\\}\\)\n\nFit dynamics model \\(f_{\\theta}(x_t, u_t)\\)\nPlan through \\(f_{\\theta}(x_t, u_t)\\) to get actions\nExecute first action, observe new state \\(x_{t+1}\\)\nAppend \\((x_t, u_t, x_{t+1})\\) to \\(D\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#todays-agenda-5",
    "href": "lecs/w02/lec02.html#todays-agenda-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n\n• Intro to Control & Reinforcement Learning\n• Linear Quadratic Regulator (LQR)\n• Iterative LQR\n• Model Predictive Control\n• Learning dynamics and model-based RL\n• Appendix"
  },
  {
    "objectID": "lecs/w02/lec02.html#appendix-1-optional-reading-lqr-extensions-time-varying-systems",
    "href": "lecs/w02/lec02.html#appendix-1-optional-reading-lqr-extensions-time-varying-systems",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix #1 (optional reading) LQR extensions: time-varying systems",
    "text": "Appendix #1 (optional reading) LQR extensions: time-varying systems\n• What can we do when \\(x_{t+1} = A_t x_t + B_t u_t\\) and \\(c(x_t, u_t) = x_t^T Q x_t + u_t^T R u_t\\) ?\n• Turns out, the proof and the algorithm are almost the same\n\\(P_0 = Q_N\\)\n// n is the # of steps left\nfor n = 1…N\n\\(K_n = -\\left(R_{N-n} + B_{N-n}^T P_{n-1} B_{N-n}\\right)^{-1} B_{N-n}^T P_{n-1} A_{N-n}\\)\n\n\\(P_n = Q_{N-n} + K_n^T R_{N-n} K_n + (A_{N-n} + B_{N-n} K_n)^T P_{n-1} (A_{N-n} + B_{N-n} K_n)\\)\nOptimal controller for n-step horizon is \\(u_n = K_n x_n\\) with cost-to-go \\(J_n(x) = x^T P_n x\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#appendix-2-optional-reading-why-not-use-pid-control",
    "href": "lecs/w02/lec02.html#appendix-2-optional-reading-why-not-use-pid-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix #2 (optional reading) Why not use PID control?",
    "text": "Appendix #2 (optional reading) Why not use PID control?\n• We could, but:\n• The gains for PID are good for a small region of state-space.\n\n• System reaches a state outside this set → becomes unstable\n\n\n• PID has no formal guarantees on the size of the set\n\n\n\n• We would need to tune PID gains for every control variable.\n\n\n\n• If the state vector has multiple dimensions it becomes harder to tune every control variable in isolation. Need to consider interactions and correlations.\n\n\n• We would need to tune PID gains for different regions of the state-space and guarantee smooth gain transitions\n\n\n\n• This is called gain scheduling, and it takes a lot of effort and time\n\n\nLQR addresses these problems"
  },
  {
    "objectID": "lecs/w02/lec02.html#appendix-3-optional-reading-examples-of-models-and-solutions-with-lqr",
    "href": "lecs/w02/lec02.html#appendix-3-optional-reading-examples-of-models-and-solutions-with-lqr",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix #3 (optional reading) Examples of models and solutions with LQR",
    "text": "Appendix #3 (optional reading) Examples of models and solutions with LQR"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n• Similar to double integrator dynamical system, but with friction:\n\\[\n\\underset{\\color{red}\\text{Force applied to the vehicle}}{\\underline{m\\ddot{p}}} = \\underset{\\color{red}\\text{Control applied to the vehicle}}{\\underline{\\mathbf{u}}} - \\underset{\\color{red}\\text{Friction opposed to motion}}{\\underline{\\alpha \\dot{p}}}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-1",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n• Similar to double integrator dynamical system, but with friction:\n\\[\nm\\ddot{p} = \\mathbf{u} - \\alpha \\dot{p}\n\\]\n• Set \\(\\dot{p} = v\\) and then you get:\n\\[\nm\\dot{v} = \\mathbf{u} - \\alpha \\mathbf{v}\n\\]\n\n• We discretize by setting\n\n\\[\n\\frac{\\mathbf{p}_{t+1} - \\mathbf{p}_t}{\\delta t} \\simeq \\mathbf{v}_t\n\\]\n\\[\nm \\frac{\\mathbf{v}_{t+1} - \\mathbf{v}_t}{\\delta t} \\simeq \\mathbf{u}_t - \\alpha \\mathbf{v}_t\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-2",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n\n\\[\n\\frac{\\mathbf{p}_{t+1} - \\mathbf{p}_t}{\\delta t} \\simeq \\mathbf{v}_t\n\\]\n\\[\nm \\frac{\\mathbf{v}_{t+1} - \\mathbf{v}_t}{\\delta t} \\simeq \\mathbf{u}_t - \\alpha \\mathbf{v}_t\n\\]\n\n• Define the state vector \\(\\mathbf{x}_t = \\begin{bmatrix}\\mathbf{p}_t \\\\\\mathbf{v}_t\\end{bmatrix}\\)\n\nQ: How can we express this as a linear system?\n\n\n\\[\n\\mathbf{x}_{t+1} =\n\\begin{bmatrix}\n\\mathbf{p}_{t+1} \\\\\n\\mathbf{v}_{t+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{p}_t + \\delta t \\mathbf{v}_t \\\\\n\\mathbf{v}_t + \\frac{\\delta t}{m} \\mathbf{u}_t - \\frac{\\alpha \\delta t}{m} \\mathbf{v}_t\n\\end{bmatrix}\n=\n\\begin{bmatrix}  \np_t + \\delta t v_t \\\\\nv_t - \\frac{\\alpha \\delta t}{m} \\mathbf{v}_t  \n\\end{bmatrix}\n+\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n\\frac{\\delta t}{m} & 0 \\\\\n0 & \\frac{\\delta t}{m}\n\\end{bmatrix}\n\\mathbf{u}_t\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-3",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n\n\\[\n\\frac{\\mathbf{p}_{t+1} - \\mathbf{p}_t}{\\delta t} \\simeq \\mathbf{v}_t\n\\]\n\\[\nm \\frac{\\mathbf{v}_{t+1} - \\mathbf{v}_t}{\\delta t} \\simeq \\mathbf{u}_t - \\alpha \\mathbf{v}_t\n\\]\n\n• Define the state vector \\(\\mathbf{x}_t = \\begin{bmatrix}\\mathbf{p}_t \\\\\\mathbf{v}_t\\end{bmatrix}\\)\n\n\\[\n\\mathbf{x}_{t+1} = \\begin{bmatrix} p_{t+1} \\\\ v_{t+1} \\end{bmatrix} = \\begin{bmatrix} p_t + \\delta t v_t \\\\ v_t + \\frac{\\delta t}{m} u_t - \\frac{\\alpha \\delta t}{m} v_t \\end{bmatrix} = \\underbrace{\\begin{bmatrix} 1 & 0 & \\delta t & 0 \\\\ 0 & 1 & 0 & \\delta t \\\\ 0 & 0 & 1 - \\alpha \\delta t/m & 0 \\\\ 0 & 0 & 0 & 1 - \\alpha \\delta t/m \\end{bmatrix}}_{\\color{red}A} \\mathbf{x}_t + \\underbrace{\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\frac{\\delta t}{m} & 0 \\\\ 0 & \\frac{\\delta t}{m} \\end{bmatrix}}_{\\color{red}B}\\mathbf{u}_t\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-4",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n• Define the state vector \\(\\mathbf{x}_t = \\begin{bmatrix}\\mathbf{p}_t \\\\\\mathbf{v}_t\\end{bmatrix}\\)\n\n\\[\n\\mathbf{x}_{t+1} = \\begin{bmatrix} p_{t+1} \\\\ v_{t+1} \\end{bmatrix} = \\begin{bmatrix} p_t + \\delta t v_t \\\\ v_t + \\frac{\\delta t}{m} u_t - \\frac{\\alpha \\delta t}{m} v_t \\end{bmatrix} = \\underbrace{\\begin{bmatrix} 1 & 0 & \\delta t & 0 \\\\ 0 & 1 & 0 & \\delta t \\\\ 0 & 0 & 1 - \\alpha \\delta t/m & 0 \\\\ 0 & 0 & 0 & 1 - \\alpha \\delta t/m \\end{bmatrix}}_{\\color{red}A} \\mathbf{x}_t + \\underbrace{\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\frac{\\delta t}{m} & 0 \\\\ 0 & \\frac{\\delta t}{m} \\end{bmatrix}}_{\\color{red}B}\\mathbf{u}_t\n\\]\n\n\n\n• Define the instantaneous cost function\n\n\\[\n\\begin{aligned}\nc(\\mathbf{x}, \\mathbf{u}) = \\mathbf{x}^T Q \\mathbf{x} + \\mathbf{u}^T R \\mathbf{u} \\\\\n= \\mathbf{x}^T \\mathbf{x} + \\rho \\mathbf{u}^T \\mathbf{u} \\\\\n= \\|\\mathbf{x}\\|^2 + \\rho \\|\\mathbf{u}\\|^2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-5",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n\n\nWith initial state \\(\\mathbf{x}_0 = \\begin{bmatrix}10 \\\\ 30\\\\ 10\\\\ -5\\end{bmatrix}\\)\nInstantaneous cost function\n\\(c(\\mathbf{x}, \\mathbf{u}) = \\|\\mathbf{x}\\|^2 + 100 \\|\\mathbf{u}\\|^2\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-6",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-6",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n\n\nWith initial state \\(\\mathbf{x}_0 = \\begin{bmatrix}10 \\\\ 30\\\\ 10\\\\ -5\\end{bmatrix}\\)\nInstantaneous cost function\n\\(c(\\mathbf{x}, \\mathbf{u}) = \\|\\mathbf{x}\\|^2 + 100 \\|\\mathbf{u}\\|^2\\)\n\n\n\nNotice how the controls tend to zero.\nIt’s because the state tends to zero as well.\n\n\nAlso note that in the current LQR framework,\nwe have not included hard constraints on the controls,\ni.e. upper or lower bounds. We only penalize large norm for controls."
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-7",
    "href": "lecs/w02/lec02.html#lqr-example-1-omnidirectional-vehicle-with-friction-7",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #1: omnidirectional vehicle with friction",
    "text": "LQR example #1: omnidirectional vehicle with friction\n\n\nWith initial state \\(\\mathbf{x}_0 = \\begin{bmatrix}10 \\\\ 30\\\\ 10\\\\ -5\\end{bmatrix}\\)\nInstantaneous cost function\n\\(c(\\mathbf{x}, \\mathbf{u}) = \\|\\mathbf{x}\\|^2 + 100 \\|\\mathbf{u}\\|^2\\)\n\n\n\nNotice how the controls tend to zero."
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle",
    "href": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #2: trajectory following for omnidirectional vehicle",
    "text": "LQR example #2: trajectory following for omnidirectional vehicle"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-1",
    "href": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #2: trajectory following for omnidirectional vehicle",
    "text": "LQR example #2: trajectory following for omnidirectional vehicle\n\\[\n\\mathbf{x}_{t+1} = \\begin{bmatrix} p_{t+1} \\\\ v_{t+1} \\end{bmatrix} = \\underbrace{\\begin{bmatrix} 1 & 0 & \\delta t & 0 \\\\ 0 & 1 & 0 & \\delta t \\\\ 0 & 0 & 1 - \\alpha \\delta t/m & 0 \\\\ 0 & 0 & 0 & 1 - \\alpha \\delta t/m \\end{bmatrix}}_{\\color{red}A} \\mathbf{x}_t + \\underbrace{\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\frac{\\delta t}{m} & 0 \\\\ 0 & \\frac{\\delta t}{m} \\end{bmatrix}}_{\\color{red}B}\\mathbf{u}_t\n\\]\nWe are given a desired trajectory \\(\\mathbf{p}_0^*, \\mathbf{p}_1^*, \\ldots, \\mathbf{p}_T^*\\)\nInstantaneous cost \\(c(\\mathbf{x}_t, \\mathbf{u}_t) = (\\mathbf{p}_t - \\mathbf{p}_t^*)^T Q (\\mathbf{p}_t - \\mathbf{p}_t^*) + \\mathbf{u}_t^T R \\mathbf{u}_t\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-2",
    "href": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #2: trajectory following for omnidirectional vehicle",
    "text": "LQR example #2: trajectory following for omnidirectional vehicle\n\n\\[\n\\mathbf{x}_{t+1} = \\begin{bmatrix} p_{t+1} \\\\ v_{t+1} \\end{bmatrix} = \\underbrace{\\begin{bmatrix} 1 & 0 & \\delta t & 0 \\\\ 0 & 1 & 0 & \\delta t \\\\ 0 & 0 & 1 - \\alpha \\delta t/m & 0 \\\\ 0 & 0 & 0 & 1 - \\alpha \\delta t/m \\end{bmatrix}}_{\\color{red}A} \\mathbf{x}_t + \\underbrace{\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\frac{\\delta t}{m} & 0 \\\\ 0 & \\frac{\\delta t}{m} \\end{bmatrix}}_{\\color{red}B}\\mathbf{u}_t\n\\]\n\n\n\nDefine \\[\\begin{align}\n\\tilde{\\mathbf{x}}_{t+1} &= \\mathbf{x}_{t+1} - \\mathbf{x}_{t+1}^* \\\\\n&= A\\mathbf{x}_t + B\\mathbf{u}_t - \\mathbf{x}_{t+1}^* \\\\\n&= A\\tilde{\\mathbf{x}}_t + B\\mathbf{u}_t \\color{red}\\underline{- \\mathbf{x}_{t+1}^* + A\\mathbf{x}_t^*}\n\\end{align}\n\\]\n\nWe want \\(\\bar{x}_{t+1} = \\bar{A} \\bar{x}_t + \\bar{B} u_t\\)\n\n\\(\\leftarrow\\) Need to get rid of this additive term\n\n\n\n\nRedefine state: \\(z_{t+1} = \\begin{bmatrix} \\bar{x}_{t+1} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} A & c \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} \\bar{x}_t \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} B \\\\ 0 \\end{bmatrix} u_t = \\bar{A} z_t + \\bar{B} u_t\\)\n\n\n\n\nRedefine cost function: \\(c(z_t, u_t) = z_t^T \\bar{Q}_t z_t + u_t^T R u_t\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-3",
    "href": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #2: trajectory following for omnidirectional vehicle",
    "text": "LQR example #2: trajectory following for omnidirectional vehicle\n\n\nWith initial state \\(\\mathbf{z}_0 = \\begin{bmatrix}10 \\\\ 30\\\\ 0\\\\ 0\\\\ 1\\end{bmatrix}\\)\nInstantaneous cost function\n\\(c(\\mathbf{z}, \\mathbf{u}) = \\|\\mathbf{z}\\|^2 + 100 \\|\\mathbf{u}\\|^2\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-4",
    "href": "lecs/w02/lec02.html#lqr-example-2-trajectory-following-for-omnidirectional-vehicle-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR example #2: trajectory following for omnidirectional vehicle",
    "text": "LQR example #2: trajectory following for omnidirectional vehicle\n\n\nWith initial state \\(\\mathbf{z}_0 = \\begin{bmatrix}10 \\\\ 30\\\\ 0\\\\ 0\\\\ 1\\end{bmatrix}\\)\nInstantaneous cost function\n\\(c(\\mathbf{z}, \\mathbf{u}) = \\|\\mathbf{z}\\|^2 + 100 \\|\\mathbf{u}\\|^2\\)"
  },
  {
    "objectID": "lecs/w02/lec02.html#appendix-4-optional-reading-lqr-extensions-trajectory-following",
    "href": "lecs/w02/lec02.html#appendix-4-optional-reading-lqr-extensions-trajectory-following",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix #4 (optional reading) LQR extensions: trajectory following",
    "text": "Appendix #4 (optional reading) LQR extensions: trajectory following\n• You are given a reference trajectory (not just path, but states and times, or states and controls) that needs to be approximated\n\n\n\\(\\mathbf{x}_0^*, \\mathbf{x}_1^*, \\ldots, \\mathbf{x}_N^*\\)\n\n\\(\\mathbf{u}_0^*, \\mathbf{u}_1^*, \\ldots, \\mathbf{u}_N^*\\)\n\nLinearize the nonlinear dynamics \\(x_{t+1} = f(x_t, u_t)\\) around the reference point \\((x^*_t, u^*_t)\\):\n\\(x_{t+1} \\simeq f(x^*_t, u^*_t) + \\frac{\\partial f}{\\partial x}(x^*_t, u^*_t)(x_t - x^*_t) + \\frac{\\partial f}{\\partial u}(x^*_t, u^*_t)(u_t - u^*_t)\\)\n\n\n\\(\\bar{x}_{t+1} \\simeq A_t \\bar{x}_t + B_t \\bar{u}_t\\)\n\n\\(c(x_t, u_t) = \\bar{x}_t^T Q \\bar{x}_t + \\bar{u}_t^T R \\bar{u}_t\\)\n\nwhere\n\n\\(\\bar{x}_t = x_t - x_t^*\\)\n\n\\(\\bar{u}_t = u_t - u_t^*\\)\n\nTrajectory following can be implemented as a time-varying LQR approximation. Not always clear if this is the best way though."
  },
  {
    "objectID": "lecs/w02/lec02.html#appendix-5-optional-reading-lqr-with-nonlinear-dynamics-quadratic-cost",
    "href": "lecs/w02/lec02.html#appendix-5-optional-reading-lqr-with-nonlinear-dynamics-quadratic-cost",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Appendix #5 (optional reading) LQR with nonlinear dynamics, quadratic cost",
    "text": "Appendix #5 (optional reading) LQR with nonlinear dynamics, quadratic cost"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-variants-nonlinear-dynamics-quadratic-cost",
    "href": "lecs/w02/lec02.html#lqr-variants-nonlinear-dynamics-quadratic-cost",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR variants: nonlinear dynamics, quadratic cost",
    "text": "LQR variants: nonlinear dynamics, quadratic cost\n\nWhat can we do when \\(x_{t+1} = f(x_t, u_t)\\) but the cost is quadratic \\(c(x_t, u_t) = \\bar{x}_t^T Q \\bar{x}_t + \\bar{u}_t^T R \\bar{u}_t\\) ?\nWe want to stabilize the system around state \\(x_t = 0\\)\nBut with nonlinear dynamics we do not know if \\(u_t = 0\\) will keep the system at the zero state.\n\n\n\n\\(\\rightarrow\\) Need to compute \\(u^*\\) such that \\(0_{t+1} = f(0_t, u^*)\\)\n\n\n\n\nTaylor expansion: linearize the nonlinear dynamics around the point \\((0, u^*)\\)\n\\[\n\\mathbf{x}_{t+1} \\simeq f(\\mathbf{0}, \\mathbf{u}^*) + \\underbrace{\\frac{\\partial f}{\\partial \\mathbf{x}}(\\mathbf{0}, \\mathbf{u}^*)(\\mathbf{x}_t - \\mathbf{0})}_{\\mathbf{A}} + \\underbrace{\\frac{\\partial f}{\\partial \\mathbf{u}}(\\mathbf{0}, \\mathbf{u}^*)(\\mathbf{u}_t - \\mathbf{u}^*)}_{\\mathbf{B}}\n\\]\n\n\n\n\n\\(\\mathbf{x}_{t+1} \\simeq A \\mathbf{x}_t + B(\\mathbf{u}_t - \\mathbf{u}^*)\\)\nSolve this via LQR"
  },
  {
    "objectID": "lecs/w02/lec02.html#lqr-examples-code-to-replicate-these-results",
    "href": "lecs/w02/lec02.html#lqr-examples-code-to-replicate-these-results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "LQR examples: code to replicate these results",
    "text": "LQR examples: code to replicate these results\n• https://github.com/florianshkurti/comp417.git\n• Look under comp417/lqr_examples/python"
  },
  {
    "objectID": "lecs/w10/lec10.html#todays-agenda",
    "href": "lecs/w10/lec10.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n• Shared autonomy for assistive robotics\n• Shared autonomy with human in the loop in deep RL\n• Hindsight optimization and interactive goal prediction\n• Relaxed inverse kinematics for fluid interaction with robot arms\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Andrei Barsan, Bin Yang, and Tingwu Wang"
  },
  {
    "objectID": "lecs/w10/lec10.html#shared-autonomy-via-deep-reinforcement-learning",
    "href": "lecs/w10/lec10.html#shared-autonomy-via-deep-reinforcement-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Shared Autonomy via Deep Reinforcement Learning",
    "text": "Shared Autonomy via Deep Reinforcement Learning\nSiddharth Reddy, Anca Dragan, Sergey Levine UC Berkeley\nPresented by Ioan Andrei Bârsan on February 22, 2019\niab@cs.toronto.edu"
  },
  {
    "objectID": "lecs/w10/lec10.html#key-question",
    "href": "lecs/w10/lec10.html#key-question",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Key Question",
    "text": "Key Question\n\nHow can a robot collaborating with a human infer the human’s goals with as few assumptions as possible?"
  },
  {
    "objectID": "lecs/w10/lec10.html#motivation",
    "href": "lecs/w10/lec10.html#motivation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation",
    "text": "Motivation\n\n\n• Hard: Actuating a robot with many DoF and/or unfamiliar dynamics.\n• Hard: Specifying a goal formally (e.g., coordinates).\n• Easy: Demonstrating the goal indirectly.\n• …let the machine figure out what I want!\n\n\n\nImage source: “Multihierarchical Interactive Task Planning. Application to Mobile Robotics” Galindo et al., 2008"
  },
  {
    "objectID": "lecs/w10/lec10.html#motivation-unknown-dynamics-are-hard-for-humans",
    "href": "lecs/w10/lec10.html#motivation-unknown-dynamics-are-hard-for-humans",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation: Unknown Dynamics are Hard for Humans",
    "text": "Motivation: Unknown Dynamics are Hard for Humans"
  },
  {
    "objectID": "lecs/w10/lec10.html#it-can-get-even-worse-than-lunar-lander",
    "href": "lecs/w10/lec10.html#it-can-get-even-worse-than-lunar-lander",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "It can get even worse than Lunar Lander…",
    "text": "It can get even worse than Lunar Lander…\n\n\n\n\nhttps://www.foddy.net/Athletics.html or Google “qwop”"
  },
  {
    "objectID": "lecs/w10/lec10.html#challenges",
    "href": "lecs/w10/lec10.html#challenges",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Challenges",
    "text": "Challenges\n• Recall: Want to demonstrate the goal indirectly with minimal assumptions.\n• → We expect the computer to start helping while it is still learning.\n• Challenge #1: How to actually infer user’s goal?\n• Challenge #2: How can we learn this online with low latency?"
  },
  {
    "objectID": "lecs/w10/lec10.html#main-hypothesis",
    "href": "lecs/w10/lec10.html#main-hypothesis",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Main Hypothesis",
    "text": "Main Hypothesis\nShared autonomy can improve human performance without any assumptions about:\n\ndynamics,\nthe human’s policy,\nthe nature of the goal."
  },
  {
    "objectID": "lecs/w10/lec10.html#formulation-reward",
    "href": "lecs/w10/lec10.html#formulation-reward",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Formulation: Reward",
    "text": "Formulation: Reward\n\n\\[\nR(s, a, s') = \\underbrace{R_{\\text{general}}(s, a, s')}_{\\text{known}} + \\underbrace{R_{\\text{feedback}}(s, a, s')}_{\\text{unknown, but observed}}\n\\]\n\n\n\\(\\qquad \\qquad \\quad \\uparrow\\)\nAgent’s reward\n(what we want to maximize)\n\n\n\\(\\qquad \\qquad \\qquad \\uparrow\\)\nHandcrafted “common sense”\nknowledge: do not crash, do\nnot tip, etc.\n\n\n\\(\\uparrow\\)\nStuﬀ inferred from the human\n(Main focus of this paper!)"
  },
  {
    "objectID": "lecs/w10/lec10.html#formulation",
    "href": "lecs/w10/lec10.html#formulation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Formulation",
    "text": "Formulation\n\n\\[\n\\underbrace{R_{\\text{feedback}}(s, a, s')}_{\\text{unknown, but observed}}\n\\]\n\n\n\n\n\nNeeds\nvirtual\n“user”!\n\n• The authors introduce three variants of their method:\n\nKnown goal space, known user policy.\nKnown goal space, unknown user policy.\nUnknown goal space, unknown user policy."
  },
  {
    "objectID": "lecs/w10/lec10.html#the-method",
    "href": "lecs/w10/lec10.html#the-method",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The Method",
    "text": "The Method\n\nBased on Q-Learning.\nUser input has two roles:\n\nA prior policy we should fine-tune.\nA sensor which can be used to decode the goal.\n\nShort version: Like Q-Learning, but execute closest high-value action to the user’s input, instead of highest-value action."
  },
  {
    "objectID": "lecs/w10/lec10.html#the-method-continued",
    "href": "lecs/w10/lec10.html#the-method-continued",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The Method (Continued)",
    "text": "The Method (Continued)"
  },
  {
    "objectID": "lecs/w10/lec10.html#the-method-continued-1",
    "href": "lecs/w10/lec10.html#the-method-continued-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "The Method (Continued)",
    "text": "The Method (Continued)"
  },
  {
    "objectID": "lecs/w10/lec10.html#but-where-is-textr_textfeedback",
    "href": "lecs/w10/lec10.html#but-where-is-textr_textfeedback",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "But where is \\(\\text{R}_{\\text{feedback}}\\)?",
    "text": "But where is \\(\\text{R}_{\\text{feedback}}\\)?\n\nThe choice of \\(\\text{R}_{\\text{feedback}}\\) determines what kind of input we give to the Q- Learning agent in addition to state!\n\n\nKnown goal space & user policy → exact goal.\nKnown goal space & unknown policy → predicted goal (pretrained LSTM).\nUnknown goal space & policy → the user’s input (main focus)"
  },
  {
    "objectID": "lecs/w10/lec10.html#input-to-rl-agent",
    "href": "lecs/w10/lec10.html#input-to-rl-agent",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Input to RL Agent",
    "text": "Input to RL Agent"
  },
  {
    "objectID": "lecs/w10/lec10.html#experiments",
    "href": "lecs/w10/lec10.html#experiments",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments",
    "text": "Experiments\n\n\nVirtual experiments with Lunar Lander in OpenAI gym.\nPhysical experiments with an actual drone."
  },
  {
    "objectID": "lecs/w10/lec10.html#real-world-experiments",
    "href": "lecs/w10/lec10.html#real-world-experiments",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Real-World Experiments",
    "text": "Real-World Experiments\n\n\n\nGoal: Land drone on pad facing a certain way.\nPilot: Human, knows target orientation.\nCopilot: Our Agent, knows where pad is, but not target orientation."
  },
  {
    "objectID": "lecs/w10/lec10.html#real-world-results",
    "href": "lecs/w10/lec10.html#real-world-results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Real-World Results",
    "text": "Real-World Results\n\n\n\n\n\nImportant observation: Only n = 4 humans in drone study."
  },
  {
    "objectID": "lecs/w10/lec10.html#experimental-results-assumptions",
    "href": "lecs/w10/lec10.html#experimental-results-assumptions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experimental Results: Assumptions",
    "text": "Experimental Results: Assumptions\n\n\n\n\n\n\nHigher alpha means we take any action. α = 1.0 means we ignore the pilot.\nExperimented in virtual environment."
  },
  {
    "objectID": "lecs/w10/lec10.html#recap-strengths",
    "href": "lecs/w10/lec10.html#recap-strengths",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recap: Strengths",
    "text": "Recap: Strengths\n\nGood results even when making no assumptions about user/goal.\nWriting is very clear!\nPossible applications in many fields, including e.g. prosthetics, wheelchairs.\nSource code released on GitHub!"
  },
  {
    "objectID": "lecs/w10/lec10.html#recap-weaknesses",
    "href": "lecs/w10/lec10.html#recap-weaknesses",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recap: Weaknesses",
    "text": "Recap: Weaknesses\n\nUser studies could have had more participants.\nCould have shown results on more Gym environments.\nSolution does not generalize to sophisticated long-term goals."
  },
  {
    "objectID": "lecs/w10/lec10.html#conclusion",
    "href": "lecs/w10/lec10.html#conclusion",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Conclusion",
    "text": "Conclusion\n\nCan do shared autonomy with minimal assumptions!\nIdea: Q-Learning & pick high-value action most similar to user’s action.\nWorks well in virtual environments (real humans).\nSeems to work well in real environments, too."
  },
  {
    "objectID": "lecs/w10/lec10.html#thanks-for-your-attention",
    "href": "lecs/w10/lec10.html#thanks-for-your-attention",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Thanks for your attention!",
    "text": "Thanks for your attention!\nQ&A, if time permits it.\nProject website: https://sites.google.com/view/deep-assist\n\n\n\n\n\nVideo of computer-assisted human piloting the lander."
  },
  {
    "objectID": "lecs/w10/lec10.html#shared-autonomy-via-hindsight-optimization",
    "href": "lecs/w10/lec10.html#shared-autonomy-via-hindsight-optimization",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Shared Autonomy via Hindsight Optimization",
    "text": "Shared Autonomy via Hindsight Optimization"
  },
  {
    "objectID": "lecs/w10/lec10.html#teleoperation",
    "href": "lecs/w10/lec10.html#teleoperation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Teleoperation",
    "text": "Teleoperation\n  \n  \nNoisy, insuﬃcient degrees of freedom, tedious\n\nImage credit: Javdani RSS2015 talk"
  },
  {
    "objectID": "lecs/w10/lec10.html#shared-autonomy",
    "href": "lecs/w10/lec10.html#shared-autonomy",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Shared Autonomy",
    "text": "Shared Autonomy\n\n\n User Input\n\n\n+\n\n\n Autonomous Assistance\n\n\n=\n\n\n Achieve Goal"
  },
  {
    "objectID": "lecs/w10/lec10.html#shared-autonomy-1",
    "href": "lecs/w10/lec10.html#shared-autonomy-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Shared Autonomy",
    "text": "Shared Autonomy\n\n\n User Input\n\n\n+\n\n Autonomous Assistance\n\n\n=\n\n\n Achieve Goal"
  },
  {
    "objectID": "lecs/w10/lec10.html#shared-autonomy-2",
    "href": "lecs/w10/lec10.html#shared-autonomy-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Shared Autonomy",
    "text": "Shared Autonomy\n\n\nPredict goal Assist for single goal\n\n[Dragan and Srinivasa 13]\n[Kofman et al. 05]\n[Kragic et al. 05]\n[Yu et al. 05]\n[McMullen et al. 14]\n…\n\n\n\n+\n\n Autonomous Assistance\n\n\n=\n\n\n Achieve Goal"
  },
  {
    "objectID": "lecs/w10/lec10.html#shared-autonomy-3",
    "href": "lecs/w10/lec10.html#shared-autonomy-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Shared Autonomy",
    "text": "Shared Autonomy\n\n\nPredict goal Assist for single goal\n\n[Dragan and Srinivasa 13]\n[Kofman et al. 05]\n[Kragic et al. 05]\n[Yu et al. 05]\n[McMullen et al. 14]\n…\n\nPredict goal distribution Assist for distribution\n\n[Hauser 13]\nThis work!\n\n\n\n+\n\n Autonomous Assistance\n\n\n=\n\n\n Achieve Goal"
  },
  {
    "objectID": "lecs/w10/lec10.html#method",
    "href": "lecs/w10/lec10.html#method",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Method",
    "text": "Method\n\nSystem dynamics: \\(x’ = T(x, a)\\)"
  },
  {
    "objectID": "lecs/w10/lec10.html#method-1",
    "href": "lecs/w10/lec10.html#method-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Method",
    "text": "Method\n\n\n\nSystem dynamics: \\(x' = T(x, a)\\)\nUser (MDP) as \\((X, U, T, C_g^{\\text{usr}})\\)\n\nUser policy: \\(\\pi_g^{\\text{usr}}(x) = p(u|x, g)\\)\nMaxEnt IOC: \\(C_g^{\\text{usr}} : X \\times U \\to \\mathcal{R}\\)"
  },
  {
    "objectID": "lecs/w10/lec10.html#method-2",
    "href": "lecs/w10/lec10.html#method-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Method",
    "text": "Method\n\n\n\nSystem dynamics: \\(x' = T(x, a)\\)\nUser (MDP) as \\((X, U, T, C_g^{\\text{usr}})\\)\n\nUser policy: \\(\\pi_g^{\\text{usr}}(x) = p(u|x, g)\\)\nMaxEnt IOC: \\(C_g^{\\text{usr}} : X \\times U \\to \\mathcal{R}\\)\n\nSystem (POMDP) as \\((S, A, T, C^{\\text{rob}}, U, \\Omega)\\)\n\nUncertainty over user’s goal\nSystem state: \\(s = X \\times G\\)\nObservation: user inputs \\(U\\)\nObservation model \\(\\Omega\\)\n\n\\(p(g|\\xi^{0 \\to t}) = \\frac{p(\\xi^{0 \\to t}|g)p(g)}{\\sum_{g'} p(\\xi^{0 \\to t}|g')p(g')}\\)\n\nCost function \\(C^{\\text{rob}} : S \\times A \\times U \\to \\mathcal{R}\\)"
  },
  {
    "objectID": "lecs/w10/lec10.html#hindsight-optimization",
    "href": "lecs/w10/lec10.html#hindsight-optimization",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Hindsight Optimization",
    "text": "Hindsight Optimization\n\n\n\nMDP solution:\n\\[V^{\\pi^r}(s) = \\mathbb{E}\\left[\\sum_t C^r(s_t, u_t, a_t) \\mid s_0 = s\\right]\\]\n\\[V^*(s) = \\min_{\\pi^r} V^{\\pi^r}(s)\\]\n\n\n\nPOMDP solution:\n\\[V^{\\pi^r}(b) = \\mathbb{E}\\left[\\sum_t C^r(s_t, u_t, a_t) \\mid b_0 = b\\right]\\]\n\\[V^*(b) = \\min_{\\pi^r} V^{\\pi^r}(b)\\]\n\n\n\n\nHOP approximation:\n\\[V^{\\text{HS}}(b) = \\mathbb{E}_b\\left[\\min_{\\pi^r} V^{\\pi^r}(s)\\right]\\]\n\\[= \\mathbb{E}_g[V_g(x)]\\]\n\n\n\n\n\n\n\n\n\n\nDeterministic\nproblem for\neach future"
  },
  {
    "objectID": "lecs/w10/lec10.html#results-video",
    "href": "lecs/w10/lec10.html#results-video",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results (video)",
    "text": "Results (video)"
  },
  {
    "objectID": "lecs/w10/lec10.html#results",
    "href": "lecs/w10/lec10.html#results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results",
    "text": "Results\n\n\nCompare with method that predicts one goal, the proposed method has:\n\nFaster execution time\nFewer user inputs"
  },
  {
    "objectID": "lecs/w10/lec10.html#user-study",
    "href": "lecs/w10/lec10.html#user-study",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "User Study",
    "text": "User Study"
  },
  {
    "objectID": "lecs/w10/lec10.html#limitations",
    "href": "lecs/w10/lec10.html#limitations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Limitations",
    "text": "Limitations\n\nRequires prior knowledge about the world:\n\na dynamics model that predicts the consequences of taking a given action in a given state of the environment;\nthe set of possible goals for the user;\nthe user’s control policy given their goal.\n\nSuitable in constrained domains where where this knowledge can be directly hard-coded or learned.\nUnsuitable for unstructured environments with ill-defined goals and unpredictable user behavior."
  },
  {
    "objectID": "lecs/w10/lec10.html#references",
    "href": "lecs/w10/lec10.html#references",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "References",
    "text": "References\n\nJavdani, S., Srinivasa, S. S., & Bagnell, J. A. (2015). Shared autonomy via hindsight optimization. Robotics science and systems: online proceedings, 2015.\nRSS2015 talk: “Shared autonomy via hindsight optimization”\nJavdani, S., Admoni, H., Pellegrinelli, S., Srinivasa, S. S., & Bagnell, J. A. (2018). Shared autonomy via hindsight optimization for teleoperation and teaming. The International Journal of Robotics Research, 37(7), 717-742.\nICAPS 2015 talk: “Hindsight Optimization for Probabilistic Planning with Factored Actions”"
  },
  {
    "objectID": "lecs/w10/lec10.html#relaxedik-real-time-synthesis-of-accurate-and-feasible-robot-arm-motion",
    "href": "lecs/w10/lec10.html#relaxedik-real-time-synthesis-of-accurate-and-feasible-robot-arm-motion",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "RelaxedIK: Real-time Synthesis of Accurate and Feasible Robot Arm Motion",
    "text": "RelaxedIK: Real-time Synthesis of Accurate and Feasible Robot Arm Motion"
  },
  {
    "objectID": "lecs/w10/lec10.html#recap-forward-kinematics-fk",
    "href": "lecs/w10/lec10.html#recap-forward-kinematics-fk",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recap: Forward Kinematics (FK)",
    "text": "Recap: Forward Kinematics (FK)\n\nForward Kinematics\n\nA common robotic skeleton is a tree of rigid bones\nThe relative Euler angles of all the joints determine the end-effector\n\nEnd-effectors? A tool that’s connected to the end of a robot arm\n\n\n\n\n\n\n\n\n\n\n\nmaterials from https://github.com/alecjacobson/computer-graphics-csc418"
  },
  {
    "objectID": "lecs/w10/lec10.html#recap-inverse-kinematics-ik",
    "href": "lecs/w10/lec10.html#recap-inverse-kinematics-ik",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recap: Inverse Kinematics (IK)",
    "text": "Recap: Inverse Kinematics (IK)\n\nInverse Kinematics\n\nWe formulate the inverse kinematics function as: \\(\\Theta = IK(p)\\) , which can be easily written in an analytic form for a simple tree skeleton.\n\nPose contains velocity?\nHard to find feasible state space?\n\nIn reality, IK is often treated as an optimization problem \\[\n\\chi_p(\\Theta) = \\| p_g - FK(\\Theta) \\|_2\n\\]"
  },
  {
    "objectID": "lecs/w10/lec10.html#imitation-learning",
    "href": "lecs/w10/lec10.html#imitation-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\n\nImitation learning using IK\n\nBasic idea: Using IK to bridge between target pose and agent’s angles\nInput: M (consecutive) expert (goal) poses\nOutput: M (consecutive) frames of agent’s euler joints\nConstraints:\n\nIK constraints (goal constraints)\nBetween-frames constraints"
  },
  {
    "objectID": "lecs/w10/lec10.html#imitation-learning-1",
    "href": "lecs/w10/lec10.html#imitation-learning-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\n\nImitation learning using IK\n\nBasic idea: minimize the difference of target pose and agent pose\n\nDirect point-to-point approach\n\nTRAC-IK (previous state-of-the-art)\nPose2pose / frame2frame imitation learning\n\nIgnore most of the constraints between frames\n\nProblems\n\nSelf-collision\nTime constraints"
  },
  {
    "objectID": "lecs/w10/lec10.html#imitation-learning-2",
    "href": "lecs/w10/lec10.html#imitation-learning-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\nSelf-collision"
  },
  {
    "objectID": "lecs/w10/lec10.html#imitation-learning-3",
    "href": "lecs/w10/lec10.html#imitation-learning-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\nDiscontinuities"
  },
  {
    "objectID": "lecs/w10/lec10.html#imitation-learning-4",
    "href": "lecs/w10/lec10.html#imitation-learning-4",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\nGoal mistracking"
  },
  {
    "objectID": "lecs/w10/lec10.html#imitation-learning-5",
    "href": "lecs/w10/lec10.html#imitation-learning-5",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\nUnpredictable behaviors"
  },
  {
    "objectID": "lecs/w10/lec10.html#relaxed-ik",
    "href": "lecs/w10/lec10.html#relaxed-ik",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Relaxed IK",
    "text": "Relaxed IK\n\nBasic Idea: Using soft (Relaxed) IK loss that considers self-collision and singularity for faster optimization\n\n\\[\nf(\\Theta) = \\sum_{i=1}^{k} w_i f_i(\\Theta, \\Omega_i)\n\\]\n\nLoss functions\n\nEnd-effector position & orientation matching\nMinimize joint velocity, acceleration, jerk\nSelf-collision loss (fast)\nSingularity loss"
  },
  {
    "objectID": "lecs/w10/lec10.html#relaxed-ik-1",
    "href": "lecs/w10/lec10.html#relaxed-ik-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Relaxed IK",
    "text": "Relaxed IK\nSelf-collision loss\n\nCommon approach: very slow\nRelaxed IK:\n\nApproximate how imminent the robot is to a collision state\nUsing simulated data to train a network to predict the distances between links\n\n\n\n\\[\n\\text{col}(\\Theta) = \\sum_{i,j} b \\cdot \\exp\\left( -\\frac{\\text{dis}(l_i, l_j)^2}{2c^2} \\right)\n\\]"
  },
  {
    "objectID": "lecs/w10/lec10.html#relaxed-ik-2",
    "href": "lecs/w10/lec10.html#relaxed-ik-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Relaxed IK",
    "text": "Relaxed IK\n\nSingularity loss\n\n\nKinematic singularities are well studied in robotics\nRelaxed IK:\n\nFind a metric that can approximate distance to a singularity\nJacobian’s condition number is used as a proxy distance to singularity\nWhy?\n\n\\[\n\\dot{\\mathbf{x}} = \\mathbf{J}(\\Theta) \\dot{\\Theta}\n\\]\n\nPenalize condition values less than mean - b * std\nEstimate mean, std from simulated data"
  },
  {
    "objectID": "lecs/w10/lec10.html#relaxed-ik-3",
    "href": "lecs/w10/lec10.html#relaxed-ik-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Relaxed IK",
    "text": "Relaxed IK\nPros\n\nMuch faster and smoother performance\n\ncombining neural network and traditional robotics\n\nData driven, less human-engineering\n\nnovel singularity metric\n\nEasy to deploy\n\nsim2Real"
  },
  {
    "objectID": "lecs/w09/lec09.html#todays-agenda",
    "href": "lecs/w09/lec09.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n• Adversarial optimization for imitation learning (GAIL)\n• Adversarial optimization with multiple inferred behaviors (InfoGAIL)\n• Model based adversarial optimization (MGAIL)\n• Multi-agent imitation learning\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong"
  },
  {
    "objectID": "lecs/w09/lec09.html#detour-solving-mdps-via-linear-programming",
    "href": "lecs/w09/lec09.html#detour-solving-mdps-via-linear-programming",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Detour: solving MDPs via linear programming",
    "text": "Detour: solving MDPs via linear programming\n\\(\\underset{v}{\\arg\\min} \\quad \\underset{s \\in S}{\\sum} d_s v_s\\)\n\\(\\text{subject to:} \\quad v_s \\geq r_{s,a} + \\gamma \\underset{s' \\in S}{\\sum} T_{s,a}^{s'} v_{s'} \\quad \\forall s \\in S, a \\in A\\)\n\\(\\qquad \\qquad \\quad d\\) is the initial state distribution.\n\n\nOptimal policy\n\\(\\pi^*(s) = \\underset{a \\in A}{\\text{argmax}} Q^*(s, a)\\)"
  },
  {
    "objectID": "lecs/w09/lec09.html#detour-solving-mdps-via-linear-programming-1",
    "href": "lecs/w09/lec09.html#detour-solving-mdps-via-linear-programming-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Detour: solving MDPs via linear programming",
    "text": "Detour: solving MDPs via linear programming\n\\(\\underset{\\mu}{\\text{argmax}} \\underset{s \\in S, a \\in A}{\\sum} \\mu_{s,a} r_{s,a}\\)\n\\(\\text{subject to} \\quad \\underset{a \\in A}{\\sum} \\mu_{s',a} = d_{s'} + \\gamma \\underset{s \\in S, a \\in A}{\\sum} T_{s,a}^{s'} \\mu_{s,a} \\quad \\forall s' \\in S\\)\n\\(\\qquad \\qquad \\quad \\mu_{s,a} \\geq 0\\)\n\n\nDiscounted state action counts / occupancy measure\n\\(\\mu(s,a) = \\underset{t=0}{\\sum}^{\\infty} \\gamma^t p(s_t = s, a_t = a)\\)\nOptimal policy\n\\(\\pi^*(s) = \\underset{a \\in A}{\\text{argmax}} \\, \\mu(s,a)\\)"
  },
  {
    "objectID": "lecs/w09/lec09.html#detour-solving-mdps-via-linear-programming-2",
    "href": "lecs/w09/lec09.html#detour-solving-mdps-via-linear-programming-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Detour: solving MDPs via linear programming",
    "text": "Detour: solving MDPs via linear programming\n\n\n\\(\\underset{v}{\\text{argmin}} \\underset{s \\in S}{\\sum} d_s v_s\\)\n\\(\\text{subject to} \\quad v_s \\geq r_{s,a} + \\gamma \\underset{s' \\in S}{\\sum} T_{s,a}^{s'} v_{s'} \\quad \\forall s \\in S, a \\in A\\)\n\\(\\qquad \\qquad \\quad d\\) is the initial state distribution\n\n\\(\\underset{\\mu}{\\text{argmax}} \\underset{s \\in S, a \\in A}{\\sum} \\mu_{s,a} r_{s,a}\\)\n\\(\\text{subject to} \\quad \\underset{a \\in A}{\\sum} \\mu_{s',a} = d_{s'} + \\underset{s \\in S, a \\in A}{\\sum} T_{s,a}^{s'} \\mu_{s,a} \\quad \\forall s' \\in S\\)\n\\(\\qquad \\qquad \\quad \\mu_{s,a} \\geq 0\\)\n\nPrimal LP\n\n\nDual LP"
  },
  {
    "objectID": "lecs/w09/lec09.html#background",
    "href": "lecs/w09/lec09.html#background",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Background",
    "text": "Background\n▶ Definitions: Action space \\(\\mathcal{A}\\) and sample space \\(\\mathcal{S}\\). \\(\\Pi\\) is the set of all policies. Also assume \\(P(s'|s,a)\\) is the dynamics model. In this paper, \\(\\pi_E\\) denotes the expert policy.\n▶ Imitation Learning: Learning to perform a task from expert demonstrations without querying the expert while training.\n▶ Behavioral cloning: Its success depends on large amounts of data.\n▶ Inverse RL: The paper adopts the maximum causal entropy IRL which fits a cost function \\(c\\) with the following problem.\n\\[\\pi^* = \\arg \\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_\\pi[c(s,a)]\\]\n\\[\\tilde{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{E}_{\\pi^*}[c(s,a)] - \\mathbb{E}_{\\pi_E}[c(s,a)]\\]\nwhere \\(H(\\pi) = \\mathbb{E}_\\pi[-\\log \\pi(a|s)]\\) is the entropy of the policy."
  },
  {
    "objectID": "lecs/w09/lec09.html#formulation",
    "href": "lecs/w09/lec09.html#formulation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Formulation",
    "text": "Formulation\n▶ We first study the policies found by RL on costs learned by IRL on the largest possible set of cost functions \\(\\mathcal{C} = \\{c : S \\times A \\to \\mathbb{R}\\}\\).\n▶ Also need to define a convex cost function regularizer \\(\\psi : \\mathbb{R}_{S \\times A} \\to \\mathbb{R}\\), which turns out to be important in this paper.\n▶ Re-write the Eq. 1 as the following:\n\\[IRL_\\psi(\\pi_E) = \\arg \\max_{c \\in \\mathcal{C}} -\\psi(c) + (\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_\\pi[c(s,a)])\\]\n\\[- \\mathbb{E}_{\\pi_E}[c(s,a)]\\]\n▶ Define \\(RL(c) = \\arg \\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_\\pi[c(s,a)]\\).\nLet \\(\\tilde{c} \\in IRL_\\psi(\\pi_E)\\). We are interested in characterizing the induced policy \\(RL(\\tilde{c})\\)."
  },
  {
    "objectID": "lecs/w09/lec09.html#derivations",
    "href": "lecs/w09/lec09.html#derivations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Derivations",
    "text": "Derivations\n▶ It is easier to characterize \\(RL(\\tilde{c})\\) if we transform optimization problems over policies into convex problems.\n▶ So the paper introduces an occupancy measure \\(\\rho_\\pi : S \\times A \\to \\mathbb{R}\\):\n\\[\\rho_\\pi(s,a) = \\pi(a|s) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s|\\pi) \\qquad (1)\\]\nIt can be interpreted as the distribution of state-action pairs when roll-out with policy \\(\\pi\\).\n▶ There is an one-to-one correspondence between policy and occupancy measure. It also allows us to re-write the expected cost as\n\\[\\mathbb{E}_\\pi[c(s,a)] = \\sum_{s,a} \\rho_\\pi(s,a)c(s,a) \\qquad (2)\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#derivations-1",
    "href": "lecs/w09/lec09.html#derivations-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Derivations",
    "text": "Derivations\n▶ Lemma 1: If we define\n\\[\\hat{H}(\\rho) = -\\sum_{s,a} \\rho(s,a) \\log(\\rho(s,a)/\\sum_{a'} \\rho(s,a')) \\qquad (3)\\]\nthen we have \\(\\hat{H}(\\rho) = H(\\pi_\\rho)\\) and \\(H(\\pi) = \\hat{H}(\\rho_\\pi)\\). So we can represent the entropy of a policy \\(\\pi\\) with the occupancy measure \\(\\rho_\\pi\\).\n▶ Lemma 2: If we define,\n\\[L(\\pi, c) = -H(\\pi) + \\mathbb{E}_\\pi[c(s,a)]\\]\n\\[\\hat{L}(\\rho, c) = -\\hat{H}(\\rho) + \\sum_{s,a} \\rho(s,a)c(s,a)\\]\nthen we have \\(L(\\pi, c) = \\hat{L}(\\rho_\\pi, c)\\) and \\(\\hat{L}(\\rho, c) = L(\\pi_\\rho, c)\\). The Lemma allows us to transform the problem from optimizing \\(\\pi\\) to \\(\\rho\\)."
  },
  {
    "objectID": "lecs/w09/lec09.html#convex-conjugate",
    "href": "lecs/w09/lec09.html#convex-conjugate",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Convex Conjugate",
    "text": "Convex Conjugate\n▶ Given a function \\(f\\), it can be represented by the supremum of all affine functions that are majorized by \\(f\\).\n▶ For any given slope \\(m\\), there may be many different constants \\(b\\) such that the affine function \\(\\langle m, x \\rangle - b\\) is majorized by \\(f\\). We only need the best such constant.\n▶ That’s what the convex conjugate \\(f^*\\) does. Given a slope \\(m\\), \\(f^*\\) returns the best constant \\(b\\) such that \\(\\langle m, x \\rangle - b\\) is majorized by \\(f\\). Thus,\n\\[f^*(m) = \\sup_x \\langle m, x \\rangle - f(x)\\]\n▶ Note that \\(f^{**} = f\\).\n\nThere is a nice visualization of convex conjugate at https://remilepriol.github.io/dualityviz/"
  },
  {
    "objectID": "lecs/w09/lec09.html#derivations-2",
    "href": "lecs/w09/lec09.html#derivations-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Derivations",
    "text": "Derivations\n▶ By Lemma 2, if \\(\\psi\\) is a constant regularizer and \\(\\tilde{c} \\in IRL_\\psi(\\pi_E)\\) and \\(\\hat{\\pi} \\in RL(\\hat{c})\\), then \\(\\rho_{\\hat{\\pi}} = \\rho_{\\pi_E}\\).\n▶ Furthermore, we can also get the main result of the paper\n\\[RL \\circ IRL_\\psi(\\pi_E) = \\arg \\min_{\\pi \\in \\Pi} -H(\\pi) + \\psi^*(\\rho_\\pi - \\rho_{\\pi_E}) \\qquad (4)\\]\nwhere \\(\\psi^*\\) is the convex conjugate of \\(\\psi\\), which is defined as\n\\[\\psi^*(m) = \\sup_{x \\in \\mathbb{R}^{S \\times A}} m^T x - \\psi(x)\\]\n▶ It tells us that the \\(\\psi\\)-regularized inverse RL seeks a policy whose occupancy measure is close to the expert’s as measured by the convex function \\(\\psi^*\\).\n▶ A good imitation learning algorithm boils down to a good choice of the regularizer \\(\\psi\\)."
  },
  {
    "objectID": "lecs/w09/lec09.html#occupancy-measure-matching",
    "href": "lecs/w09/lec09.html#occupancy-measure-matching",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Occupancy Measure Matching",
    "text": "Occupancy Measure Matching\n▶ As we showed previously, if \\(\\psi\\) is a constant, then the resulting policy would have the same occupancy measures with expert at all states and actions.\n▶ It is not practically useful because most of the occupancy measure of the expert values are exactly zero, due to the limited expert samples.\n▶ Thus, exact occupancy measure matching will force the learned policy to never visit the unseen state-action pairs.\n▶ If we restrict the class of cost function \\(\\mathcal{C}\\) to be convex and set the regularizer \\(\\psi\\) to be the indicator function of the set \\(\\mathcal{C}\\). Then optimization problem in (6) can be written as\n\\[\\min_\\pi -H(\\pi) + \\max_{c \\in \\mathcal{C}} \\mathbb{E}_\\pi[c(s,a)] - \\mathbb{E}_{\\pi_E}[c(s,a)] \\qquad (5)\\]\nwhich is a entropy-regularized apprenticeship learning problem."
  },
  {
    "objectID": "lecs/w09/lec09.html#apprenticeship-learning",
    "href": "lecs/w09/lec09.html#apprenticeship-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Apprenticeship Learning",
    "text": "Apprenticeship Learning\n▶ Policy gradient method can be used to update the parameterized policy \\(\\pi_\\theta\\) to optimize the apprenticeship objective, Eq. 7.\n\\[\\nabla_\\theta \\max_{c \\in \\mathcal{C}} \\mathbb{E}_{\\pi_\\theta}[c(s,a)] - \\mathbb{E}_{\\pi_E}[c(s,a)] = \\nabla_\\theta \\mathbb{E}_{\\pi_\\theta}[c^*(s,a)]\\]\n\\[= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q_{c^*}(s,a)]\\]\nwhere\n\\[c^* = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{E}_{\\pi_\\theta}[c(s,a)] - \\mathbb{E}_{\\pi_E}[c(s,a)] \\qquad (6)\\]\n\\[Q_{c^*}(\\bar{s}, \\bar{a}) = \\mathbb{E}_{\\pi_\\theta}[c^*(\\bar{s}, \\bar{a})|s_0 = \\bar{s}, a_0 = \\bar{a}] \\qquad (7)\\]\n▶ Fit \\(c_i^*\\) as defined above. Analytical solution is feasible if \\(\\mathcal{C}\\) is restricted to Convex or Linear cost classes.\n▶ Given the \\(c_i^*\\), compute the policy gradient and take a TRPO step to produce \\(\\pi_{\\theta_{i+1}}\\)."
  },
  {
    "objectID": "lecs/w09/lec09.html#gail",
    "href": "lecs/w09/lec09.html#gail",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "GAIL",
    "text": "GAIL\n▶ Apprenticeship learning via TRPO is tractable in large environments but is incapable of exactly matching occupancy measures without careful tuning due to the restrictive cost classes \\(\\mathcal{C}\\).\n▶ Constant regularizer \\(\\psi\\) leads to exact matching but is intractable in large environments. Thus, GAIL is proposed to combine the best of both methods.\n\\[\n\\psi_{GA}(c) \\triangleq \\begin{cases}\n\\mathbb{E}_{\\pi_E}[g(c(s,a))] & \\text{if } c &lt; 0 \\\\\n+\\infty & \\text{otherwise}\n\\end{cases}\n\\]\nwhere\n\\[\ng(x) = \\begin{cases}\n-x - \\log(1 - e^x) & \\text{if } x &lt; 0 \\\\\n+\\infty & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#gail-1",
    "href": "lecs/w09/lec09.html#gail-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "GAIL",
    "text": "GAIL\n▶ The GAIL regularizer \\(\\psi_{GA}\\) places low penalty on cost functions \\(c\\) that assign an amount of negative cost to expert state-action pairs; It havily penalizes \\(c\\) if it assigns large cost to the expert.\n▶ \\(\\psi_{GA}\\) is an average over expert data so it can adjust to arbitrary expert datasets.\n▶ In comparison, if \\(\\psi\\) is an indicator function (Apprenticeship Learning), then it’s always fixed.\n▶ Another property of \\(\\psi_{GA}\\) is its convex conjugate \\(\\psi_{GA}^*(\\rho_\\pi - \\rho_{\\pi_E})\\) can be derived in the following form:\n\\[\\max_{D \\in (0,1)^{S \\times A}} \\mathbb{E}_\\pi[\\log(D(s,a))] + \\mathbb{E}_{\\pi_E}[\\log(1 - D(s,a))] \\qquad (8)\\]\n▶ It can be interpreted to find a discriminator that distinguishes trajectory between learned policy and expert policy. t"
  },
  {
    "objectID": "lecs/w09/lec09.html#gail-2",
    "href": "lecs/w09/lec09.html#gail-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "GAIL",
    "text": "GAIL\n▶ Combining with the main result Eq. (6) in the paper,\n\\[RL \\circ IRL_\\psi(\\pi_E) = \\arg \\min_{\\pi \\in \\Pi} -H(\\pi) + \\psi^*(\\rho_\\pi - \\rho_{\\pi_E})\\]\nThe imitation learning problem is equivalent to find a saddle point \\((\\pi, D)\\) of the expression\n\\[\\mathbb{E}_\\pi[\\log(D(s,a))] + \\mathbb{E}_{\\pi_E}[\\log(1 - D(s,a))] - \\lambda H(\\pi) \\qquad (9)\\]\n▶ In terms of implementation, we just need to fit a parameterized policy \\(\\pi_\\theta\\) with weights \\(\\theta\\) and a discriminator network \\(D_w : S \\times A \\to (0,1)\\) with weights \\(w\\).\n▶ Update \\(D_w\\) with Adam and update \\(\\pi_\\theta\\) with TRPO iteratively."
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm",
    "href": "lecs/w09/lec09.html#algorithm",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm",
    "text": "Algorithm"
  },
  {
    "objectID": "lecs/w09/lec09.html#results",
    "href": "lecs/w09/lec09.html#results",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "lecs/w09/lec09.html#todays-agenda-1",
    "href": "lecs/w09/lec09.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Adversarial optimization for imitation learning (GAIL)\n\n• Adversarial optimization with multiple inferred behaviors (InfoGAIL)\n• Model based adversarial optimization (MGAIL)\n• Multi-agent imitation learning\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong"
  },
  {
    "objectID": "lecs/w09/lec09.html#infogail-interpretable-imitation-learning-from-visual-demonstrations",
    "href": "lecs/w09/lec09.html#infogail-interpretable-imitation-learning-from-visual-demonstrations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
    "text": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations\nPresenter: Yin-Hung Chen"
  },
  {
    "objectID": "lecs/w09/lec09.html#gail-3",
    "href": "lecs/w09/lec09.html#gail-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "GAIL",
    "text": "GAIL\nA generator producing a policy 𝜋 competes with a discriminator distinguishing 𝜋 and the expert."
  },
  {
    "objectID": "lecs/w09/lec09.html#drawbacks-of-gail",
    "href": "lecs/w09/lec09.html#drawbacks-of-gail",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Drawbacks of GAIL",
    "text": "Drawbacks of GAIL\n\nExpert demonstrations can show significant variability.\nThe observations might have been sampled from different experts with different skills and habits.\nExternal latent factors of variation are not explicitly captured by GAIL, but they can significantly affect the observed behaviors."
  },
  {
    "objectID": "lecs/w09/lec09.html#modified-gail",
    "href": "lecs/w09/lec09.html#modified-gail",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Modified GAIL",
    "text": "Modified GAIL"
  },
  {
    "objectID": "lecs/w09/lec09.html#objective-function",
    "href": "lecs/w09/lec09.html#objective-function",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Objective Function",
    "text": "Objective Function\nGAIL: \\[\n\\underset{\\pi}{\\min} \\underset{𝐷\\in(0,1)^{S×A}}{\\max} + 𝔼_{𝜋_𝐸}  [ 𝑙𝑜𝑔(1 − 𝐷(𝑠, a))] − 𝜆𝐻(𝜋)\n\\]\nwhere 𝜋 is learner policy, and \\(𝜋_𝐸\\) is expert policy.\nInfoGAIL:\n\nDiscriminator: same with GAIL\nGenerator: simply introducing latent factor c into 𝜋 → 𝜋 𝑎 𝑠, 𝑐\nHowever, applying GAIL to 𝜋(𝑎 | 𝑠, 𝑐) could simply ignore c and fail to separate different expert behaviors → adding more constraints over c"
  },
  {
    "objectID": "lecs/w09/lec09.html#constraints-over-latent-features",
    "href": "lecs/w09/lec09.html#constraints-over-latent-features",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Constraints over Latent Features",
    "text": "Constraints over Latent Features\nThere should be high mutual information between the latent factor c and learner trajectory τ.\n\\[\nI(c; \\tau) = \\sum_{\\tau} p(\\tau) \\sum_c p(c|\\tau) \\log_2 \\frac{p(c|\\tau)}{p(c)}\n\\]\nIndependence between c and trajectory τ:\n\\[\np(c|\\tau) = \\frac{p(c)p(\\tau)}{p(\\tau)}, \\quad \\frac{p(c|\\tau)}{p(c)} = 1, \\quad \\log_2 \\frac{p(c|\\tau)}{p(c)} = 0\n\\]\nMaximizing mutual information \\(I(c; \\tau)\\)\n→ hard to maximize directly as it requires the posterior \\(P(c|\\tau)\\)\n→ using \\(Q(c|\\tau)\\) to estimate \\(P(c|\\tau)\\) There should be high mutual information between the latent factor c and learner trajectory 𝜏."
  },
  {
    "objectID": "lecs/w09/lec09.html#constraints-over-latent-features-1",
    "href": "lecs/w09/lec09.html#constraints-over-latent-features-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Constraints over Latent Features",
    "text": "Constraints over Latent Features\nIntroducing the lower bound \\(L_I(\\pi, Q)\\) of \\(I(c; \\tau)\\)\n\\[\n\\begin{align}\n& I(c; \\tau) \\\\\n&= H(c) - H(c|\\tau) \\\\\n&= \\mathbb{E}_{a \\sim \\pi(\\cdot|s,c)} \\left[ \\mathbb{E}_{c' \\sim P(c|\\tau)} [log P(c'|\\tau)] \\right] + H(c) \\\\\n&= \\mathbb{E}_{a \\sim \\pi(\\cdot|s,c)} \\left[ D_{KL}(P(\\cdot|\\tau) \\| Q(\\cdot|\\tau)) + \\mathbb{E}_{c' \\sim P(c|\\tau)} [log Q(c'|\\tau)] \\right] + H(c) \\\\\n&\\geq \\mathbb{E}_{a \\sim \\pi(\\cdot|s,c)} \\left[ \\mathbb{E}_{c' \\sim P(c|\\tau)} [log Q(c'|\\tau)] \\right] + H(c) \\\\\n&= \\mathbb{E}_{c \\sim P(c), a \\sim \\pi(\\cdot|s,c)} [log Q(c|\\tau)] + H(c) \\\\\n&= L_I(\\pi, Q)\n\\end{align}\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#constraints-over-latent-features-2",
    "href": "lecs/w09/lec09.html#constraints-over-latent-features-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Constraints over Latent Features",
    "text": "Constraints over Latent Features\nThere should be high mutual information between the latent factor c and learner trajectory 𝜏.\n\nMaximizing mutual information 𝐼(𝑐; 𝜏)\n→ hard to maximize directly as it requires the posterior 𝑃(𝑐|𝜏)\n→ using 𝑄(𝑐|𝜏) to estimate 𝑃(𝑐|𝜏)\n\nMaximizing 𝐼(𝑐; 𝜏) through maximize the lower bound 𝐿𝐼(𝜋, 𝑄)"
  },
  {
    "objectID": "lecs/w09/lec09.html#objective-function-1",
    "href": "lecs/w09/lec09.html#objective-function-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Objective Function",
    "text": "Objective Function\nGAIL:\n\\[\n\\min_\\pi \\max_{D \\in (0,1)^{S \\times A}} \\mathbb{E}_\\pi [log D(s, a)] + \\mathbb{E}_{\\pi_E} [log(1 - D(s, a))] - \\lambda H(\\pi)\n\\]\nwhere \\(\\pi\\) is learner policy, and \\(\\pi_E\\) is expert policy.\n\nInfoGAIL:\n\\[\n\\min_{\\pi,Q} \\max_D \\mathbb{E}_\\pi [log D(s, a)] + \\mathbb{E}_{\\pi_E} [log(1 - D(s, a))] - \\lambda_1 L_I(\\pi, Q) - \\lambda_2 H(\\pi)\n\\]\n\\(\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\\) where \\(\\lambda_1 &gt; 0\\) and \\(\\lambda_2 &gt; 0\\)."
  },
  {
    "objectID": "lecs/w09/lec09.html#infogail-1",
    "href": "lecs/w09/lec09.html#infogail-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "InfoGAIL",
    "text": "InfoGAIL"
  },
  {
    "objectID": "lecs/w09/lec09.html#improved-optimization",
    "href": "lecs/w09/lec09.html#improved-optimization",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Improved Optimization",
    "text": "Improved Optimization\nThe traditional GAN objective suffers from vanishing gradient and mode collapse problems.\nVanishing gradient\n\n\n\\[\n\\frac{\\partial C}{\\partial b_1} = \\frac{\\partial C}{\\partial y_3} \\frac{\\partial y_3}{\\partial z_3} \\frac{\\partial z_3}{\\partial x_3} \\frac{\\partial x_3}{\\partial z_2} \\frac{\\partial z_2}{\\partial x_2} \\frac{\\partial x_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial b_1}\n\\]\n\\[\n= \\frac{\\partial C}{\\partial y_{3}} \\sigma'(z_{3}) w_{3} \\sigma'(z_{2}) w_{2} \\sigma'(z_{1})\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#improved-optimization-1",
    "href": "lecs/w09/lec09.html#improved-optimization-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Improved Optimization",
    "text": "Improved Optimization\nThe traditional GAN objective suffers from vanishing gradient and mode collapse problems.\n\nMode collapse: generator tends to produce the same type of data → generator yields the same G(z) for different z"
  },
  {
    "objectID": "lecs/w09/lec09.html#improved-optimization-2",
    "href": "lecs/w09/lec09.html#improved-optimization-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Improved Optimization",
    "text": "Improved Optimization\nThe traditional GAN objective suffers from vanishing gradient and mode collapse problems.\n→ using the Wasserstein GAN (WGAN)\n\\[\n\\min_{\\theta, \\psi} \\max_{\\omega} \\mathbb{E}_{\\pi_\\theta}[D_\\omega(s, a)] - \\mathbb{E}_{\\pi_E}[D_\\omega(s, a)] - \\lambda_0 \\eta(\\pi_\\theta) - \\lambda_1 L_I(\\pi_\\theta, Q_\\psi) - \\lambda_2 H(\\pi_\\theta)\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#learning-to-distinguish-trajectories",
    "href": "lecs/w09/lec09.html#learning-to-distinguish-trajectories",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning to Distinguish Trajectories",
    "text": "Learning to Distinguish Trajectories\n\nThe observations at time t are positions from t − 4 to t.\nThe latent code is a one-hot encoded vector with 3 dimensions and a uniform prior."
  },
  {
    "objectID": "lecs/w09/lec09.html#self-driving-car-in-the-torcs-environment",
    "href": "lecs/w09/lec09.html#self-driving-car-in-the-torcs-environment",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Self-driving car in the TORCS Environment",
    "text": "Self-driving car in the TORCS Environment\n• The demonstrations collected by manually driving\n• Three-dimensional continuous action composed of steering, acceleration,and braking\n• Raw visual inputs as the only external inputs for the state\n• Auxiliary information as internal input, including velocity at time t, actions at time t − 1 and t − 2, and damage of the car\n• Pre-trained ResNet on ImageNet"
  },
  {
    "objectID": "lecs/w09/lec09.html#performance",
    "href": "lecs/w09/lec09.html#performance",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Performance",
    "text": "Performance\nTurn\n[0, 1] corresponds to using the inside lane (blue lines), while [1, 0] corresponds to the outside lane (red lines)."
  },
  {
    "objectID": "lecs/w09/lec09.html#performance-1",
    "href": "lecs/w09/lec09.html#performance-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Performance",
    "text": "Performance\nPass\n[0, 1] corresponds to passing from right (red lines), while [1, 0] corresponds to passing from left (blue lines).\n\n\ninfoGAIL\nGAIL"
  },
  {
    "objectID": "lecs/w09/lec09.html#performance-2",
    "href": "lecs/w09/lec09.html#performance-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Performance",
    "text": "Performance\n• Classification accuracies of 𝑄(𝑐|𝜏)\n• Reward augmentation encouraging the car to drive faster"
  },
  {
    "objectID": "lecs/w09/lec09.html#section-3",
    "href": "lecs/w09/lec09.html#section-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "",
    "text": "https://www.youtube.com/watch?v=YtNPBAW6h5k"
  },
  {
    "objectID": "lecs/w09/lec09.html#todays-agenda-2",
    "href": "lecs/w09/lec09.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Adversarial optimization for imitation learning (GAIL)\n• Adversarial optimization with multiple inferred behaviors (InfoGAIL)\n\n• Model based adversarial optimization (MGAIL)\n• Multi-agent imitation learning\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong"
  },
  {
    "objectID": "lecs/w09/lec09.html#model-based-adversarial-imitation-learning",
    "href": "lecs/w09/lec09.html#model-based-adversarial-imitation-learning",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Model-based Adversarial Imitation Learning",
    "text": "Model-based Adversarial Imitation Learning\nNir Baram, Oron Anschel, Shie Mannor\nPresented by Yuwen Xiong, Mar 1 st"
  },
  {
    "objectID": "lecs/w09/lec09.html#recap-gail-algorithm",
    "href": "lecs/w09/lec09.html#recap-gail-algorithm",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Recap: GAIL algorithm",
    "text": "Recap: GAIL algorithm\n\\[\n\\underset{\\pi}{\\text{argmin}} \\quad \\underset{D \\in (0,1)}{\\text{argmax}} \\mathbb{E}_{\\pi}[\\log D(s, a)] + \\mathbb{E}_{\\pi_E}[\\log(1 - D(s, a))] - \\lambda H(\\pi)\n\\]\n\nWe use Adam to optimize the discriminator and use TRPO to optimize the policy\nThe optimization of the discriminator can be done by using backpropagation, but this is not the case for the optimization of the policy\n\\(\\pi\\) affects the data distribution but do not appear in the objective itself\nWe use these two equations to get gradient estimation for \\(\\pi_{\\theta}\\)\n\n\\[\n\\nabla_{\\theta} \\mathbb{E}_{\\pi}[\\log D(s, a)] \\cong \\mathbb{\\hat{E}}_{\\tau_i}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) Q(s, a)]\n\\]\n\\[Q(\\hat{s}, \\hat{a}) = \\mathbb{\\hat{E}}_{\\tau_i}[\\log D(s, a) \\mid s_0 = \\hat{s}, a_0 = \\hat{a}]\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#motivation-1",
    "href": "lecs/w09/lec09.html#motivation-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Motivation",
    "text": "Motivation\n\nA model-free approach like GAIL has its limitations\n\nThe generative model can no longer be trained by simply backpropagating the gradient from the loss function defined over the discriminator\nHas to resort to high-variance gradient estimations\n\n\n\n\n\nIf we have a model-based version of adversarial imitation learning\n\nThe system can be easily trained end-to-end using regular backpropagation\nThe policy gradient can be derived directly from the gradient of the discriminator\nPolicies can be more robust and training requires fewer interactions with the environment"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm---overview",
    "href": "lecs/w09/lec09.html#algorithm---overview",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - overview",
    "text": "Algorithm - overview\n\n\nThe model-free approach treats the state s as fixed and only tries to optimize the behavior.\n\nInstead, we treat s as a function of the policy: \\[s' = f(s, a)\\]\nSo that, by using the law of total derivative we can get:\n\\[\n\\begin{align}\n\\left.\\nabla_{\\theta} D(s_t, a_t)\\right|_{s=s_t, a=a_t} &= \\left.\\frac{\\partial D}{\\partial a} \\frac{\\partial a}{\\partial \\theta}\\right|_{a=a_t} + \\left.\\frac{\\partial D}{\\partial s} \\frac{\\partial s}{\\partial \\theta}\\right|_{s=s_t} \\\\\n&= \\left.\\frac{\\partial D}{\\partial a} \\frac{\\partial a}{\\partial \\theta}\\right|_{a=a_t} + \\frac{\\partial D}{\\partial s} \\left(\\left.\\frac{\\partial f}{\\partial s} \\frac{\\partial s}{\\partial \\theta}\\right|_{s=s_{t-1}} + \\left.\\frac{\\partial f}{\\partial a} \\frac{\\partial a}{\\partial \\theta}\\right|_{a=a_{t-1}}\\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm---preparation",
    "href": "lecs/w09/lec09.html#algorithm---preparation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - preparation",
    "text": "Algorithm - preparation\nFirst, we know that \\(D(s, a) = p(y|s, a)\\), where \\(y = \\{\\pi_E, \\pi\\}\\)\n\nBy using Bayes rule and the law of total probability we can get:\n\\[D(s, a) = p(\\pi|s, a) = \\frac{p(s, a|\\pi)p(\\pi)}{p(s, a)}\\]\n\\[\\qquad \\qquad \\qquad = \\frac{p(s, a|\\pi)p(\\pi)}{p(s, a|\\pi)p(\\pi) + p(s, a|\\pi_E)p(\\pi_E)}\\]\n\n\n\\[\\qquad = \\frac{p(s, a|\\pi)}{p(s, a|\\pi) + p(s, a|\\pi_E)}\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm---preparation-1",
    "href": "lecs/w09/lec09.html#algorithm---preparation-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - preparation",
    "text": "Algorithm - preparation\nRe-writing it as following:\n\\[\nD(s, a) = \\frac{1}{\\frac{p(s,a|\\pi)+p(s,a|\\pi_E)}{p(s,a|\\pi)}} = \\frac{1}{1 + \\frac{p(s,a|\\pi_E)}{p(s,a|\\pi)}} = \\frac{1}{1 + \\frac{p(a|s,\\pi_E)}{p(a|s,\\pi)} \\cdot \\frac{p(s|\\pi_E)}{p(s|\\pi)}}\\]\n\nLet \\(\\varphi(s, a) = \\frac{p(a|s,\\pi_E)}{p(a|s,\\pi)}\\) and \\(\\psi(s) = \\frac{p(s|\\pi_E)}{p(s|\\pi)}\\), we can get:\n\\[\nD(s, a) = \\frac{1}{1 + \\varphi(s, a) \\cdot \\psi(s)}\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm---preparation-2",
    "href": "lecs/w09/lec09.html#algorithm---preparation-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - preparation",
    "text": "Algorithm - preparation\nHere \\(\\varphi(s, a) = \\frac{p(a|s,\\pi_E)}{p(a|s,\\pi)}\\) stands for policy likelihood ratio\nAnd \\(\\psi(s) = \\frac{p(s|\\pi_E)}{p(s|\\pi)}\\) stands for state distribution likelihood ratio\n\nBy using differentiation rule we can easily get:\n\\[\n\\nabla_a D = -\\frac{\\varphi_a(s, a)\\psi(s)}{(1 + \\varphi(s, a)\\psi(s))^2}\n\\]\n\\[\n\\nabla_s D = -\\frac{\\varphi_s(s, a)\\psi(s) + \\varphi(s, a)\\psi_s(s)}{(1 + \\varphi(s, a)\\psi(s))^2}\n\\]\n\n\nRecall what we need: \\(\\left.\\nabla_\\theta D(s_t, a_t)\\right|_{s=s_t,a=a_t} = \\left.\\frac{\\partial D}{\\partial a} \\frac{\\partial a}{\\partial \\theta}\\right|_{a=a_t} + \\left.\\frac{\\partial D}{\\partial s} \\frac{\\partial s}{\\partial \\theta}\\right|_{s=s_t}\\)"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm---re-parameterization-of-distribution",
    "href": "lecs/w09/lec09.html#algorithm---re-parameterization-of-distribution",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm - re-parameterization of distribution",
    "text": "Algorithm - re-parameterization of distribution\nAssuming the policy is given by\n\\[\n\\pi_\\theta(a|s) = \\mathcal{N}(a|\\mu_\\theta(s), \\sigma_\\theta^2(s))\n\\]\n\nWe can rewrite it to\n\\[\n\\pi_\\theta(a|s) = \\mu_\\theta(s) + \\xi\\sigma_\\theta(s), \\text{ where } \\xi \\sim \\mathcal{N}(0, 1)\n\\]\n\n\nSo that we can get a Monte-Carlo estimator of the derivative\n\\[\n\\begin{align}\n\\nabla_\\theta \\mathbb{E}_\\pi(a|s) D(s, a) &= \\mathbb{E}_{\\rho(\\xi)} \\nabla_a D(a, s) \\nabla_\\theta \\pi_\\theta(a|s) \\\\\n& \\cong \\frac{1}{M} \\sum_{i=1}^M \\left.\\nabla_a D(s, a) \\nabla_\\theta \\pi_\\theta(a|s)\\right|_{\\xi=\\xi_i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm-1",
    "href": "lecs/w09/lec09.html#algorithm-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm",
    "text": "Algorithm"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm-2",
    "href": "lecs/w09/lec09.html#algorithm-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm",
    "text": "Algorithm\nTo maximize the reward function, we can view reward as \\(r(s, a) = -D(s, a)\\), and then maximizing the total reward is equivalent to minimizing the total discriminator beliefs along a trajectory.\n\nSo that we can define: \\[\nJ(\\theta) = \\mathbb{E} \\left[ \\sum_{t=0} \\gamma^t D(s_t, a_t) \\mid \\theta \\right]\n\\]\nAnd write down the derivatives: (this follows SVG paper [Heess et al. 2015]) \\[\nJ_s = \\mathbb{E}_{p(a\\mid s)}\\mathbb{E}_{p(s'\\mid s,a)}\\mathbb{E}_{p(\\xi\\mid s,a,s')}\n\\left[ D_s + D_a\\pi_s + \\gamma J'_{s'}(f_s + f_a\\pi_s)\n\\right]\n\\]\n\\[\nJ_\\theta =  \\mathbb{E}_{p(a\\mid s)}\\mathbb{E}_{p(s'\\mid s,a)}\\mathbb{E}_{p(\\xi\\mid s,a,s')}\n[D_a\\pi_\\theta +  \\gamma(J'_{s'}f_a\\pi_\\theta + J'_\\theta)]\n\\]"
  },
  {
    "objectID": "lecs/w09/lec09.html#algorithm-3",
    "href": "lecs/w09/lec09.html#algorithm-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Algorithm",
    "text": "Algorithm"
  },
  {
    "objectID": "lecs/w09/lec09.html#experiments-1",
    "href": "lecs/w09/lec09.html#experiments-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Experiments",
    "text": "Experiments"
  },
  {
    "objectID": "lecs/w09/lec09.html#todays-agenda-3",
    "href": "lecs/w09/lec09.html#todays-agenda-3",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\n• Adversarial optimization for imitation learning (GAIL)\n• Adversarial optimization with multiple inferred behaviors (InfoGAIL)\n• Model based adversarial optimization (MGAIL)\n\n• Multi-agent imitation learning\n\n\n\n\nAcknowledgments\nToday’s slides are based on student presentations from 2019 by: Yeming Wen, Yin-Hung Chen , and Yuwen Xiong"
  },
  {
    "objectID": "lecs/w12/lec12.html#todays-agenda",
    "href": "lecs/w12/lec12.html#todays-agenda",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nRepresentation Learning for Imitation\n\nIs it better to separate representation learning from policy learning?\n\nProvable generalization\n\nPAC-Bayes generalization bounds\n\nRobustness and safety\n\nRobustness as stability"
  },
  {
    "objectID": "lecs/w12/lec12.html#learning-dense-representations-for-manipulation",
    "href": "lecs/w12/lec12.html#learning-dense-representations-for-manipulation",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning dense representations for manipulation",
    "text": "Learning dense representations for manipulation\n\n\nDense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation. Florence, Manuelli, Tedrake. 2018"
  },
  {
    "objectID": "lecs/w12/lec12.html#learning-policies-on-top-of-pretrained-representations",
    "href": "lecs/w12/lec12.html#learning-policies-on-top-of-pretrained-representations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Learning policies on top of pretrained representations",
    "text": "Learning policies on top of pretrained representations\n\n\nSelf-Supervised Correspondence in Visuomotor Policy Learning. Florence, Manuelli, Tedrake. 2020"
  },
  {
    "objectID": "lecs/w12/lec12.html#another-example-of-policy-learning-on-pretrained-representations",
    "href": "lecs/w12/lec12.html#another-example-of-policy-learning-on-pretrained-representations",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Another example of policy learning on pretrained representations",
    "text": "Another example of policy learning on pretrained representations"
  },
  {
    "objectID": "lecs/w12/lec12.html#can-we-show-theoretically-that-representation-learning-helps",
    "href": "lecs/w12/lec12.html#can-we-show-theoretically-that-representation-learning-helps",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Can we show theoretically that representation learning helps?",
    "text": "Can we show theoretically that representation learning helps?\n[switch to notes]"
  },
  {
    "objectID": "lecs/w12/lec12.html#todays-agenda-1",
    "href": "lecs/w12/lec12.html#todays-agenda-1",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\n\nRepresentation Learning for Imitation\n\nIs it better to separate representation learning from policy learning?\n\n\n\n\nProvable generalization\n\nPAC-Bayes generalization bounds\n\nRobustness and safety\n\nRobustness as stability"
  },
  {
    "objectID": "lecs/w12/lec12.html#pac-bayes-generalization-bounds",
    "href": "lecs/w12/lec12.html#pac-bayes-generalization-bounds",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "PAC-Bayes generalization bounds",
    "text": "PAC-Bayes generalization bounds\n\n\nPAC-Bayes Control: Learning Policies that Provably Generalize to New Environments. Majumdar et al. 2020"
  },
  {
    "objectID": "lecs/w12/lec12.html#pac-bayes-bound-applied-to-control",
    "href": "lecs/w12/lec12.html#pac-bayes-bound-applied-to-control",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "PAC-Bayes Bound Applied to Control",
    "text": "PAC-Bayes Bound Applied to Control\n\n\nPAC-Bayes Control: Learning Policies that Provably Generalize to New Environments. Majumdar et al. 2020"
  },
  {
    "objectID": "lecs/w12/lec12.html#pac-bayes-example",
    "href": "lecs/w12/lec12.html#pac-bayes-example",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "PAC-Bayes Example",
    "text": "PAC-Bayes Example\n\n\nPAC-Bayes Control: Learning Policies that Provably Generalize to New Environments. Majumdar et al. 2020"
  },
  {
    "objectID": "lecs/w12/lec12.html#pac-bayes-control-for-visuomotor-policies",
    "href": "lecs/w12/lec12.html#pac-bayes-control-for-visuomotor-policies",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "PAC-Bayes Control for Visuomotor Policies",
    "text": "PAC-Bayes Control for Visuomotor Policies\n\n\nGeneralization Guarantees for Imitation Learning. Ren, Veer, Majumdar. CoRL 2020."
  },
  {
    "objectID": "lecs/w12/lec12.html#todays-agenda-2",
    "href": "lecs/w12/lec12.html#todays-agenda-2",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\n\nRepresentation Learning for Imitation\n\nIs it better to separate representation learning from policy learning?\n\nProvable generalization\n\nPAC-Bayes generalization bounds\n\n\n\n\nRobustness and safety\n\nRobustness as stability"
  },
  {
    "objectID": "lecs/w12/lec12.html#what-does-it-mean-for-a-policy-to-be-robust",
    "href": "lecs/w12/lec12.html#what-does-it-mean-for-a-policy-to-be-robust",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "What does it mean for a policy to be robust?",
    "text": "What does it mean for a policy to be robust?\n• The system (policy, dynamics) must be such that the policy will achieve its goal point, even if a family of disturbances/noise affect it\n\n\n\n\n\n\nComposition of robust policies\nLQR Trees. Tedrake, RSS 2005."
  },
  {
    "objectID": "lecs/w12/lec12.html#robustness-as-stability-of-dynamical-systems",
    "href": "lecs/w12/lec12.html#robustness-as-stability-of-dynamical-systems",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Robustness as stability of dynamical systems",
    "text": "Robustness as stability of dynamical systems\n\nGlobal asymptotic stability to a goal: the region of attraction includes all states. The system will converge to the goal, no matter where it starts from.\n\n\n\nHow to ensure global asymptotic stability:\n\nShow that there exists a positive energy (Lyapunov) function that\nDecreases over time along the system trajectories\nAnd will become 0 at the convergence point"
  },
  {
    "objectID": "lecs/w12/lec12.html#examples-of-lyapunov-functions",
    "href": "lecs/w12/lec12.html#examples-of-lyapunov-functions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Examples of Lyapunov functions",
    "text": "Examples of Lyapunov functions\n[switches to notes]"
  },
  {
    "objectID": "lecs/w12/lec12.html#two-examples-of-learning-lyapunov-functions",
    "href": "lecs/w12/lec12.html#two-examples-of-learning-lyapunov-functions",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Two examples of learning Lyapunov functions",
    "text": "Two examples of learning Lyapunov functions"
  },
  {
    "objectID": "lecs/w12/lec12.html#stability-via-contraction-theory",
    "href": "lecs/w12/lec12.html#stability-via-contraction-theory",
    "title": "CSC2626 Imitation Learning for Robotics",
    "section": "Stability via Contraction Theory",
    "text": "Stability via Contraction Theory"
  }
]